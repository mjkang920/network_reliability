{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe80baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import os, json, time, math, itertools\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "\n",
    "import gurobipy as gp\n",
    "\n",
    "BASE_DIR = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\"\n",
    "edge_fp   = os.path.join(BASE_DIR, \"edges.json\")\n",
    "route_fp  = os.path.join(BASE_DIR, \"routes_nodes.json\")\n",
    "demand_fp = os.path.join(BASE_DIR, \"demand.json\")\n",
    "dept_fp   = os.path.join(BASE_DIR, \"dep_time.json\")\n",
    "\n",
    "# 재현성\n",
    "GLOBAL_SEED = 42\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e79ee156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Main] Pairs detected: 95\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "# 0-b. 고정 파라미터\n",
    "T        = 12\n",
    "max_wait = 0\n",
    "CAPACITY = 3\n",
    "w1, w2, w3 = 1000, 50, 10\n",
    "BIG_M = 10**6\n",
    "SINK  = \"SINK\"\n",
    "\n",
    "# 0-c. JSON 로드\n",
    "with open(edge_fp, encoding=\"utf-8\") as f:\n",
    "    edges_raw = json.load(f)\n",
    "edges = {eid: (src, dst, int(tau)) for eid, (src, dst, tau) in edges_raw.items()}\n",
    "\n",
    "with open(route_fp, encoding=\"utf-8\") as f:\n",
    "    routes_nodes = json.load(f)\n",
    "\n",
    "with open(dept_fp, encoding=\"utf-8\") as f:\n",
    "    dep_time = {tr: int(t) for tr, t in json.load(f).items()}\n",
    "\n",
    "with open(demand_fp, encoding=\"utf-8\") as f:\n",
    "    dem_raw = json.load(f)\n",
    "demand = {tr: [(o, d, float(q)) for o, d, q in lst] for tr, lst in dem_raw.items()}\n",
    "\n",
    "trains = list(routes_nodes)\n",
    "nodes  = {n for _, (s, d, _) in edges.items() for n in (s, d)}\n",
    "nodes.add(SINK)\n",
    "\n",
    "# e1/e1r ~ e95/e95r 쌍 구성 (실제 edges에 존재하는 것만)\n",
    "edge_pairs = []\n",
    "for i in range(1, 96):\n",
    "    e  = f\"e{i}\"\n",
    "    er = f\"e{i}r\"\n",
    "    if e in edges and er in edges:\n",
    "        edge_pairs.append((e, er))\n",
    "NUM_PAIRS = len(edge_pairs)\n",
    "print(f\"[Main] Pairs detected: {NUM_PAIRS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419a44fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Main] Sampling device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 3\n",
    "DEVICE_SAMPLING = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[Main] Sampling device: {DEVICE_SAMPLING}\")\n",
    "\n",
    "def sample_failed_sets(batch_size: int,\n",
    "                       p_low=0.05, p_high=0.20,\n",
    "                       prob_mode='fixed',\n",
    "                       device: str = DEVICE_SAMPLING):\n",
    "    pairs = NUM_PAIRS\n",
    "    if prob_mode == 'fixed':\n",
    "        p_pairs = torch.empty(pairs, device=device).uniform_(p_low, p_high)\n",
    "        U = torch.rand(batch_size, pairs, device=device)\n",
    "        fail_mask = (U < p_pairs)\n",
    "    elif prob_mode == 'resample':\n",
    "        p_pairs = torch.empty(batch_size, pairs, device=device).uniform_(p_low, p_high)\n",
    "        U = torch.rand(batch_size, pairs, device=device)\n",
    "        fail_mask = (U < p_pairs)\n",
    "    else:\n",
    "        raise ValueError(\"prob_mode must be 'fixed' or 'resample'.\")\n",
    "\n",
    "    b_idx, j_idx = fail_mask.nonzero(as_tuple=True)\n",
    "    b_idx = b_idx.cpu().numpy()\n",
    "    j_idx = j_idx.cpu().numpy()\n",
    "\n",
    "    failed_sets = [set() for _ in range(batch_size)]\n",
    "    for b, j in zip(b_idx, j_idx):\n",
    "        e, er = edge_pairs[j]\n",
    "        failed_sets[b].add(e); failed_sets[b].add(er)\n",
    "    return failed_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "784d9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "def build_arc_and_caches(failed_edges: set):\n",
    "    arc_list = []  # (from,to,eid,tau,t0,t1)\n",
    "    for eid, (src, dst, tau) in edges.items():\n",
    "        if eid in failed_edges:\n",
    "            continue\n",
    "        for t in range(T + 1 - tau):\n",
    "            arc_list.append((f\"{src}^{t}\", f\"{dst}^{t+tau}\", eid, tau, t, t+tau))\n",
    "\n",
    "    for n in nodes - {SINK}:\n",
    "        for w in range(1, max_wait+1):\n",
    "            for t in range(T + 1 - w):\n",
    "                arc_list.append((f\"{n}^{t}\", f\"{n}^{t+w}\", f\"w_{n}_{w}\", w, t, t+w))\n",
    "    for n in nodes - {SINK}:\n",
    "        for t in range(T+1):\n",
    "            arc_list.append((f\"{n}^{t}\", f\"{SINK}^{t}\", f\"dummy_{n}\", 0, t, t))\n",
    "\n",
    "    arc_idx = {info: i for i, info in enumerate(arc_list)}\n",
    "\n",
    "    out_arcs = defaultdict(list)\n",
    "    in_arcs  = defaultdict(list)\n",
    "    node_in_arcs  = defaultdict(list)\n",
    "    node_out_arcs = defaultdict(list)\n",
    "\n",
    "    for k, (fr, to, eid, _, t0, t1) in enumerate(arc_list):\n",
    "        n_fr, tt_fr = fr.split(\"^\"); tt_fr = int(tt_fr)\n",
    "        n_to, tt_to = to.split(\"^\"); tt_to = int(tt_to)\n",
    "        out_arcs[(n_fr, tt_fr)].append(k)\n",
    "        in_arcs [(n_to, tt_to)].append(k)\n",
    "        if not eid.startswith((\"w_\", \"dummy\")):\n",
    "            node_out_arcs[n_fr].append(k)\n",
    "            node_in_arcs [n_to].append(k)\n",
    "\n",
    "    # 예정 도착시각(경로 기준, 고장 엣지는 우회/캔슬로 처리될 수 있으므로 그대로 구성)\n",
    "    sched = {}\n",
    "    for tr, path in routes_nodes.items():\n",
    "        t, arr = 0, {path[0]: dep_time[tr]}\n",
    "        for u, v in zip(path[:-1], path[1:]):\n",
    "            eid = next(e for e, (s, d, _) in edges.items() if s == u and d == v)\n",
    "            t += edges[eid][2]; arr[v] = dep_time[tr] + t\n",
    "        sched[tr] = arr\n",
    "\n",
    "    q_r = {tr: sum(q for *_, q in demand[tr]) for tr in trains}\n",
    "\n",
    "    cap_map = defaultdict(list)\n",
    "    for k, (fr, to, eid, _, t0, t1) in enumerate(arc_list):\n",
    "        if eid.startswith((\"w_\", \"dummy\")): \n",
    "            continue\n",
    "        for tt in range(t0, t1):\n",
    "            cap_map[(eid, tt)].append(k)\n",
    "\n",
    "    return {\n",
    "        \"arc_list\": arc_list,\n",
    "        \"arc_idx\": arc_idx,\n",
    "        \"out_arcs\": out_arcs,\n",
    "        \"in_arcs\": in_arcs,\n",
    "        \"node_in_arcs\": node_in_arcs,\n",
    "        \"node_out_arcs\": node_out_arcs,\n",
    "        \"sched\": sched,\n",
    "        \"q_r\": q_r,\n",
    "        \"cap_map\": cap_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ced482e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "def build_base_model(components, env):\n",
    "    arc_list      = components[\"arc_list\"]\n",
    "    arc_idx       = components[\"arc_idx\"]\n",
    "    out_arcs      = components[\"out_arcs\"]\n",
    "    in_arcs       = components[\"in_arcs\"]\n",
    "    node_in_arcs  = components[\"node_in_arcs\"]\n",
    "    node_out_arcs = components[\"node_out_arcs\"]\n",
    "    sched         = components[\"sched\"]\n",
    "    q_r           = components[\"q_r\"]\n",
    "\n",
    "    m = gp.Model(env=env)\n",
    "    m.Params.OutputFlag = 0\n",
    "    m.Params.Threads = 1\n",
    "\n",
    "    nA, nT = len(arc_list), len(trains)\n",
    "    x = m.addVars(nA, nT, vtype=gp.GRB.BINARY, name=\"x\")\n",
    "    h = m.addVars(trains, vtype=gp.GRB.BINARY, name=\"h\")\n",
    "\n",
    "    y = {(tr, n): m.addVar(vtype=gp.GRB.BINARY) for tr in trains for n in routes_nodes[tr][1:]}\n",
    "    s = {(tr, n): m.addVar(vtype=gp.GRB.BINARY) for tr in trains for n in nodes - {SINK}}\n",
    "    z = {(tr,o,d): m.addVar(vtype=gp.GRB.BINARY) for tr in trains for (o,d,_) in demand[tr]}\n",
    "    delta = {(tr,o,d): m.addVar(lb=0) for tr in trains for (o,d,_) in demand[tr]}\n",
    "    t_arr = {(tr, n): m.addVar(lb=0, ub=T, vtype=gp.GRB.INTEGER) for tr in trains for n in nodes - {SINK}}\n",
    "\n",
    "    # (2) 출발-flow\n",
    "    for tr_i, tr in enumerate(trains):\n",
    "        r_o, t_dep = routes_nodes[tr][0], dep_time[tr]\n",
    "        idx_out = out_arcs[(r_o, t_dep)]\n",
    "        m.addConstr(x.sum(idx_out, tr_i) == h[tr])\n",
    "\n",
    "    # (5) 노드-시간 보존\n",
    "    for tr_i, tr in enumerate(trains):\n",
    "        r_o, t_dep = routes_nodes[tr][0], dep_time[tr]\n",
    "        for n in nodes - {SINK}:\n",
    "            for t in range(T+1):\n",
    "                inflow  = x.sum(in_arcs[(n, t)],  tr_i)\n",
    "                outflow = x.sum(out_arcs[(n, t)], tr_i)\n",
    "                if (n == r_o) and (t == t_dep):\n",
    "                    m.addConstr(outflow - inflow == h[tr])\n",
    "                else:\n",
    "                    m.addConstr(inflow == outflow)\n",
    "\n",
    "    # terminal & dummy\n",
    "    for tr_i, tr in enumerate(trains):\n",
    "        cand = routes_nodes[tr][1:]\n",
    "        m.addConstr(gp.quicksum(y[tr, n] for n in cand) == h[tr])\n",
    "        for n in cand:\n",
    "            idx_dum = [arc_idx[a] for a in arc_list if a[2] == f\"dummy_{n}\"]\n",
    "            m.addConstr(x.sum(idx_dum, tr_i) == y[tr, n])\n",
    "\n",
    "    # s (visit)\n",
    "    for (tr, n), var in s.items():\n",
    "        tr_i = trains.index(tr)\n",
    "        idx = node_out_arcs[n] + node_in_arcs[n]\n",
    "        flow = x.sum(idx, tr_i)\n",
    "        m.addConstr(flow >= var)\n",
    "        m.addConstr(flow <= BIG_M * var)\n",
    "\n",
    "    # z-logic & delta\n",
    "    for tr in trains:\n",
    "        for (o, d, q) in demand[tr]:\n",
    "            m.addConstr(z[tr, o, d] <= s[tr, o])\n",
    "            m.addConstr(z[tr, o, d] <= s[tr, d])\n",
    "            m.addConstr(z[tr, o, d] >= s[tr, o] + s[tr, d] - 1)\n",
    "            m.addConstr(z[tr, o, d] <= h[tr])\n",
    "\n",
    "    for tr_i, tr in enumerate(trains):\n",
    "        for k, (fr, to, eid, *_ ) in enumerate(arc_list):\n",
    "            if eid.startswith((\"w_\", \"dummy\")):\n",
    "                continue\n",
    "            n_to, tt = to.split(\"^\"); tt = int(tt)\n",
    "            m.addConstr(t_arr[tr, n_to] >= tt - BIG_M * (1 - x[k, tr_i]))\n",
    "            m.addConstr(t_arr[tr, n_to] <= tt + BIG_M * (1 - x[k, tr_i]))\n",
    "        for (o, d, q) in demand[tr]:\n",
    "            sched_t = sched[tr].get(d, T)\n",
    "            m.addConstr(delta[tr, o, d] >= t_arr[tr, d] - sched_t - BIG_M * (1 - z[tr, o, d]))\n",
    "            m.addConstr(delta[tr, o, d] <= BIG_M * z[tr, o, d])\n",
    "\n",
    "    obj  = gp.quicksum(w1 * q_r[tr] * (1 - h[tr]) for tr in trains)\n",
    "    obj += gp.quicksum(w2 * q * (1 - z[tr, o, d]) for tr in trains for (o, d, q) in demand[tr])\n",
    "    obj += gp.quicksum(w3 * q * delta[tr, o, d] * z[tr, o, d] for tr in trains for (o, d, q) in demand[tr])\n",
    "    m.setObjective(obj)\n",
    "    return m, x, h, delta, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab253b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "def _make_env():\n",
    "    env = gp.Env(empty=True)\n",
    "    env.setParam('OutputFlag', 0)\n",
    "    env.setParam('Threads', 1)\n",
    "    # 각 워커별 구로비 로그파일(필요시 확인)\n",
    "    try:\n",
    "        env.setParam('LogFile', f'gurobi_{os.getpid()}.log')\n",
    "    except gp.GurobiError:\n",
    "        pass\n",
    "    env.start()\n",
    "    return env\n",
    "\n",
    "def _log_worker(msg: str, sample_id: int = None):\n",
    "    # 자식 프로세스에서 파일 로그 남기기 (부모 콘솔로는 잘 안 보임)\n",
    "    pid = os.getpid()\n",
    "    prefix = f\"[PID {pid}]\"\n",
    "    if sample_id is not None:\n",
    "        prefix += f\"[S{sample_id}]\"\n",
    "    line = f\"{prefix} {msg}\\n\"\n",
    "    with open(f\"worker_{pid}.log\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line)\n",
    "\n",
    "def solve_one_sample(sample_id: int, failed_edges: set, time_limit=30):\n",
    "    try:\n",
    "        _log_worker(\"start\", sample_id)\n",
    "        env = _make_env()\n",
    "        comp = build_arc_and_caches(failed_edges)\n",
    "        m, x, h, delta, z = build_base_model(comp, env)\n",
    "\n",
    "        arc_list = comp[\"arc_list\"]\n",
    "        cap_map  = comp[\"cap_map\"]\n",
    "\n",
    "        MAX_ITER, EPS = 40, 1e-6\n",
    "\n",
    "        t0 = time.time()\n",
    "        sol_prev = None\n",
    "        for it in range(MAX_ITER):\n",
    "            spent = time.time() - t0\n",
    "            remain = max(0.05, time_limit - spent)\n",
    "            m.Params.TimeLimit = remain\n",
    "\n",
    "            if sol_prev:\n",
    "                for (idx, ti), v in sol_prev.items():\n",
    "                    if v: x[idx, ti].Start = 1\n",
    "\n",
    "            _log_worker(f\"optimize iter={it}, remain={remain:.2f}s\", sample_id)\n",
    "            m.optimize()\n",
    "\n",
    "            status = int(m.Status)\n",
    "            _log_worker(f\"status={status}\", sample_id)\n",
    "\n",
    "            # 종료/이어가기 판단\n",
    "            x_val = {(idx, ti): int(round(x[idx, ti].X))\n",
    "                     for idx in range(len(arc_list)) for ti in range(len(trains))}\n",
    "            x_mat = np.zeros((len(arc_list), len(trains)), dtype=np.uint8)\n",
    "            for (idx, ti), v in x_val.items():\n",
    "                if v: x_mat[idx, ti] = 1\n",
    "            viol = {key for key, idx_list in cap_map.items()\n",
    "                    if x_mat[idx_list].sum() > CAPACITY + EPS}\n",
    "\n",
    "            if (time.time() - t0) >= time_limit or status in (gp.GRB.OPTIMAL, gp.GRB.TIME_LIMIT):\n",
    "                if not viol or (time.time() - t0) >= time_limit:\n",
    "                    _log_worker(f\"stop iter={it}, viol={len(viol)}\", sample_id)\n",
    "                    break\n",
    "                # 시간이 남아있으면 위반 추가\n",
    "                for (eid, tt) in viol:\n",
    "                    expr = gp.quicksum(x[idx, ti] for idx in cap_map[(eid, tt)]\n",
    "                                       for ti in range(len(trains)))\n",
    "                    m.addConstr(expr <= CAPACITY, name=f\"cap_{eid}_{tt}_{it}\")\n",
    "                sol_prev = x_val\n",
    "                continue\n",
    "            else:\n",
    "                # 기타 상태(INF_OR_UNBD 등) → 위반 없으면 종료, 있으면 한번 더 시도\n",
    "                if not viol:\n",
    "                    _log_worker(f\"stop-other-status iter={it}, no viol\", sample_id)\n",
    "                    break\n",
    "                for (eid, tt) in viol:\n",
    "                    expr = gp.quicksum(x[idx, ti] for idx in cap_map[(eid, tt)]\n",
    "                                       for ti in range(len(trains)))\n",
    "                    m.addConstr(expr <= CAPACITY, name=f\"cap_{eid}_{tt}_{it}\")\n",
    "                sol_prev = x_val\n",
    "\n",
    "        cancelled = sum(1 for tr in trains if h[tr].X < 0.5)\n",
    "        delayed   = sum(1 for tr in trains if (h[tr].X >= 0.5) and\n",
    "                        any(delta[tr, o, d].X > 1e-6 for (o, d, _) in demand[tr]))\n",
    "        truncated = sum(1 for tr in trains if (h[tr].X >= 0.5) and\n",
    "                        (not any(delta[tr, o, d].X > 1e-6 for (o, d, _) in demand[tr])) and\n",
    "                        any(z[tr, o, d].X < 0.5 for (o, d, _) in demand[tr]))\n",
    "\n",
    "        res = {\n",
    "            \"sample_id\": sample_id,\n",
    "            \"status\": int(m.Status),\n",
    "            \"obj\": (float(m.ObjVal) if m.Status == gp.GRB.OPTIMAL else None),\n",
    "            \"cancelled\": cancelled, \"delayed\": delayed, \"truncated\": truncated,\n",
    "            \"failed_edges_count\": len(failed_edges),\n",
    "            \"walltime_sec\": round(time.time() - t0, 3),\n",
    "        }\n",
    "        _log_worker(f\"done wall={res['walltime_sec']}s status={res['status']}\", sample_id)\n",
    "        m.dispose(); env.dispose()\n",
    "        return res\n",
    "\n",
    "    except Exception as e:\n",
    "        _log_worker(f\"exception: {repr(e)}\", sample_id)\n",
    "        return {\"sample_id\": sample_id, \"status\": -1, \"obj\": None, \"error\": repr(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Main] start: total=2, concurrent=2, time_limit=20s\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Pool 버전 (spawn + maxtasksperchild=1)\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def _init_worker(seed=None):\n",
    "    # 필요시 시드/환경 설정\n",
    "    np.random.seed(GLOBAL_SEED)\n",
    "    torch.manual_seed(GLOBAL_SEED)\n",
    "\n",
    "# ★ 래퍼: 튜플(arg)을 언패킹해서 solve_one_sample에 전달\n",
    "def _solve_wrapper(arg):\n",
    "    return solve_one_sample(*arg)\n",
    "\n",
    "def run_parallel_sampling_pool(total_samples=10,\n",
    "                               concurrent=2,\n",
    "                               prob_mode='fixed',\n",
    "                               time_limit=20,\n",
    "                               p_low=0.05, p_high=0.20):\n",
    "    \"\"\"\n",
    "    spawn 컨텍스트의 Pool을 사용해 안정적으로 병렬 실행.\n",
    "    각 워커는 한 건 처리 후 재시작되어 리소스 누수/라이선스 충돌을 줄인다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mp.set_start_method(\"spawn\")\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    failed_sets = sample_failed_sets(total_samples, p_low, p_high, prob_mode)\n",
    "\n",
    "    # args는 (sample_id, failed_set, time_limit) 튜플 리스트\n",
    "    args = [(sid, failed_sets[sid], time_limit) for sid in range(total_samples)]\n",
    "\n",
    "    start = time.time()\n",
    "    results = []\n",
    "    print(f\"[Main] start: total={total_samples}, concurrent={concurrent}, time_limit={time_limit}s\")\n",
    "\n",
    "    with ctx.Pool(processes=concurrent,\n",
    "                  initializer=_init_worker,\n",
    "                  maxtasksperchild=1) as pool:\n",
    "\n",
    "        # ★ 여기! imap_unordered에 래퍼를 넣어서 언패킹 문제 해결\n",
    "        for i, res in enumerate(pool.imap_unordered(_solve_wrapper, args, chunksize=1), 1):\n",
    "            results.append(res)\n",
    "            # 진행 표시 (10% 단위)\n",
    "            if i % max(1, total_samples // 10) == 0 or i == total_samples:\n",
    "                print(f\"[Main] progress: {i}/{total_samples} done\")\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"[Main] Done {len(results)} samples in {elapsed:.1f}s (concurrent={concurrent}).\")\n",
    "\n",
    "    df = pd.DataFrame(results).sort_values(\"sample_id\").reset_index(drop=True)\n",
    "    if \"error\" in df.columns and df[\"error\"].notna().any():\n",
    "        print(\"[Main] Some workers returned errors. First few:\")\n",
    "        print(df[df[\"error\"].notna()].head())\n",
    "    return df\n",
    "\n",
    "# ⚠️ Windows/Jupyter에서는 반드시 메인 가드 사용!\n",
    "if __name__ == \"__main__\":\n",
    "    mp.freeze_support()  # Windows 안전장치\n",
    "    # 작은 케이스부터 점진적으로\n",
    "    df_res = run_parallel_sampling_pool(total_samples=2, concurrent=2, prob_mode='fixed', time_limit=20)\n",
    "    display(df_res.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# 10,000 샘플을 100개 동시 실행 (예시)\n",
    "# df_all = run_parallel_sampling(total_samples=10000, concurrent=100, prob_mode='fixed', time_limit=20)\n",
    "# df_all.to_csv(\"mcs_results_10k.csv\", index=False)\n",
    "# display(df_all.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjkang",
   "language": "python",
   "name": "mjkang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
