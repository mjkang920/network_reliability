{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c9612c",
   "metadata": {},
   "source": [
    "#### QGIS -> demand \n",
    "ë…¸ì„ ë³„ë¡œ ê²½ë¡œ ìƒì„±\n",
    "distance ë°ì´í„° ë„£ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab9b4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ë…¸ì„ ë³„ ë‹¨ì¼ ê²½ë¡œ (ë°©í–¥ ë¬´ì‹œ)\n",
      "KTXê°•ë¦‰ì„ : [22, 23, 34, 51]\n",
      "ê²½ë¶€ê³ ì†ì² ë„: [4, 3, 1, 2, 13, 18, 28, 42, 60, 72, 75, 79]\n",
      "ê²½ë¶€ì„ : [79, 77, 76, 73, 70, 63, 60, 59, 46, 41, 28, 17, 14, 9, 7, 1, 4]\n",
      "ê²½ë¶ì„ : [41, 38, 36, 40, 43]\n",
      "ê²½ì „ì„ : [48, 57, 64, 65, 62, 66, 71, 73]\n",
      "ë™í•´ì„ : [55, 58, 69, 74, 72]\n",
      "ì˜ë™ì„ : [51, 55, 54, 50, 43]\n",
      "ì¥í•­ì„ : [14, 11, 10, 15, 16, 27, 30]\n",
      "ì „ë¼ì„ : [68, 62, 56, 47, 45, 33, 30]\n",
      "ì¤‘ì•™ì„ : [3, 5, 6, 8, 12, 22, 29, 31, 35, 39, 43, 49, 52, 67, 72, 78, 79]\n",
      "ì¶©ë¶ì„ : [17, 18, 19, 20, 26, 29]\n",
      "í˜¸ë‚¨ê³ ì†ì² ë„: [48, 37, 30, 21, 18, 13, 2, 1, 3]\n",
      "í˜¸ë‚¨ì„ : [61, 53, 48, 44, 37, 32, 30, 24, 25, 28]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. íŒŒì¼ ê²½ë¡œ\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "xlsx_path = base_path + r\"\\qgis_export.xlsx\"\n",
    "sheet_name = \"LINE\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "df = pd.read_excel(xlsx_path, sheet_name=sheet_name)\n",
    "df = df.dropna(subset=[\"from_node_\", \"to_node_id\"])\n",
    "df[\"from_node_\"] = df[\"from_node_\"].astype(int)\n",
    "df[\"to_node_id\"] = df[\"to_node_id\"].astype(int)\n",
    "\n",
    "print(\"ğŸ“Œ ë…¸ì„ ë³„ ë‹¨ì¼ ê²½ë¡œ (ë°©í–¥ ë¬´ì‹œ)\")\n",
    "for line, group in df.groupby(\"RLWAY_NM\"):\n",
    "    # ì–‘ë°©í–¥ ê·¸ë˜í”„ ìƒì„±\n",
    "    graph = defaultdict(list)\n",
    "    for u, v in zip(group[\"from_node_\"], group[\"to_node_id\"]):\n",
    "        graph[u].append(v)\n",
    "        graph[v].append(u)\n",
    "\n",
    "    # ë…¸ë“œ ì—°ê²° ìƒíƒœ ì§„ë‹¨\n",
    "    degree = {n: len(adj) for n, adj in graph.items()}\n",
    "    endpoints = [n for n, d in degree.items() if d == 1]\n",
    "    if len(endpoints) != 2:\n",
    "        print(f\"âš ï¸ {line}: ì„ í˜• ê²½ë¡œ ì•„ë‹˜ (ë ë…¸ë“œ {len(endpoints)}ê°œ)\")\n",
    "        continue\n",
    "\n",
    "    # í•œìª½ ëì—ì„œë¶€í„° BFSë¡œ ê²½ë¡œ ë³µì›\n",
    "    start = endpoints[0]\n",
    "    visited = set()\n",
    "    path = []\n",
    "\n",
    "    def dfs(u):\n",
    "        visited.add(u)\n",
    "        path.append(u)\n",
    "        for v in graph[u]:\n",
    "            if v not in visited:\n",
    "                dfs(v)\n",
    "\n",
    "    dfs(start)\n",
    "    print(f\"{line}: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef1b54",
   "metadata": {},
   "source": [
    "#### EDGE timestep ë°ì´í„° ë§Œë“¤ê¸° ìœ„í•´\n",
    "- distanceë¡œ ì´ë™ì‹œê°„\n",
    "- ì´ë™ì‹œê°„ ê¸°ë°˜ time step ê³„ì‚°\n",
    "- EDGE ë°ì´í„° .json íŒŒì¼ë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f068c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ: time_step ì—´ì´ ì¶”ê°€ëœ íŒŒì¼ì´ ì €ì¥ë¨ â†’\n",
      "D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export_with_time.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "input_file  = base_path + r\"\\qgis_export.xlsx\"\n",
    "output_file = base_path + r\"\\qgis_export_with_time.xlsx\"\n",
    "sheet_name  = \"EDGE\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. ì›ë³¸ ì—‘ì…€ ì „ì²´ ì½ê¸° (ëª¨ë“  ì‹œíŠ¸ ë³´ì¡´ ëª©ì )\n",
    "xlsx_all = pd.read_excel(input_file, sheet_name=None)\n",
    "\n",
    "# 2. EDGE ì‹œíŠ¸ë§Œ ìˆ˜ì •\n",
    "df_edge = xlsx_all[sheet_name]\n",
    "\n",
    "# 3. time_step ê³„ì‚°\n",
    "if 'distance' not in df_edge.columns:\n",
    "    raise ValueError(\"'distance' ì—´ì´ EDGE ì‹œíŠ¸ì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "df_edge['time_step'] = df_edge['distance'] / 150 * 60\n",
    "\n",
    "# 4. ìˆ˜ì •í•œ EDGE ì‹œíŠ¸ ë‹¤ì‹œ ë„£ê¸°\n",
    "xlsx_all[sheet_name] = df_edge\n",
    "\n",
    "# 5. ì „ì²´ ì‹œíŠ¸ë¥¼ ìƒˆ íŒŒì¼ë¡œ ì €ì¥\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    for sheet, df in xlsx_all.items():\n",
    "        df.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "print(f\"âœ… ì™„ë£Œ: time_step ì—´ì´ ì¶”ê°€ëœ íŒŒì¼ì´ ì €ì¥ë¨ â†’\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb45211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ: 'time_step' ê³„ì‚° í›„ ì €ì¥ë¨ â†’\n",
      "D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export_with_step.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "input_file  = base_path + r\"\\qgis_export_with_time.xlsx\"\n",
    "output_file = base_path + r\"\\qgis_export_with_step.xlsx\"\n",
    "sheet_name  = \"EDGE\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. ëª¨ë“  ì‹œíŠ¸ ì½ê¸°\n",
    "xlsx_all = pd.read_excel(input_file, sheet_name=None)\n",
    "\n",
    "# 2. EDGE ì‹œíŠ¸ ìˆ˜ì •\n",
    "df_edge = xlsx_all[sheet_name]\n",
    "\n",
    "# 3. time_step ê³„ì‚° (5ë¡œ ë‚˜ëˆ„ê³  ì˜¬ë¦¼)\n",
    "if 'time(min)' not in df_edge.columns:\n",
    "    raise ValueError(\"'time(min)' ì—´ì´ EDGE ì‹œíŠ¸ì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "df_edge['time_step'] = df_edge['time(min)'].apply(lambda x: math.ceil(x / 5))\n",
    "\n",
    "# 4. ë®ì–´ì“°ê¸°\n",
    "xlsx_all[sheet_name] = df_edge\n",
    "\n",
    "# 5. ìƒˆ íŒŒì¼ë¡œ ì €ì¥\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    for sheet, df in xlsx_all.items():\n",
    "        df.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "print(f\"âœ… ì™„ë£Œ: 'time_step' ê³„ì‚° í›„ ì €ì¥ë¨ â†’\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "072ab939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì •ë ¬ëœ edges.json ì €ì¥ ì™„ë£Œ â†’\n",
      "D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\edges.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "input_file  = os.path.join(base_path, \"qgis_export_with_step.xlsx\")\n",
    "sheet_name  = \"EDGE\"\n",
    "output_json = os.path.join(base_path, \"edges.json\")\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# 2. í•„ìš”í•œ ì—´ í™•ì¸\n",
    "required_cols = ['edge_id', 'from_node_id', 'to_node_id', 'time_step']\n",
    "missing = set(required_cols) - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"ë‹¤ìŒ ì—´ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing}\")\n",
    "\n",
    "# 3. ì •ìˆ˜í˜• edge_id ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
    "df = df.dropna(subset=['edge_id'])\n",
    "df['edge_id'] = df['edge_id'].astype(int)\n",
    "df = df.sort_values(by='edge_id')\n",
    "\n",
    "# 4. edge ë”•ì…”ë„ˆë¦¬ ìƒì„± (OrderedDictë¡œ ìˆœì„œ ìœ ì§€)\n",
    "edges = OrderedDict()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    eid = f\"e{int(row['edge_id'])}\"\n",
    "    n_from = f\"n{int(row['from_node_id'])}\"\n",
    "    n_to   = f\"n{int(row['to_node_id'])}\"\n",
    "    tstep  = int(row['time_step'])\n",
    "\n",
    "    edges[eid] = (n_from, n_to, tstep)\n",
    "\n",
    "# 5. JSON ì €ì¥\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(edges, f, indent=4)\n",
    "\n",
    "print(f\"âœ… ì •ë ¬ëœ edges.json ì €ì¥ ì™„ë£Œ â†’\\n{output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0837e648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\routes_nodes.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "xlsx_file = os.path.join(base_path, \"qgis_export_with_step.xlsx\")\n",
    "sheet_name = \"TRAIN\"\n",
    "output_json = os.path.join(base_path, \"routes_nodes.json\")\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. TRAIN ì‹œíŠ¸ ì½ê¸°\n",
    "df = pd.read_excel(xlsx_file, sheet_name=sheet_name)\n",
    "\n",
    "# 2. routes_nodes ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "routes_nodes = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    name = str(row[\"RLWAY_NM\"]).strip()\n",
    "    nodes_str = str(row[\"NODE\"]).strip()\n",
    "\n",
    "    # ë…¸ë“œ ë¬¸ìì—´ íŒŒì‹± â†’ ['n4', 'n3', 'n1', ...]\n",
    "    node_list = [f\"n{int(n)}\" for n in nodes_str.split(\"-\") if n.isdigit()]\n",
    "\n",
    "    # ì •ë°©í–¥ ë° ì—­ë°©í–¥ ì €ì¥\n",
    "    routes_nodes[f\"{name}1\"] = node_list\n",
    "    routes_nodes[f\"{name}2\"] = node_list[::-1]\n",
    "\n",
    "# 3. JSON ì €ì¥\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(routes_nodes, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ea2954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì–‘ë°©í–¥ edges ì €ì¥ ì™„ë£Œ: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\edges_bidirectional.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "input_json = os.path.join(base_path, \"edges.json\")\n",
    "output_json = os.path.join(base_path, \"edges_bidirectional.json\")\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. ê¸°ì¡´ edges.json ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    edges = json.load(f)\n",
    "\n",
    "# 2. ì–‘ë°©í–¥ ë”•ì…”ë„ˆë¦¬ ìƒì„± (ìˆœì„œ ë³´ì¡´)\n",
    "edges_bidir = OrderedDict()\n",
    "\n",
    "for eid, (n1, n2, t) in edges.items():\n",
    "    # ì •ë°©í–¥ edge ì¶”ê°€\n",
    "    edges_bidir[eid] = [n1, n2, t]\n",
    "    \n",
    "    # ì—­ë°©í–¥ edge ì¶”ê°€\n",
    "    eid_r = f\"{eid}r\"\n",
    "    edges_bidir[eid_r] = [n2, n1, t]\n",
    "\n",
    "# 3. ìƒˆ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(edges_bidir, f, indent=4)\n",
    "\n",
    "print(f\"âœ… ì–‘ë°©í–¥ edges ì €ì¥ ì™„ë£Œ: {output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9416e8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… demand_template.json ì €ì¥ ì™„ë£Œ â†’\n",
      "D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\demand_template.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "routes_json = os.path.join(base_path, \"routes_nodes.json\")\n",
    "output_demand_json = os.path.join(base_path, \"demand_template.json\")\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. routes_nodes.json ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(routes_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    routes_nodes = json.load(f)\n",
    "\n",
    "# 2. demand ìƒì„±\n",
    "demand = {}\n",
    "\n",
    "for tr, path in routes_nodes.items():\n",
    "    od_list = []\n",
    "    for i in range(len(path)-1):\n",
    "        for j in range(i+1, len(path)):\n",
    "            o, d = path[i], path[j]\n",
    "            od_list.append((o, d, None))  # ìˆ˜ìš”ëŠ” ì•„ì§ ì—†ìŒ\n",
    "    demand[tr] = od_list\n",
    "\n",
    "# 3. ì €ì¥\n",
    "with open(output_demand_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(demand, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… demand_template.json ì €ì¥ ì™„ë£Œ â†’\\n{output_demand_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42781bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨ â†’ D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\demand_template_compact.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "input_file  = os.path.join(base_path, \"demand_template.json\")\n",
    "output_file = os.path.join(base_path, \"demand_template_compact.json\")\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    demand = json.load(f)\n",
    "\n",
    "# 2. ì €ì¥ (ìˆ˜ë™ ë¬¸ìì—´ í¬ë§·)\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"{\\n\")\n",
    "    for i, (k, od_list) in enumerate(demand.items()):\n",
    "        f.write(f'    \"{k}\": [\\n')\n",
    "        line = \"        \"  # ë“¤ì—¬ì“°ê¸° ì‹œì‘\n",
    "        for j, od in enumerate(od_list):\n",
    "            # JSON ê°’ìœ¼ë¡œ ì¸ì½”ë”© (ensure null/quote ë“± ë§ì¶¤)\n",
    "            od_json = json.dumps(od, ensure_ascii=False)\n",
    "            line += od_json\n",
    "            if j < len(od_list) - 1:\n",
    "                line += \", \"\n",
    "            if len(line) > 120:\n",
    "                f.write(line + \"\\n\")\n",
    "                line = \"        \"\n",
    "        if line.strip():\n",
    "            f.write(line + \"\\n\")\n",
    "        f.write(\"    ]\")\n",
    "        if i < len(demand) - 1:\n",
    "            f.write(\",\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")\n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"âœ… ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨ â†’ {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a2836b",
   "metadata": {},
   "source": [
    "#### ì—­ì‚¬ë³„ ìŠ¹í•˜ì°¨ë°ì´í„°ë¡œ ì„ì‹œ journeys demand data ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "618ed86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì €ì¥ ì™„ë£Œ: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\data\\od-matrix\\network-od-matrix-with-journeys-fl.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === ê²½ë¡œ ì„¤ì • ===\n",
    "BASE_DIR = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\data\\od-matrix\"\n",
    "in_xlsx  = os.path.join(BASE_DIR, \"network-od-matrix-with-journeys.xlsx\")\n",
    "out_xlsx = os.path.join(BASE_DIR, \"network-od-matrix-with-journeys-fl.xlsx\")\n",
    "\n",
    "# === 1) ì—‘ì…€ ì½ê¸° ===\n",
    "df = pd.read_excel(in_xlsx)\n",
    "\n",
    "# === 2) node_path ì²˜ë¦¬ ===\n",
    "def get_first_last(path_str):\n",
    "    if pd.isna(path_str):\n",
    "        return None, None\n",
    "    parts = [p.strip() for p in str(path_str).split(\",\") if p.strip() != \"\"]\n",
    "    if not parts:\n",
    "        return None, None\n",
    "    if len(parts) == 1:\n",
    "        return parts[0], parts[0]\n",
    "    return parts[0], parts[-1]\n",
    "\n",
    "df[[\"start_node\", \"end_node\"]] = df[\"node_path\"].apply(\n",
    "    lambda x: pd.Series(get_first_last(x))\n",
    ")\n",
    "\n",
    "# === 3) ì €ì¥ (ì›ë³¸ ì—´ + ì¶”ê°€ ì—´ ê°™ì´) ===\n",
    "df.to_excel(out_xlsx, index=False)\n",
    "\n",
    "print(f\"ì €ì¥ ì™„ë£Œ: {out_xlsx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f5c43cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•„í„°ë§ ì™„ë£Œ: 105813ê°œ í–‰ ë‚¨ìŒ â†’ D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\data\\od-matrix\\network-od-matrix-with-journeys-fl-filtered.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === ê²½ë¡œ ì„¤ì • ===\n",
    "BASE_DIR = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\"\n",
    "od_xlsx   = os.path.join(BASE_DIR, \"data\", \"od-matrix\", \"network-od-matrix-with-journeys-fl.xlsx\")\n",
    "node_xlsx = os.path.join(BASE_DIR, \"json\", \"qgis_export_uk.xlsx\")\n",
    "out_xlsx  = os.path.join(BASE_DIR, \"data\", \"od-matrix\", \"network-od-matrix-with-journeys-fl-filtered.xlsx\")\n",
    "\n",
    "# === 1) ë°ì´í„° ì½ê¸° ===\n",
    "df = pd.read_excel(od_xlsx)\n",
    "nodes = pd.read_excel(node_xlsx)\n",
    "\n",
    "# === 2) wgs84_node_id ì§‘í•© ===\n",
    "valid_nodes = set(nodes[\"wgs84_node_id\"].astype(str))  # ë¬¸ìì—´ ë¹„êµë¥¼ ì•ˆì „í•˜ê²Œ\n",
    "\n",
    "# === 3) start_node / end_node ê°’ ë¹„êµ í›„ í•„í„°ë§ ===\n",
    "# start_node, end_nodeë„ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "df[\"start_node\"] = df[\"start_node\"].astype(str)\n",
    "df[\"end_node\"]   = df[\"end_node\"].astype(str)\n",
    "\n",
    "filtered = df[\n",
    "    (df[\"start_node\"].isin(valid_nodes)) | (df[\"end_node\"].isin(valid_nodes))\n",
    "]\n",
    "\n",
    "# === 4) ì €ì¥ ===\n",
    "filtered.to_excel(out_xlsx, index=False)\n",
    "\n",
    "print(f\"í•„í„°ë§ ì™„ë£Œ: {len(filtered)}ê°œ í–‰ ë‚¨ìŒ â†’ {out_xlsx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77df676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë§¤í•‘ ì™„ë£Œ: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\data\\od-matrix\\network-od-matrix-with-journeys-fl-filtered-mapped.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === ê²½ë¡œ ì„¤ì • ===\n",
    "BASE_DIR   = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\"\n",
    "od_xlsx    = os.path.join(BASE_DIR, \"data\", \"od-matrix\", \"network-od-matrix-with-journeys-fl-filtered.xlsx\")\n",
    "node_xlsx  = os.path.join(BASE_DIR, \"json\", \"qgis_export_uk.xlsx\")\n",
    "out_xlsx   = os.path.join(BASE_DIR, \"data\", \"od-matrix\", \"network-od-matrix-with-journeys-fl-filtered-mapped.xlsx\")\n",
    "\n",
    "# === 1) ë°ì´í„° ì½ê¸° ===\n",
    "df = pd.read_excel(od_xlsx)\n",
    "nodes = pd.read_excel(node_xlsx, sheet_name=\"NODE\")\n",
    "\n",
    "# === 2) ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ (wgs84_node_id -> node_id) ===\n",
    "mapping = dict(zip(nodes[\"wgs84_node_id\"].astype(str), nodes[\"node_id\"]))\n",
    "\n",
    "# === 3) start_node/end_node ë³€í™˜ ===\n",
    "df[\"o_node_id\"] = df[\"start_node\"].astype(str).map(mapping)\n",
    "df[\"d_node_id\"] = df[\"end_node\"].astype(str).map(mapping)\n",
    "\n",
    "# === 4) ì €ì¥ ===\n",
    "df.to_excel(out_xlsx, index=False)\n",
    "\n",
    "print(f\"ë§¤í•‘ ì™„ë£Œ: {out_xlsx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6238ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ë‘˜ ë‹¤ í¬í•¨ëœ ì—­ (matched):\n",
      " - ê²½ì‚°ì—­\n",
      " - ê³„ë£¡ì—­\n",
      " - ê³¡ì„±ì—­\n",
      " - ê³µì£¼ì—­\n",
      " - ê´‘ëª…ì—­\n",
      " - ê´‘ì£¼ì†¡ì •ì—­\n",
      " - ê´‘ì²œì—­\n",
      " - êµ¬ë¡€êµ¬ì—­\n",
      " - êµ¬ë¯¸ì—­\n",
      " - êµ¬í¬ì—­\n",
      " - ê¸°ì¥ì—­\n",
      " - ê¹€ì œì—­\n",
      " - ê¹€ì²œêµ¬ë¯¸ì—­\n",
      " - ê¹€ì²œì—­\n",
      " - ë‚˜ì£¼ì—­\n",
      " - ë‚¨ì›ì—­\n",
      " - ë…¼ì‚°ì—­\n",
      " - ëŠ¥ì£¼ì—­\n",
      " - ë‹¨ì–‘ì—­\n",
      " - ëŒ€êµ¬ì—­\n",
      " - ëŒ€ì „ì—­\n",
      " - ëŒ€ì²œì—­\n",
      " - ë•ì†Œì—­\n",
      " - ë™ëŒ€êµ¬ì—­\n",
      " - ë™í•´ì—­\n",
      " - ë§ˆì‚°ì—­\n",
      " - ëª©í¬ì—­\n",
      " - ë¬¼ê¸ˆì—­\n",
      " - ë°€ì–‘ì—­\n",
      " - ë²Œêµì—­\n",
      " - ë³´ì„±ì—­\n",
      " - ë´‰ì–‘ì—­\n",
      " - ë¶€ì‚°ì—­\n",
      " - ì‚¼ë‘ì§„ì—­\n",
      " - ìƒë´‰ì—­\n",
      " - ìƒì£¼ì—­\n",
      " - ì„œìš¸ì—­\n",
      " - ì„œì°½ì—­\n",
      " - ìˆ˜ì›ì—­\n",
      " - ìˆœì²œì—­\n",
      " - ì•ˆë™ì—­\n",
      " - ì–‘í‰ì—­\n",
      " - ì—¬ìˆ˜ì—‘ìŠ¤í¬ì—­\n",
      " - ì˜ë•ì—­\n",
      " - ì˜ë“±í¬ì—­\n",
      " - ì˜ì£¼ì—­\n",
      " - ì˜ì²œì—­\n",
      " - ì˜ˆì‚°ì—­\n",
      " - ì˜ˆì²œì—­\n",
      " - ì˜¤ì†¡ì—­\n",
      " - ì˜¨ì–‘ì˜¨ì²œì—­\n",
      " - ìš©ì‚°ì—­\n",
      " - ìš¸ì‚°ì—­\n",
      " - ì›ì£¼ì—­\n",
      " - ì˜ì„±ì—­\n",
      " - ìµì‚°ì—­\n",
      " - ì¥ì„±ì—­\n",
      " - ì¥í•­ì—­\n",
      " - ì „ì£¼ì—­\n",
      " - ì ì´Œì—­\n",
      " - ì •ë™ì§„ì—­\n",
      " - ì •ìì—­\n",
      " - ì œì²œì—­\n",
      " - ì§„ì£¼ì—­\n",
      " - ì²œì•ˆì—­\n",
      " - ì²­ëŸ‰ë¦¬ì—­\n",
      " - ì²­ì£¼ì—­\n",
      " - ì¶˜ì–‘ì—­\n",
      " - ì¶©ì£¼ì—­\n",
      " - íƒœí™”ê°•ì—­\n",
      " - í‰ì°½ì—­\n",
      " - í‰íƒì—­\n",
      " - í¬í•­ì—­\n",
      " - í’ê¸°ì—­\n",
      " - íš¡ì„±ì—­\n",
      "\n",
      "ğŸ“Œ QGISì—ëŠ” ìˆìœ¼ë‚˜ ìŠ¹í•˜ì°¨ ë°ì´í„°ì—ëŠ” ì—†ëŠ” ì—­ (only_in_qgis):\n",
      " - ë°±ì‚°ì—­\n",
      " - ë³´ì²œì—­\n",
      " - ì‚¼ì²™ì—­\n",
      " - ì‹ ê²½ì£¼ì—­\n",
      " - ì²œì•ˆì•„ì‚°ì—­(ì˜¨ì–‘ì˜¨ì²œ)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_dir = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\")\n",
    "qgis_path = base_dir / \"qgis_export_with_step.xlsx\"\n",
    "ridership_path = base_dir / \"station_ridership.xlsx\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. QGIS NODE ë°ì´í„°ì—ì„œ RLNODE_NM ì¶”ì¶œ\n",
    "qgis_df = pd.read_excel(qgis_path, sheet_name=\"NODE\")\n",
    "qgis_nodes = qgis_df[\"RLNODE_NM\"].astype(str).str.strip()\n",
    "\n",
    "# 2. ìŠ¹í•˜ì°¨ ë°ì´í„°ì—ì„œ RLNODE_NM ì¶”ì¶œ (Eì—´)\n",
    "ridership_df = pd.read_excel(ridership_path)\n",
    "ridership_nodes = ridership_df[\"RLNODE_NM\"].astype(str).str.strip()\n",
    "\n",
    "# 3. ë¹„êµ\n",
    "matched = sorted(set(qgis_nodes) & set(ridership_nodes))\n",
    "only_in_qgis = sorted(set(qgis_nodes) - set(ridership_nodes))\n",
    "\n",
    "# 4. ì¶œë ¥\n",
    "print(\"ğŸ“Œ ë‘˜ ë‹¤ í¬í•¨ëœ ì—­ (matched):\")\n",
    "for name in matched:\n",
    "    print(f\" - {name}\")\n",
    "\n",
    "print(\"\\nğŸ“Œ QGISì—ëŠ” ìˆìœ¼ë‚˜ ìŠ¹í•˜ì°¨ ë°ì´í„°ì—ëŠ” ì—†ëŠ” ì—­ (only_in_qgis):\")\n",
    "for name in only_in_qgis:\n",
    "    print(f\" - {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3eb01c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\demand_with_route_filled.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "base_path = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\")\n",
    "file_path = base_path / \"qgis_export.xlsx\"\n",
    "\n",
    "# ì—‘ì…€ íŒŒì¼ ì½ê¸°\n",
    "train_df = pd.read_excel(file_path, sheet_name=\"TRAIN\")\n",
    "demand_df = pd.read_excel(file_path, sheet_name=\"DEMAND\")\n",
    "\n",
    "# ë…¸ì„  ì´ë¦„ â†’ ë…¸ë“œ ê²½ë¡œ dict (n ì ‘ë‘ì–´ ë¶™ì´ê¸°)\n",
    "line_routes = {}\n",
    "for _, row in train_df.iterrows():\n",
    "    line_name = row[\"RLWAY_NM\"].strip()\n",
    "    node_seq = [f\"n{n.strip()}\" for n in str(row[\"NODE\"]).split(\"-\")]\n",
    "    line_routes[line_name] = node_seq\n",
    "\n",
    "# ë…¸ì„  ì´ë¦„ì—ì„œ ìˆ«ì ì œê±°í•´ì„œ base nameìœ¼ë¡œ ë§¤í•‘\n",
    "def get_base_line(line):\n",
    "    return ''.join(filter(lambda x: not x.isdigit(), line)).strip()\n",
    "\n",
    "# ODì— ëŒ€í•´ Route ìƒì„± í•¨ìˆ˜\n",
    "def extract_route(line, origin, destination):\n",
    "    base_line = get_base_line(line)\n",
    "    nodes = line_routes.get(base_line)\n",
    "    if not nodes:\n",
    "        return None\n",
    "    if origin not in nodes or destination not in nodes:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        idx_o = nodes.index(origin)\n",
    "        idx_d = nodes.index(destination)\n",
    "        # ì •ë°©í–¥ ë˜ëŠ” ì—­ë°©í–¥ ìŠ¬ë¼ì´ì‹±\n",
    "        if idx_o <= idx_d:\n",
    "            path = nodes[idx_o:idx_d + 1]\n",
    "        else:\n",
    "            path = nodes[idx_o:idx_d - 1:-1]  # ì—­ë°©í–¥\n",
    "        return \"-\".join(path)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Route ì—´ ì¶”ê°€\n",
    "demand_df[\"Route\"] = demand_df.apply(\n",
    "    lambda row: extract_route(row[\"Line\"], row[\"Origin\"], row[\"Destination\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ì €ì¥\n",
    "output_path = base_path / \"demand_with_route_filled.xlsx\"\n",
    "demand_df.to_excel(output_path, index=False)\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fe683ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ ì €ì¥ ì™„ë£Œ: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export_with_node_id.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_dir = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\")\n",
    "xlsx_path = base_dir / \"qgis_export.xlsx\"\n",
    "output_path = base_dir / \"qgis_export_with_node_id.xlsx\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. NODE ì‹œíŠ¸ ì½ê¸° (ì°¸ì¡°ìš©)\n",
    "node_df = pd.read_excel(xlsx_path, sheet_name=\"NODE\")\n",
    "node_df = node_df[[\"RLNODE_NM\", \"node_id\"]].dropna()\n",
    "node_df[\"RLNODE_NM\"] = node_df[\"RLNODE_NM\"].astype(str).str.strip()\n",
    "\n",
    "# 2. BOARD_ALIGHT ì‹œíŠ¸ì— node_id ë§¤ì¹­\n",
    "df = pd.read_excel(xlsx_path, sheet_name=\"BOARD_ALIGHT\")\n",
    "df[\"RLNODE_NM\"] = df[\"RLNODE_NM\"].astype(str).str.strip()\n",
    "\n",
    "# ë§¤ì¹­ ìˆ˜í–‰\n",
    "merged_df = df.merge(node_df, on=\"RLNODE_NM\", how=\"left\")\n",
    "\n",
    "# 3. ì €ì¥\n",
    "with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
    "    merged_df.to_excel(writer, sheet_name=\"BOARD_ALIGHT\", index=False)\n",
    "\n",
    "print(f\"âœ”ï¸ ì €ì¥ ì™„ë£Œ: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "639d0058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì™„ë£Œ: NODE ì‹œíŠ¸ì— board/alight ê°±ì‹ \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === ê²½ë¡œ ì„¤ì • ===\n",
    "BASE_DIR = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\"\n",
    "od_fp    = os.path.join(BASE_DIR, \"data\", \"od-matrix\", \"network-od-matrix-with-journeys-fl-filtered-mapped.xlsx\")\n",
    "node_fp  = os.path.join(BASE_DIR, \"json\", \"qgis_export_uk.xlsx\")\n",
    "\n",
    "# === 1) ë°ì´í„° ì½ê¸° ===\n",
    "od = pd.read_excel(od_fp)                       # o_node_id, d_node_id, journeys í¬í•¨\n",
    "nodes = pd.read_excel(node_fp, sheet_name=\"NODE\")  # node_id í¬í•¨\n",
    "\n",
    "# ì•ˆì „í•œ dtype ì •ë¦¬\n",
    "od[\"o_node_id\"] = pd.to_numeric(od[\"o_node_id\"], errors=\"coerce\")\n",
    "od[\"d_node_id\"] = pd.to_numeric(od[\"d_node_id\"], errors=\"coerce\")\n",
    "od[\"journeys\"]  = pd.to_numeric(od[\"journeys\"],  errors=\"coerce\").fillna(0)\n",
    "\n",
    "# === 2) ì§‘ê³„: o_node_id/d_node_idë³„ journeys í•© ===\n",
    "board_sum  = od.dropna(subset=[\"o_node_id\"]).groupby(\"o_node_id\")[\"journeys\"].sum()\n",
    "alight_sum = od.dropna(subset=[\"d_node_id\"]).groupby(\"d_node_id\")[\"journeys\"].sum()\n",
    "\n",
    "# === 3) NODE ì‹œíŠ¸ì— ë§¤í•‘ (ì—†ìœ¼ë©´ 0) ===\n",
    "nodes[\"board\"]  = nodes[\"node_id\"].map(board_sum).fillna(0)\n",
    "nodes[\"alight\"] = nodes[\"node_id\"].map(alight_sum).fillna(0)\n",
    "\n",
    "# ì •ìˆ˜ë¡œ ì›í•˜ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ\n",
    "# nodes[\"board\"]  = nodes[\"board\"].round(0).astype(int)\n",
    "# nodes[\"alight\"] = nodes[\"alight\"].round(0).astype(int)\n",
    "\n",
    "# === 4) ì €ì¥: NODE ì‹œíŠ¸ë§Œ êµì²´, ë‹¤ë¥¸ ì‹œíŠ¸ ë³´ì¡´ ===\n",
    "try:\n",
    "    # pandas>=1.4 ê¶Œì¥\n",
    "    with pd.ExcelWriter(node_fp, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as w:\n",
    "        nodes.to_excel(w, sheet_name=\"NODE\", index=False)\n",
    "except TypeError:\n",
    "    # if_sheet_exists ë¯¸ì§€ì› pandas ëŒ€ì‘: ì „ì²´ë¥¼ ìƒˆ íŒŒì¼ì— ì¨ì„œ êµì²´\n",
    "    xls = pd.ExcelFile(node_fp)\n",
    "    sheets = {sn: pd.read_excel(node_fp, sheet_name=sn) for sn in xls.sheet_names}\n",
    "    sheets[\"NODE\"] = nodes\n",
    "    with pd.ExcelWriter(node_fp, engine=\"openpyxl\", mode=\"w\") as w:\n",
    "        for sn, df in sheets.items():\n",
    "            df.to_excel(w, sheet_name=sn, index=False)\n",
    "\n",
    "print(\"ì™„ë£Œ: NODE ì‹œíŠ¸ì— board/alight ê°±ì‹ \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f8a6fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì™„ë£Œ. ê²°ê³¼ ì €ì¥: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\qgis_export_uk_with_journeys.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "INPUT_XLSX = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\qgis_export_uk.xlsx\"\n",
    "OUTPUT_XLSX = os.path.splitext(INPUT_XLSX)[0] + \"_with_journeys.xlsx\"\n",
    "\n",
    "# === helper: ì‹œíŠ¸ ìë™ íƒì§€ ===\n",
    "def find_sheet_with_columns(sheets_dict, required_cols):\n",
    "    for name, df in sheets_dict.items():\n",
    "        cols = {c.strip().lower() for c in df.columns}\n",
    "        if required_cols.issubset(cols):\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "def find_col(df, target):\n",
    "    t = target.strip().lower()\n",
    "    for c in df.columns:\n",
    "        if c.strip().lower() == t:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def norm_node(tok):\n",
    "    # 'n46' -> '46' ; if already numeric string, keep it\n",
    "    if pd.isna(tok): return \"\"\n",
    "    s = str(tok).strip()\n",
    "    if s.lower().startswith('n'):\n",
    "        return s[1:].strip()\n",
    "    return s\n",
    "\n",
    "# === 1) ì½ê¸°(ëª¨ë“  ì‹œíŠ¸) ===\n",
    "sheets = pd.read_excel(INPUT_XLSX, sheet_name=None, dtype=str, engine='openpyxl')\n",
    "\n",
    "# === 2) ì‹œíŠ¸ ì°¾ê¸° ===\n",
    "demand_sheet = find_sheet_with_columns(sheets, {'route','origin','destination'})\n",
    "node_sheet = find_sheet_with_columns(sheets, {'rlnode_nm','node_id','board','alight'})\n",
    "\n",
    "if demand_sheet is None or node_sheet is None:\n",
    "    raise RuntimeError(\"DEMAND ì‹œíŠ¸ ë˜ëŠ” NODE ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì»¬ëŸ¼ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "df_demand = sheets[demand_sheet].copy()\n",
    "df_node = sheets[node_sheet].copy()\n",
    "\n",
    "# ì»¬ëŸ¼ ì‹¤ì œ ì´ë¦„ ì°¾ê¸°\n",
    "route_col = find_col(df_demand, 'route')\n",
    "origin_col = find_col(df_demand, 'origin')\n",
    "dest_col = find_col(df_demand, 'destination')\n",
    "\n",
    "node_id_col = find_col(df_node, 'node_id')\n",
    "board_col = find_col(df_node, 'board')\n",
    "alight_col = find_col(df_node, 'alight')\n",
    "\n",
    "# ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜(ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "df_node[node_id_col] = df_node[node_id_col].astype(str).str.strip()\n",
    "df_node[board_col] = pd.to_numeric(df_node[board_col], errors='coerce').fillna(0.0)\n",
    "df_node[alight_col] = pd.to_numeric(df_node[alight_col], errors='coerce').fillna(0.0)\n",
    "\n",
    "# node dicts\n",
    "board_by_node = { str(r[node_id_col]).strip(): float(r[board_col]) for _, r in df_node.iterrows() }\n",
    "alight_by_node = { str(r[node_id_col]).strip(): float(r[alight_col]) for _, r in df_node.iterrows() }\n",
    "\n",
    "# === ì˜µì…˜: station_line_share ë§¤í•‘ (node_id -> fraction for this route)\n",
    "# ë§Œì•½ ì œê³µ ê°€ëŠ¥í•˜ë©´ routeë³„ë¡œ (ì˜ˆ: route identifier(ë˜ëŠ” Line))ì— ëŒ€í•´ stationë³„ ë¶„ë°°ë¹„ë¥¼ ì£¼ë©´ ë” ì •í™•í•¨.\n",
    "# ê¸°ë³¸: ëª¨ë“  board/alightë¥¼ í•´ë‹¹ routeì— ì „ë¶€ ì‚¬ìš© (ì¦‰ fraction=1.0)\n",
    "# ì‚¬ìš©ìê°€ ì œê³µí•˜ë©´ ì•„ë˜ dict í˜•íƒœ: station_share = {'46':0.3, '35':0.5, ...}\n",
    "# ì˜ˆ: station_share = {}  # ë¹„ì›Œë‘ë©´ 1.0ìœ¼ë¡œ ì²˜ë¦¬\n",
    "station_share = {}  # ì‚¬ìš©ìê°€ ì±„ì›Œ ë„£ìœ¼ë©´ ë¨\n",
    "\n",
    "def get_station_share(node_id):\n",
    "    # ë°˜í™˜: 0..1\n",
    "    return float(station_share.get(str(node_id).strip(), 1.0))\n",
    "\n",
    "# === ë°©ë²• A: ë¹„ë¡€í• ë‹¹ í•¨ìˆ˜ ===\n",
    "def journeys_proportional(route_tokens, origin_node, dest_node):\n",
    "    # route_tokens: ['n46','n35','n31',...]\n",
    "    nodes_after_origin = []\n",
    "    found_origin = False\n",
    "    for tok in route_tokens:\n",
    "        nid = norm_node(tok)\n",
    "        if not found_origin:\n",
    "            if nid == origin_node:\n",
    "                found_origin = True\n",
    "            continue\n",
    "        # origin found previously -> collect downstream nodes including first after origin\n",
    "        nodes_after_origin.append(nid)\n",
    "    # if destination not in downstream, return NaN\n",
    "    if dest_node not in nodes_after_origin or not nodes_after_origin:\n",
    "        return np.nan\n",
    "    # sum of alight over downstream nodes, with optional station_share\n",
    "    sum_alights = sum(alight_by_node.get(nid, 0.0) * get_station_share(nid) for nid in nodes_after_origin)\n",
    "    if sum_alights == 0:\n",
    "        return np.nan\n",
    "    board_origin = board_by_node.get(origin_node, 0.0) * get_station_share(origin_node)\n",
    "    dest_alight = alight_by_node.get(dest_node, 0.0) * get_station_share(dest_node)\n",
    "    return board_origin * (dest_alight / sum_alights)\n",
    "\n",
    "# === ë°©ë²• B: ê²½ë¡œ íë¦„ ì‹œë®¬ë ˆì´ì…˜ í•¨ìˆ˜ ===\n",
    "def simulate_route_od(route_tokens):\n",
    "    \"\"\"\n",
    "    route_tokens: ['n46','n35','n31', ...] (order of stops along the route)\n",
    "    returns: od_counts dict keyed by ('46','35') -> float journeys\n",
    "    \"\"\"\n",
    "    # normalize tokens to node ids\n",
    "    nodes = [norm_node(tok) for tok in route_tokens]\n",
    "    # active pools: list of (origin_node, remaining_count)\n",
    "    active = []  # list of dicts {'origin':id, 'rem':float}\n",
    "    od_counts = {}  # (origin, dest) -> float\n",
    "\n",
    "    for idx, nid in enumerate(nodes):\n",
    "        # 1) boarding at this station (scaled by station_share)\n",
    "        b = board_by_node.get(nid, 0.0) * get_station_share(nid)\n",
    "        if b > 0:\n",
    "            active.append({'origin': nid, 'rem': float(b)})\n",
    "\n",
    "        # 2) alighting at this station\n",
    "        al = alight_by_node.get(nid, 0.0) * get_station_share(nid)\n",
    "        if al <= 0 or not active:\n",
    "            continue\n",
    "\n",
    "        total_onboard = sum(a['rem'] for a in active)\n",
    "        # guard\n",
    "        if total_onboard <= 0:\n",
    "            continue\n",
    "\n",
    "        # allocate alighting proportionally to each active origin pool\n",
    "        for a in active:\n",
    "            if a['rem'] <= 0:\n",
    "                continue\n",
    "            share = a['rem'] / total_onboard\n",
    "            al_from_origin = share * al\n",
    "            # decrement\n",
    "            a['rem'] -= al_from_origin\n",
    "            key = (a['origin'], nid)\n",
    "            od_counts[key] = od_counts.get(key, 0.0) + al_from_origin\n",
    "\n",
    "        # drop any active pools that are now near-zero\n",
    "        active = [a for a in active if a['rem'] > 1e-9]\n",
    "\n",
    "    return od_counts\n",
    "\n",
    "# === 3) DEMAND í‘œì— ë‘ ë°©ë²• ì ìš© ===\n",
    "journeys_prop = []\n",
    "journeys_flow = []\n",
    "missing_info_prop = []\n",
    "missing_info_flow = []\n",
    "\n",
    "for _, row in df_demand.iterrows():\n",
    "    route = row.get(route_col, \"\")\n",
    "    if pd.isna(route) or str(route).strip() == \"\":\n",
    "        journeys_prop.append(np.nan)\n",
    "        journeys_flow.append(np.nan)\n",
    "        missing_info_prop.append(\"no route\")\n",
    "        missing_info_flow.append(\"no route\")\n",
    "        continue\n",
    "\n",
    "    tokens = [t.strip() for t in str(route).split('-') if t.strip() != \"\"]\n",
    "    origin_tok = norm_node(row.get(origin_col, \"\"))\n",
    "    dest_tok = norm_node(row.get(dest_col, \"\"))\n",
    "\n",
    "    # --- proportional ---\n",
    "    try:\n",
    "        j_prop = journeys_proportional(tokens, origin_tok, dest_tok)\n",
    "        if pd.isna(j_prop):\n",
    "            missing_info_prop.append(\"insufficient alight/board or dest not downstream\")\n",
    "        else:\n",
    "            missing_info_prop.append(\"\")\n",
    "    except Exception as e:\n",
    "        j_prop = np.nan\n",
    "        missing_info_prop.append(str(e))\n",
    "    journeys_prop.append(j_prop)\n",
    "\n",
    "    # --- flow simulation ---\n",
    "    try:\n",
    "        od_counts = simulate_route_od(tokens)\n",
    "        j_flow = od_counts.get((origin_tok, dest_tok), 0.0)\n",
    "        # if result is 0 but data suggests missing nodes, mark note\n",
    "        if j_flow == 0.0:\n",
    "            # detect if origin or dest absent in nodes\n",
    "            if origin_tok not in [norm_node(t) for t in tokens] or dest_tok not in [norm_node(t) for t in tokens]:\n",
    "                missing_info_flow.append(\"origin/dest missing in route\")\n",
    "            else:\n",
    "                # zero might be truly zero (alight zero etc.)\n",
    "                missing_info_flow.append(\"\")\n",
    "        else:\n",
    "            missing_info_flow.append(\"\")\n",
    "    except Exception as e:\n",
    "        j_flow = np.nan\n",
    "        missing_info_flow.append(str(e))\n",
    "    journeys_flow.append(j_flow)\n",
    "\n",
    "# attach to df_demand\n",
    "df_demand['journeys_prop'] = journeys_prop\n",
    "df_demand['journeys_flow'] = journeys_flow\n",
    "df_demand['note_prop'] = missing_info_prop\n",
    "df_demand['note_flow'] = missing_info_flow\n",
    "\n",
    "# write back (preserve other sheets)\n",
    "sheets_out = sheets.copy()\n",
    "sheets_out[demand_sheet] = df_demand\n",
    "\n",
    "with pd.ExcelWriter(OUTPUT_XLSX, engine='openpyxl', mode='w') as writer:\n",
    "    for name, df in sheets_out.items():\n",
    "        sheetname = name if len(name) <= 31 else name[:31]\n",
    "        df.to_excel(writer, sheet_name=sheetname, index=False)\n",
    "\n",
    "print(\"ì™„ë£Œ. ê²°ê³¼ ì €ì¥:\", OUTPUT_XLSX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d34b2",
   "metadata": {},
   "source": [
    "#### Journeys data ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df60ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Worksheet named 'BOARD_ALIGHT' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m eps_conv    = \u001b[32m1e-6\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 1. ë°ì´í„° ì½ê¸° -----------------------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m bal    = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBOARD_ALIGHT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m            \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnode_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mboard_1d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43malight_1d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m demand = pd.read_excel(file_path, sheet_name=\u001b[33m\"\u001b[39m\u001b[33mDEMAND\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 1â€‘a. Origin / Destination â‡’ ì •ìˆ˜ node_id\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\mjkang\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:508\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     data = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\mjkang\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1616\u001b[39m, in \u001b[36mExcelFile.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\n\u001b[32m   1577\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1578\u001b[39m     sheet_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1596\u001b[39m     **kwds,\n\u001b[32m   1597\u001b[39m ) -> DataFrame | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[32m   1598\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[33;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[32m   1600\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1614\u001b[39m \u001b[33;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[32m   1615\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\mjkang\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:773\u001b[39m, in \u001b[36mBaseExcelReader.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReading sheet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masheetname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(asheetname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     sheet = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43masheetname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume an integer if not a string\u001b[39;00m\n\u001b[32m    775\u001b[39m     sheet = \u001b[38;5;28mself\u001b[39m.get_sheet_by_index(asheetname)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\mjkang\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:582\u001b[39m, in \u001b[36mOpenpyxlReader.get_sheet_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_if_bad_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.book[name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\mjkang\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:624\u001b[39m, in \u001b[36mBaseExcelReader.raise_if_bad_sheet_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_if_bad_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sheet_names:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWorksheet named \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Worksheet named 'BOARD_ALIGHT' not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "file_path   = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export.xlsx\")\n",
    "output_path = file_path.with_stem(file_path.stem + \"_with_journeys\")\n",
    "\n",
    "min_board   = 1      # ìŠ¹ì°¨ê°€ 0/ëˆ„ë½ì¸ ì—­ì— ë¶€ì—¬í•  ìµœì†Œ ìŠ¹ì°¨ ì¸ì›\n",
    "min_alight  = 1      # í•˜ì°¨ê°€ 0/ëˆ„ë½ì¸ ì—­ì— ë¶€ì—¬í•  ìµœì†Œ í•˜ì°¨ ì¸ì›\n",
    "epsilon_od  = 1      # ìœ íš¨ OD(ìŠ¹Â·í•˜ì°¨>0)ì— ì‹¬ì„ ìµœì†Œ í†µí–‰ëŸ‰\n",
    "max_iter    = 50\n",
    "eps_conv    = 1e-6\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. ë°ì´í„° ì½ê¸° -----------------------------------------------------------\n",
    "bal    = pd.read_excel(\n",
    "            file_path, sheet_name=\"BOARD_ALIGHT\",\n",
    "            usecols=[\"node_id\", \"board_1d\", \"alight_1d\"]\n",
    "        )\n",
    "demand = pd.read_excel(file_path, sheet_name=\"DEMAND\")\n",
    "\n",
    "# 1â€‘a. Origin / Destination â‡’ ì •ìˆ˜ node_id\n",
    "demand[\"O_id\"] = demand[\"Origin\"].str.lstrip(\"n\").astype(int)\n",
    "demand[\"D_id\"] = demand[\"Destination\"].str.lstrip(\"n\").astype(int)\n",
    "\n",
    "# 2. BOARD_ALIGHT ëˆ„ë½/ì¤‘ë³µ ì²˜ë¦¬ ------------------------------------------\n",
    "#   2â€‘a. ì¤‘ë³µ node_id í•©ì‚°\n",
    "bal_agg = (bal.groupby(\"node_id\", as_index=False)\n",
    "               .agg(board=(\"board_1d\", \"sum\"),\n",
    "                    alight=(\"alight_1d\", \"sum\")))\n",
    "\n",
    "#   2â€‘b. DEMAND ì— ë“±ì¥í•˜ì§€ë§Œ bal ì— ì—†ëŠ” ë…¸ë“œ ì¶”ê°€\n",
    "all_nodes = pd.unique(demand[[\"O_id\", \"D_id\"]].values.ravel())\n",
    "bal_full  = pd.DataFrame({\"node_id\": all_nodes}).merge(\n",
    "                bal_agg, on=\"node_id\", how=\"left\"\n",
    "            )\n",
    "\n",
    "#   2â€‘c. ìŠ¹Â·í•˜ì°¨ 0/NaN â†’ ìµœì†Œê°’ ì£¼ì…\n",
    "bal_full[\"board\"]  = pd.to_numeric(bal_full[\"board\"],  errors=\"coerce\").fillna(0)\n",
    "bal_full[\"alight\"] = pd.to_numeric(bal_full[\"alight\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "bal_full.loc[bal_full[\"board\"]  == 0, \"board\"]  = min_board\n",
    "bal_full.loc[bal_full[\"alight\"] == 0, \"alight\"] = min_alight\n",
    "\n",
    "# 3. ì´ ìŠ¹Â·í•˜ì°¨ëŸ‰ ë§¤í•‘ ------------------------------------------------------\n",
    "board_map  = bal_full.set_index(\"node_id\")[\"board\"].to_dict()\n",
    "alight_map = bal_full.set_index(\"node_id\")[\"alight\"].to_dict()\n",
    "\n",
    "demand[\"o_total\"] = demand[\"O_id\"].map(board_map)\n",
    "demand[\"d_total\"] = demand[\"D_id\"].map(alight_map)\n",
    "\n",
    "# 4. ì´ˆê¸° OD í–‰ë ¬ + Îµ ì£¼ì… ---------------------------------------------------\n",
    "f = 1.0 / demand[\"Timestep\"].replace(0, np.nan)\n",
    "demand[\"T\"] = (demand[\"o_total\"] * demand[\"d_total\"] * f).fillna(0.0)\n",
    "\n",
    "valid_od = (demand[\"o_total\"] > 0) & (demand[\"d_total\"] > 0)\n",
    "demand.loc[valid_od, \"T\"] += epsilon_od      # ìµœì†Œ í†µí–‰ëŸ‰ ì‹¬ê¸°\n",
    "\n",
    "# 5. IPF (Iterative Proportional Fitting) ----------------------------------\n",
    "for _ in range(max_iter):\n",
    "    row_sum = demand.groupby(\"O_id\")[\"T\"].transform(\"sum\").replace(0, np.nan)\n",
    "    demand[\"T\"] *= demand[\"o_total\"] / row_sum\n",
    "\n",
    "    col_sum = demand.groupby(\"D_id\")[\"T\"].transform(\"sum\").replace(0, np.nan)\n",
    "    demand[\"T\"] *= demand[\"d_total\"] / col_sum\n",
    "\n",
    "    if max((row_sum - demand[\"o_total\"]).abs().max(),\n",
    "           (col_sum - demand[\"d_total\"]).abs().max()) < eps_conv:\n",
    "        break\n",
    "\n",
    "# 6. journeys ê³„ì‚° ë° í›„ì²˜ë¦¬ -------------------------------------------------\n",
    "demand[\"journeys\"] = demand[\"T\"].round(0).astype(int)   # ì •ìˆ˜(ëª…)ë¡œ ë°˜ì˜¬ë¦¼\n",
    "out = demand.drop(columns=[\"O_id\", \"D_id\", \"o_total\", \"d_total\", \"T\"])\n",
    "\n",
    "# 7. ê²°ê³¼ ì €ì¥ --------------------------------------------------------------\n",
    "if output_path.exists():\n",
    "    mode, sheet_opt = \"a\", \"overlay\"\n",
    "else:\n",
    "    mode, sheet_opt = \"w\", None                 # ìƒˆ íŒŒì¼ â€” ì‹œíŠ¸ ì¡´ì¬ ì•ˆ í•˜ë¯€ë¡œ ì˜µì…˜ ë¶ˆí•„ìš”\n",
    "\n",
    "with pd.ExcelWriter(output_path,\n",
    "                    engine=\"openpyxl\",\n",
    "                    mode=mode,\n",
    "                    if_sheet_exists=sheet_opt) as wr:\n",
    "    out.to_excel(wr, sheet_name=\"DEMAND\", index=False)\n",
    "\n",
    "print(\"âœ… journeys ì—´ì´ ì±„ì›Œì§„ íŒŒì¼:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b83a59b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modified file saved to: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export_journeys_fixed.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "file_path = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export.xlsx\")\n",
    "output_path = file_path.with_stem(file_path.stem + \"_journeys_fixed\")\n",
    "\n",
    "# DEMAND ì‹œíŠ¸ ì½ê¸°\n",
    "demand = pd.read_excel(file_path, sheet_name=\"DEMAND\")\n",
    "\n",
    "# journeys ì—´ì—ì„œ 0ì¸ ê°’ì„ 1ë¡œ ë³€ê²½\n",
    "demand[\"journeys\"] = demand[\"journeys\"].apply(lambda x: 1 if x == 0 else x)\n",
    "\n",
    "# ìƒˆ íŒŒì¼ë¡œ ì €ì¥\n",
    "with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
    "    demand.to_excel(writer, sheet_name=\"DEMAND\", index=False)\n",
    "\n",
    "print(f\"âœ… Modified file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d9a7c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ë…¸ì„ ë³„ ìµœëŒ€ íƒ‘ìŠ¹ ì¸ì› =====\n",
      "  Line      MaxLoad\n",
      " 1AWC1 62681.455319\n",
      " 1AWC2  2406.977529\n",
      " 1EMR1 53575.899146\n",
      " 1EMR2   957.062056\n",
      "  1GA1 63004.822775\n",
      "  1GA2  1607.179558\n",
      " 1GWR1 43215.627992\n",
      " 1GWR2  2646.727970\n",
      "1LNER1 71650.412764\n",
      "1LNER2  4253.587918\n",
      "  1SR1 33968.707205\n",
      "  1SR2  1601.238405\n",
      " 1TPE1  1422.917239\n",
      " 1TPE2  1747.935970\n",
      "  1TW1   768.871046\n",
      "  1TW2  2015.183499\n",
      " 2AWC1 79518.803233\n",
      " 2AWC2  1156.568444\n",
      " 2EMR1 70647.169439\n",
      " 2EMR2  2573.045776\n",
      " 2GWR1 37607.893574\n",
      " 2GWR2  1813.418767\n",
      "2LNER1 72429.739161\n",
      "2LNER2  4393.017267\n",
      "  2SR1 22464.011084\n",
      "  2SR2  1131.721169\n",
      "  2TW1  1785.686930\n",
      "  2TW2   968.922909\n",
      " 3EMR1 53157.661529\n",
      " 3EMR2   877.425438\n",
      " 3GWR1 27639.195702\n",
      " 3GWR2   701.036900\n",
      "  3SR1 46573.040225\n",
      "  3SR2  1023.326345\n",
      " 4EMR1 57122.331480\n",
      " 4EMR2  1393.145636\n",
      "\n",
      "===== ìµœëŒ€ íƒ‘ìŠ¹ Edge ìƒì„¸(ì„ íƒ) =====\n",
      "  Line From  To         Load\n",
      " 1AWC1  n46 n35 62681.455319\n",
      " 1AWC2  n15 n20  2406.977529\n",
      " 1EMR1  n46 n27 53575.899146\n",
      " 1EMR2  n22 n27   957.062056\n",
      "  1GA1  n46 n39 63004.822775\n",
      "  1GA2  n33 n36  1607.179558\n",
      " 1GWR1  n46 n45 43215.627992\n",
      " 1GWR2  n41 n42  2646.727970\n",
      "1LNER1  n46 n28 71650.412764\n",
      "1LNER2   n6  n8  4253.587918\n",
      "  1SR1  n46 n55 33968.707205\n",
      "  1SR2  n55 n46  1601.238405\n",
      " 1TPE1  n14 n13  1422.917239\n",
      " 1TPE2  n15 n13  1747.935970\n",
      "  1TW1  n41 n40   768.871046\n",
      "  1TW2  n41 n42  2015.183499\n",
      " 2AWC1  n46 n35 79518.803233\n",
      " 2AWC2  n21 n24  1156.568444\n",
      " 2EMR1  n46 n27 70647.169439\n",
      " 2EMR2  n15 n20  2573.045776\n",
      " 2GWR1  n46 n45 37607.893574\n",
      " 2GWR2  n48 n44  1813.418767\n",
      "2LNER1  n46 n28 72429.739161\n",
      "2LNER2   n6  n8  4393.017267\n",
      "  2SR1  n46 n55 22464.011084\n",
      "  2SR2  n55 n46  1131.721169\n",
      "  2TW1  n15 n25  1785.686930\n",
      "  2TW2  n43 n37   968.922909\n",
      " 3EMR1  n46 n27 53157.661529\n",
      " 3EMR2  n22 n27   877.425438\n",
      " 3GWR1  n46 n45 27639.195702\n",
      " 3GWR2  n52 n51   701.036900\n",
      "  3SR1  n46 n49 46573.040225\n",
      "  3SR2  n57 n51  1023.326345\n",
      " 4EMR1  n46 n27 57122.331480\n",
      " 4EMR2  n23 n22  1393.145636\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ì…ë ¥ ê²½ë¡œ ë° ì‹œíŠ¸ ì½ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "file_path = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\qgis_export_uk.xlsx\")\n",
    "df = pd.read_excel(file_path, sheet_name=\"DEMAND\",\n",
    "                   usecols=[\"Line\", \"Route\", \"journeys30\"])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. Edgeâ€‘ë³„ íƒ‘ìŠ¹ ì¸ì› ëˆ„ì  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "edge_load = defaultdict(int)           # {(Line, from, to): ì´íƒ‘ìŠ¹}\n",
    "\n",
    "for line, route, pax in df.itertuples(index=False):\n",
    "    nodes = route.split(\"-\")           # ex) [\"n4\",\"n3\",\"n1\",...]\n",
    "    for frm, to in zip(nodes[:-1], nodes[1:]):\n",
    "        edge_load[(line, frm, to)] += pax   # ë°©í–¥ êµ¬ë¶„ O (í•„ìš” ì—†ìœ¼ë©´ ì •ë ¬í•´ì„œ key í†µì¼)\n",
    "\n",
    "# DataFrame í™”\n",
    "edge_df = (pd.DataFrame([(l,f,t,c) for (l,f,t),c in edge_load.items()],\n",
    "                        columns=[\"Line\",\"From\",\"To\",\"Load\"]))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ë…¸ì„ ë³„ ìµœëŒ€ íƒ‘ìŠ¹ ì¸ì› ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "max_by_line = (edge_df.groupby(\"Line\")[\"Load\"]\n",
    "                        .agg(MaxLoad=\"max\")\n",
    "                        .reset_index())\n",
    "\n",
    "# (ì„ íƒ) ì–´ëŠ Edge ì—ì„œ ìµœëŒ€ê°€ ë‚˜ì™”ëŠ”ì§€ë„ ë³´ê³  ì‹¶ë‹¤ë©´:\n",
    "idx = edge_df.groupby(\"Line\")[\"Load\"].idxmax()\n",
    "edge_peak = edge_df.loc[idx].reset_index(drop=True)   # Line, From, To, Load\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. ê²°ê³¼ ì €ì¥ or ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n===== ë…¸ì„ ë³„ ìµœëŒ€ íƒ‘ìŠ¹ ì¸ì› =====\")\n",
    "print(max_by_line.to_string(index=False))\n",
    "\n",
    "print(\"\\n===== ìµœëŒ€ íƒ‘ìŠ¹ Edge ìƒì„¸(ì„ íƒ) =====\")\n",
    "print(edge_peak.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24b6a5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== í•„ìš” í¸ì„± ìˆ˜ ìš”ì•½ ===\n",
      "Line         Op          MaxLoad    Cap  #Trains Note\n",
      "1AWC1        AWC        62681.46    607      104 \n",
      "1AWC2        AWC         2406.98    607        4 \n",
      "1EMR1        EMR        53575.90    301      178 \n",
      "1EMR2        EMR          957.06    301        4 \n",
      "1GA1         GA         63004.82    704       90 \n",
      "1GA2         GA          1607.18    704        3 \n",
      "1GWR1        GWR        43215.63    650       67 \n",
      "1GWR2        GWR         2646.73    650        5 \n",
      "1LNER1       LNER       71650.41    600      120 \n",
      "1LNER2       LNER        4253.59    600        8 \n",
      "1SR1         SR         33968.71    330      103 \n",
      "1SR2         SR          1601.24    330        5 \n",
      "1TPE1        TPE         1422.92    647        3 \n",
      "1TPE2        TPE         1747.94    647        3 \n",
      "1TW1         TW           768.87    200        4 \n",
      "1TW2         TW          2015.18    200       11 \n",
      "2AWC1        AWC        79518.80    607      132 \n",
      "2AWC2        AWC         1156.57    607        2 \n",
      "2EMR1        EMR        70647.17    301      235 \n",
      "2EMR2        EMR         2573.05    301        9 \n",
      "2GWR1        GWR        37607.89    650       58 \n",
      "2GWR2        GWR         1813.42    650        3 \n",
      "2LNER1       LNER       72429.74    600      121 \n",
      "2LNER2       LNER        4393.02    600        8 \n",
      "2SR1         SR         22464.01    330       69 \n",
      "2SR2         SR          1131.72    330        4 \n",
      "2TW1         TW          1785.69    200        9 \n",
      "2TW2         TW           968.92    200        5 \n",
      "3EMR1        EMR        53157.66    301      177 \n",
      "3EMR2        EMR          877.43    301        3 \n",
      "3GWR1        GWR        27639.20    650       43 \n",
      "3GWR2        GWR          701.04    650        2 \n",
      "3SR1         SR         46573.04    330      142 \n",
      "3SR2         SR          1023.33    330        4 \n",
      "4EMR1        EMR        57122.33    301      190 \n",
      "4EMR2        EMR         1393.15    301        5 \n",
      "\n",
      "âœ… demand split JSON ì €ì¥ ì™„ë£Œ â†’ D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\\demand_split_by_capacity.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ë…¸ì„ ë³„ ìµœëŒ€íƒ‘ìŠ¹ê°’(ì£¼ì–´ì§„ ë¦¬ìŠ¤íŠ¸) + capacity_map -> í•„ìš”í•œ í¸ì„± ìˆ˜ ê³„ì‚°\n",
    "ê·¸ë¦¬ê³  demand.jsonì´ ìˆìœ¼ë©´ ë…¸ì„ ë³„ë¡œ í¸ì„± ìˆ˜ë§Œí¼ ê· ë“±ë¶„í• í•˜ì—¬ demand_split.jsonìœ¼ë¡œ ì €ì¥ (ì˜µì…˜).\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------------- user inputs ----------------\n",
    "# capacity_map: ì‚¬ìš©ìê°€ ì œê³µí•œ ê°’(ìˆ˜ì • ê¸ˆì§€)\n",
    "capacity_map = {\n",
    "    \"AWC\": 607,   # Avanti West Coast (Class 390 Pendolino ìµœëŒ€í˜• ê¸°ì¤€ ì˜ˆì‹œ)\n",
    "    \"EMR\": 301,   # East Midlands Railway (Class 810 ë“± ì˜ˆì‹œ)\n",
    "    \"GA\" : 704,   # Greater Anglia (Class 745 12-car ì˜ˆì‹œ)\n",
    "    \"GWR\": 650,   # Great Western Railway (Class 800/802 ëŒ€í˜• í¸ì„± ì˜ˆì‹œ)\n",
    "    \"LNER\":600,   # LNER (Azuma ëŒ€í˜• í¸ì„± ì˜ˆì‹œ)\n",
    "    \"SR\": 330,    # Southern (Class 377 ë“± 4~5-car ê¸°ì¤€ ì˜ˆì‹œ)\n",
    "    \"TPE\":647,    # TransPennine Express (ëŒ€í˜• í¸ì„± ì˜ˆì‹œ)\n",
    "    \"TW\": 200,    # Transport for Wales (TfW, 2~3-car ì˜ˆì‹œ)\n",
    "    # í•„ìš”í•˜ë©´ ì—¬ê¸°ì— ì¶”ê°€ ë§¤í•‘\n",
    "}\n",
    "DEFAULT_CAPACITY = 600   # ë§¤í•‘ ì—†ì„ ë•Œ ë³´ìˆ˜ì  ê¸°ë³¸ê°’\n",
    "\n",
    "# ì£¼ì–´ì§„ ë…¸ì„ ë³„ ìµœëŒ€íƒ‘ìŠ¹ê°’ (ì‚¬ìš©ìê°€ ì œê³µí•œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ dictë¡œ ë§Œë“¦)\n",
    "# ë¬¸ìì—´ í‚¤ëŠ” ì•/ë’¤ ìˆ«ìë¥¼ í¬í•¨í•œ ì›ë³¸ ë…¸ì„ ëª…(ì˜ˆ: '1AWC1')\n",
    "max_loads_raw = {\n",
    "\"1AWC1\": 62681.455319,\n",
    "\"1AWC2\": 2406.977529,\n",
    "\"1EMR1\": 53575.899146,\n",
    "\"1EMR2\": 957.062056,\n",
    "\"1GA1\": 63004.822775,\n",
    "\"1GA2\": 1607.179558,\n",
    "\"1GWR1\": 43215.627992,\n",
    "\"1GWR2\": 2646.727970,\n",
    "\"1LNER1\": 71650.412764,\n",
    "\"1LNER2\": 4253.587918,\n",
    "\"1SR1\": 33968.707205,\n",
    "\"1SR2\": 1601.238405,\n",
    "\"1TPE1\": 1422.917239,\n",
    "\"1TPE2\": 1747.935970,\n",
    "\"1TW1\": 768.871046,\n",
    "\"1TW2\": 2015.183499,\n",
    "\"2AWC1\": 79518.803233,\n",
    "\"2AWC2\": 1156.568444,\n",
    "\"2EMR1\": 70647.169439,\n",
    "\"2EMR2\": 2573.045776,\n",
    "\"2GWR1\": 37607.893574,\n",
    "\"2GWR2\": 1813.418767,\n",
    "\"2LNER1\": 72429.739161,\n",
    "\"2LNER2\": 4393.017267,\n",
    "\"2SR1\": 22464.011084,\n",
    "\"2SR2\": 1131.721169,\n",
    "\"2TW1\": 1785.686930,\n",
    "\"2TW2\": 968.922909,\n",
    "\"3EMR1\": 53157.661529,\n",
    "\"3EMR2\": 877.425438,\n",
    "\"3GWR1\": 27639.195702,\n",
    "\"3GWR2\": 701.036900,\n",
    "\"3SR1\": 46573.040225,\n",
    "\"3SR2\": 1023.326345,\n",
    "\"4EMR1\": 57122.331480,\n",
    "\"4EMR2\": 1393.145636,\n",
    "}\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ (demand.json ë¶ˆëŸ¬ì™€ ë¶„í• í•˜ë ¤ë©´ ê²½ë¡œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •)\n",
    "JSON_IN  = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\\demand.json\")\n",
    "JSON_OUT = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\\demand_split_by_capacity.json\")\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def extract_letters(s: str) -> str:\n",
    "    \"\"\"'1AWC1' -> 'AWC' ì²˜ëŸ¼ ì—°ì†ëœ ì˜ë¬¸ì ê·¸ë£¹ ì¤‘ ì²«ë²ˆì§¸ë¥¼ ëŒ€ë¬¸ìë¡œ ë°˜í™˜\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    m = re.search(r\"[A-Za-z]+\", s)\n",
    "    return m.group(0).upper() if m else \"\"\n",
    "\n",
    "# ---------------- main ê³„ì‚° ----------------\n",
    "results = []\n",
    "train_count_map = {}   # key: original line string -> n_trains\n",
    "capacity_used_map = {} # key: original line -> capacity used\n",
    "\n",
    "for line_key, maxload in max_loads_raw.items():\n",
    "    op = extract_letters(line_key)          # e.g., '1AWC1' -> 'AWC'\n",
    "    if not op:\n",
    "        cap = DEFAULT_CAPACITY\n",
    "        note = \"no_letters_found -> default cap used\"\n",
    "    else:\n",
    "        cap = capacity_map.get(op, DEFAULT_CAPACITY)\n",
    "        note = \"\" if op in capacity_map else f\"operator '{op}' not in capacity_map -> default cap used\"\n",
    "    # í•„ìš”í•œ í¸ì„± ìˆ˜ (ì˜¬ë¦¼), ìµœì†Œ 1\n",
    "    n_trains = max(1, math.ceil(float(maxload) / float(cap))) if cap > 0 else 1\n",
    "    results.append((line_key, op, float(maxload), int(cap), int(n_trains), note))\n",
    "    train_count_map[line_key] = int(n_trains)\n",
    "    capacity_used_map[line_key] = int(cap)\n",
    "\n",
    "# ì¶œë ¥(ì •ë ¬í•´ì„œ ë³´ê¸° ì¢‹ê²Œ)\n",
    "print(\"\\n=== í•„ìš” í¸ì„± ìˆ˜ ìš”ì•½ ===\")\n",
    "print(f\"{'Line':12s} {'Op':6s} {'MaxLoad':>12s} {'Cap':>6s} {'#Trains':>8s} {'Note'}\")\n",
    "for t in sorted(results, key=lambda x: x[0]):\n",
    "    line_key, op, maxload, cap, n_trains, note = t\n",
    "    print(f\"{line_key:12s} {op:6s} {maxload:12.2f} {cap:6d} {n_trains:8d} {note}\")\n",
    "\n",
    "# ---------------- optional: demand.json ë¶„í•  ----------------\n",
    "split_js = {}\n",
    "if JSON_IN.exists():\n",
    "    with open(JSON_IN, \"r\", encoding=\"utf-8\") as f:\n",
    "        demand_js = json.load(f)\n",
    "    for ln, od_list in demand_js.items():\n",
    "        # ì‚¬ìš©ìê°€ ì›í•˜ë©´ ln ìì²´(ì˜ˆ: '1AWC1')ë¥¼ í‚¤ë¡œ ì‚¬ìš©\n",
    "        n_train = train_count_map.get(ln, 1)  # ë§¤í•‘ ì—†ìœ¼ë©´ 1ëŒ€\n",
    "        # create keys\n",
    "        for idx in range(n_train):\n",
    "            split_js[f\"{ln}_{idx+1}\"] = []\n",
    "        # distribute OD trips evenly (rounded to 2 decimals)\n",
    "        for od in od_list:\n",
    "            # od expected format: [o, d, j]\n",
    "            if len(od) < 3:\n",
    "                continue\n",
    "            o, d, j = od[0], od[1], float(od[2])\n",
    "            share = round(j / n_train, 2)\n",
    "            for idx in range(n_train):\n",
    "                split_js[f\"{ln}_{idx+1}\"].append([o, d, share])\n",
    "\n",
    "    # ì €ì¥\n",
    "    JSON_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(JSON_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"{\\n\")\n",
    "        for i, (line, trips) in enumerate(split_js.items()):\n",
    "            line_str = json.dumps(line, ensure_ascii=False)\n",
    "            trips_str = json.dumps(trips, ensure_ascii=False, separators=(\",\", \":\"))\n",
    "            comma = \",\" if i < len(split_js) - 1 else \"\"\n",
    "            f.write(f\"  {line_str}: {trips_str}{comma}\\n\")\n",
    "        f.write(\"}\\n\")\n",
    "    print(f\"\\nâœ… demand split JSON ì €ì¥ ì™„ë£Œ â†’ {JSON_OUT}\")\n",
    "else:\n",
    "    print(f\"\\nì£¼ì˜: {JSON_IN} íŒŒì¼ì´ ì—†ì–´ì„œ demand ë¶„í• ì€ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ---------------- ë ----------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82cda35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\dep_time_uk.json\n",
      "ì´ ì¶œë°œ ì´ë²¤íŠ¸ ìˆ˜: 229\n",
      "1AWC1_1 : 0\n",
      "1AWC1_2 : 3\n",
      "1AWC1_3 : 6\n",
      "1AWC1_4 : 9\n",
      "1AWC1_5 : 12\n",
      "1AWC1_6 : 15\n",
      "1AWC1_7 : 18\n",
      "1AWC1_8 : 21\n",
      "1AWC1_9 : 24\n",
      "1AWC1_10 : 27\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 0) ì €ì¥ ê²½ë¡œ\n",
    "OUT_DIR = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\")\n",
    "OUT_PATH = OUT_DIR / \"dep_time_uk.json\"\n",
    "\n",
    "# 1) ë¼ì¸ë³„ ë°°ì°¨ê°„ê²©(ë¶„)\n",
    "headway = {\n",
    "  \"1AWC1\": 3, \"1AWC2\": 18, \"1EMR1\": 3, \"1EMR2\": 18, \"1GA1\": 3, \"1GA2\": 18,\n",
    "  \"1GWR1\": 3, \"1GWR2\": 9, \"1LNER1\": 3, \"1LNER2\": 9, \"1SR1\": 3, \"1SR2\": 9,\n",
    "  \"1TPE1\": 18, \"1TPE2\": 18, \"1TW1\": 18, \"1TW2\": 9, \"2AWC1\":3, \"2AWC2\": 18,\n",
    "  \"2EMR1\": 3, \"2EMR2\": 9, \"2GWR1\": 3, \"2GWR2\": 18, \"2LNER1\": 3, \"2LNER2\": 9,\n",
    "  \"2SR1\": 3, \"2SR2\": 18, \"2TW1\": 9, \"2TW2\": 9, \"3EMR1\": 3, \"3EMR2\": 18,\n",
    "  \"3GWR1\": 9, \"3GWR2\": 18, \"3SR1\": 3, \"3SR2\": 18, \"4EMR1\": 3, \"4EMR2\": 9\n",
    "}\n",
    "\n",
    "# 2) ë¼ì¸ë³„ ì´ ì†Œìš”ì‹œê°„(ë¶„)\n",
    "trip_time = {\n",
    "  \"1AWC1\":31, \"1AWC2\":31, \"1EMR1\":22, \"1EMR2\":22, \"1GA1\":38, \"1GA2\":38,\n",
    "  \"1GWR1\":31, \"1GWR2\":31, \"1LNER1\":59, \"1LNER2\":59, \"1SR1\":54, \"1SR2\":54,\n",
    "  \"1TPE1\":31, \"1TPE2\":31, \"1TW1\":34, \"1TW2\":34, \"2AWC1\":52, \"2AWC2\":52,\n",
    "  \"2EMR1\":35, \"2EMR2\":35, \"2GWR1\":46, \"2GWR2\":46, \"2LNER1\":79, \"2LNER2\":79,\n",
    "  \"2SR1\":25, \"2SR2\":25, \"2TW1\":43, \"2TW2\":43, \"3EMR1\":29, \"3EMR2\":29,\n",
    "  \"3GWR1\":35, \"3GWR2\":35, \"3SR1\":25, \"3SR2\":25, \"4EMR1\":34, \"4EMR2\":34\n",
    "}\n",
    "\n",
    "# 3) ì´ ì‹œë®¬ë ˆì´ì…˜ ê¸¸ì´(ë¶„ ë‹¨ìœ„ timestep)\n",
    "TOTAL = 72\n",
    "\n",
    "# 4) ì¶œë°œì‹œê° ìƒì„± (ë„ì°© ì œí•œ: t + trip_time[line] <= TOTAL)\n",
    "dep_map = {}\n",
    "for line in sorted(headway.keys()):\n",
    "    h = int(headway[line])\n",
    "    tt = int(trip_time.get(line, 0))   # trip_timeì— ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ê°„ì£¼(=ì œì•½ ì—†ìŒ)\n",
    "    if h <= 0:\n",
    "        continue\n",
    "\n",
    "    # ì¶œë°œ ê°€ëŠ¥í•œ ë§ˆì§€ë§‰ ì‹œê°(í¬í•¨): t <= TOTAL - tt\n",
    "    last_start = TOTAL - tt\n",
    "    if last_start < 0:\n",
    "        # ì´ë™ì‹œê°„ì´ TOTALë³´ë‹¤ ê¸¸ë©´ í•´ë‹¹ ë¼ì¸ì€ ì¶œë°œ ì—†ìŒ\n",
    "        continue\n",
    "\n",
    "    # 0, h, 2h, ... <= last_start\n",
    "    times = list(range(0, last_start + 1, h))\n",
    "    for i, t in enumerate(times, start=1):\n",
    "        dep_map[f\"{line}_{i}\"] = t\n",
    "\n",
    "# 5) ì €ì¥\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dep_map, f, ensure_ascii=False, indent=2, separators=(\",\", \": \"))\n",
    "\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {OUT_PATH}\")\n",
    "print(f\"ì´ ì¶œë°œ ì´ë²¤íŠ¸ ìˆ˜: {len(dep_map):,}\")\n",
    "# ìƒ˜í”Œ ëª‡ ê°œ í™•ì¸\n",
    "for k in list(dep_map.keys())[:10]:\n",
    "    print(k, \":\", dep_map[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1354bb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\routes_nodes_uk.json\n",
      "ì´ ìƒì„± í•­ëª© ìˆ˜: 229\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TRAIN ì‹œíŠ¸(RLWAY_NM, NODE) + dep_time_uk.json -> routes_nodes_uk.json\n",
    "- NODE: \"1-2-3-4\"  -> [\"n1\",\"n2\",\"n3\",\"n4\"] ë¡œ ë³€í™˜\n",
    "- dep_time_ukì˜ ê° í‚¤(ì˜ˆ: \"1ABC1_2\")ì— ëŒ€í•´ ë² ì´ìŠ¤(\"1ABC1\")ë¥¼ ë½‘ì•„ í•´ë‹¹ ë…¸ë“œì—´ì„ ë§¤í•‘\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE_DIR = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\")\n",
    "XLSX_PATH = BASE_DIR / \"qgis_export_uk.xlsx\"\n",
    "DEP_JSON  = BASE_DIR / \"dep_time_uk.json\"\n",
    "OUT_JSON  = BASE_DIR / \"routes_nodes_uk.json\"\n",
    "TRAIN_SHEET = \"TRAIN\"   # ì‹œíŠ¸ëª…ì´ ë‹¤ë¥´ë©´ ìˆ˜ì •\n",
    "\n",
    "# â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def to_nodes_list(node_str: str):\n",
    "    \"\"\"\n",
    "    \"46-35-31\" -> [\"n46\",\"n35\",\"n31\"]\n",
    "    ìˆ«ì/ê³µë°±/ì†Œìˆ˜í‘œê¸° ë“± ìœ ì—° ì²˜ë¦¬\n",
    "    \"\"\"\n",
    "    if node_str is None:\n",
    "        return []\n",
    "    parts = str(node_str).strip().split(\"-\")\n",
    "    nodes = []\n",
    "    for p in parts:\n",
    "        p = str(p).strip()\n",
    "        if p == \"\":\n",
    "            continue\n",
    "        # \"46.0\" ê°™ì€ ê²½ìš° ì •ìˆ˜ ë¬¸ìì—´ë¡œ\n",
    "        try:\n",
    "            if re.match(r\"^\\d+(\\.0+)?$\", p):\n",
    "                p_int = int(float(p))\n",
    "                nodes.append(f\"n{p_int}\")\n",
    "            else:\n",
    "                # ìˆ«ì ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë¶™ì´ë˜ ì ‘ë‘ 'n'ë§Œ\n",
    "                # (í•„ìš”ì‹œ ë” ì—„ê²©íˆ í•„í„°ë§)\n",
    "                nodes.append(f\"n{p}\")\n",
    "        except Exception:\n",
    "            nodes.append(f\"n{p}\")\n",
    "    return nodes\n",
    "\n",
    "def base_name(dep_key: str) -> str:\n",
    "    \"\"\"'1EMR1_3' -> '1EMR1'\"\"\"\n",
    "    return dep_key.rsplit(\"_\", 1)[0] if \"_\" in dep_key else dep_key\n",
    "\n",
    "# â”€â”€ ì…ë ¥ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if not XLSX_PATH.exists():\n",
    "    raise FileNotFoundError(f\"ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {XLSX_PATH}\")\n",
    "if not DEP_JSON.exists():\n",
    "    raise FileNotFoundError(f\"dep_time_uk.jsonì´ ì—†ìŠµë‹ˆë‹¤: {DEP_JSON}\")\n",
    "\n",
    "# TRAIN ì‹œíŠ¸ ì½ê¸°\n",
    "train_df = pd.read_excel(XLSX_PATH, sheet_name=TRAIN_SHEET, dtype=str, engine=\"openpyxl\")\n",
    "train_df.columns = [c.strip() for c in train_df.columns]\n",
    "\n",
    "need_cols = {\"RLWAY_NM\", \"NODE\"}\n",
    "missing = need_cols - set(train_df.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"TRAIN ì‹œíŠ¸ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}\")\n",
    "\n",
    "# RLWAY_NM -> [\"n..\",\"n..\",...] ë§¤í•‘\n",
    "map_line_to_nodes = {}\n",
    "for line, nodes in train_df[[\"RLWAY_NM\",\"NODE\"]].itertuples(index=False):\n",
    "    if line is None:\n",
    "        continue\n",
    "    line_key = str(line).strip()\n",
    "    nodes_list = to_nodes_list(nodes)\n",
    "    if nodes_list:\n",
    "        map_line_to_nodes[line_key] = nodes_list\n",
    "\n",
    "# dep_time_uk.json ì½ê¸°\n",
    "with DEP_JSON.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    dep_map = json.load(f)  # {\"1AWC1_1\": 0, ...}\n",
    "\n",
    "# â”€â”€ ë³€í™˜: dep í‚¤ë§ˆë‹¤ ë…¸ë“œ ì‹œí€€ìŠ¤ ë¶€ì—¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "routes_nodes = {}\n",
    "missing_bases = set()\n",
    "\n",
    "# dep_mapì˜ í‚¤ ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ë„ë¡ ë°˜ë³µ\n",
    "for dep_key in dep_map.keys():\n",
    "    b = base_name(dep_key)\n",
    "    nodes = map_line_to_nodes.get(b)\n",
    "    if nodes is None:\n",
    "        missing_bases.add(b)\n",
    "        continue\n",
    "    # ì–•ì€ ë³µì‚¬(ë¦¬ìŠ¤íŠ¸ ë³´í˜¸)\n",
    "    routes_nodes[dep_key] = list(nodes)\n",
    "\n",
    "# â”€â”€ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUT_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(routes_nodes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {OUT_JSON}\")\n",
    "print(f\"ì´ ìƒì„± í•­ëª© ìˆ˜: {len(routes_nodes):,}\")\n",
    "if missing_bases:\n",
    "    print(\"âš  ë‹¤ìŒ ë² ì´ìŠ¤ ë¼ì¸ì€ TRAIN ì‹œíŠ¸ì—ì„œ ì°¾ì§€ ëª»í•´ ìŠ¤í‚µí–ˆìŠµë‹ˆë‹¤:\")\n",
    "    for b in sorted(missing_bases):\n",
    "        print(\"  -\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "43022e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\demand_uk_2.json\n",
      "ì´ ìƒì„± í‚¤ ìˆ˜: 270\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\")\n",
    "DEP_PATH    = BASE / \"dep_time_uk.json\"   # {\"1AWC1_1\": 0, \"1AWC1_2\": 3, ...}\n",
    "DEMAND_PATH = BASE / \"demand_uk.json\"     # {\"1AWC1\": [[o,d,val], ...], ...}\n",
    "OUT_PATH    = BASE / \"demand_uk_2.json\"   # ê²°ê³¼ ì €ì¥\n",
    "\n",
    "# ---- ì…ë ¥ ë¡œë“œ ----\n",
    "if not DEP_PATH.exists():\n",
    "    raise FileNotFoundError(f\"dep_time_uk.jsonì´ ì—†ìŠµë‹ˆë‹¤: {DEP_PATH}\")\n",
    "if not DEMAND_PATH.exists():\n",
    "    raise FileNotFoundError(f\"demand_uk.jsonì´ ì—†ìŠµë‹ˆë‹¤: {DEMAND_PATH}\")\n",
    "\n",
    "with DEP_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    dep_map = json.load(f)\n",
    "with DEMAND_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    demand_map = json.load(f)\n",
    "\n",
    "# ---- dep í‚¤ë¥¼ (ë² ì´ìŠ¤, ë²ˆí˜¸)ë¡œ ë¶„í•´ í›„ ì •ë ¬ ----\n",
    "# ì˜ˆ: \"1AWC1_3\" -> base=\"1AWC1\", idx=3\n",
    "def split_base_idx(k: str):\n",
    "    if \"_\" not in k:\n",
    "        return k, None\n",
    "    base, idx = k.rsplit(\"_\", 1)\n",
    "    # ìˆ«ì ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ None\n",
    "    try:\n",
    "        idx_num = int(idx)\n",
    "    except Exception:\n",
    "        idx_num = None\n",
    "    return base, idx_num\n",
    "\n",
    "# ë² ì´ìŠ¤ë³„ë¡œ í‚¤ ëª¨ìœ¼ê³ , ë²ˆí˜¸ë¡œ ì •ë ¬\n",
    "grouped = defaultdict(list)\n",
    "for k in dep_map.keys():\n",
    "    base, idx = split_base_idx(k)\n",
    "    grouped[base].append((k, idx))\n",
    "\n",
    "for base in grouped:\n",
    "    grouped[base].sort(key=lambda x: (x[1] is None, x[1]))  # ìˆ«ì ìˆëŠ” ê²ƒ ìš°ì„ , ìˆ«ì ì˜¤ë¦„ì°¨ìˆœ\n",
    "\n",
    "# ---- ê²°ê³¼ out ìƒì„±: ê° dep í‚¤ë§ˆë‹¤ demandì˜ base OD ë³µì‚¬ ----\n",
    "out = {}\n",
    "missing_bases = []\n",
    "for base, items in sorted(grouped.items(), key=lambda x: x[0]):\n",
    "    od_list = demand_map.get(base)\n",
    "    if od_list is None:\n",
    "        missing_bases.append(base)\n",
    "        continue\n",
    "    for k, _ in items:\n",
    "        # ì–•ì€ ë³µì‚¬ë¡œ ì¶©ë¶„ (ì›ì†ŒëŠ” ë¦¬ìŠ¤íŠ¸ of [o,d,val])\n",
    "        out[k] = [list(od) for od in od_list]\n",
    "\n",
    "# ---- ì €ì¥: í‚¤ë§ˆë‹¤ í•œ ì¤„(ì—”í„°) ----\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "all_keys_ordered = []\n",
    "for base, items in sorted(grouped.items(), key=lambda x: x[0]):\n",
    "    if base in missing_bases:\n",
    "        continue\n",
    "    for k, _ in items:\n",
    "        if k in out:\n",
    "            all_keys_ordered.append(k)\n",
    "\n",
    "with OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"{\\n\")\n",
    "    for i, k in enumerate(all_keys_ordered):\n",
    "        k_json = json.dumps(k, ensure_ascii=False)\n",
    "        v_json = json.dumps(out[k], ensure_ascii=False, separators=(\",\", \":\"))\n",
    "        comma = \",\" if i < len(all_keys_ordered) - 1 else \"\"\n",
    "        # í‚¤ë§ˆë‹¤ í•œ ì¤„ì”© ê¸°ë¡(ìˆ«ì ë‹¤ë¥¸ ê²ƒë¼ë¦¬ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì—”í„°)\n",
    "        f.write(f\"  {k_json}: {v_json}{comma}\\n\")\n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {OUT_PATH}\")\n",
    "print(f\"ì´ ìƒì„± í‚¤ ìˆ˜: {len(all_keys_ordered):,}\")\n",
    "if missing_bases:\n",
    "    print(\"âš  ë‹¤ìŒ ë² ì´ìŠ¤ ë¼ì¸ì€ demand_uk.jsonì— ì—†ì–´ ìŠ¤í‚µë˜ì—ˆìŠµë‹ˆë‹¤:\")\n",
    "    for b in sorted(missing_bases):\n",
    "        print(\"  -\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a88894f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\routes_nodes_uk.json\n",
      "ì´ ìƒì„± í•­ëª© ìˆ˜: 270\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TRAIN ì‹œíŠ¸(RLWAY_NM, NODE) + dep_time_uk.json -> routes_nodes_uk.json\n",
    "- NODE: \"1-2-3-4\"  -> [\"n1\",\"n2\",\"n3\",\"n4\"] ë¡œ ë³€í™˜\n",
    "- dep_time_ukì˜ ê° í‚¤(ì˜ˆ: \"1ABC1_2\")ì— ëŒ€í•´ ë² ì´ìŠ¤(\"1ABC1\")ë¥¼ ë½‘ì•„ í•´ë‹¹ ë…¸ë“œì—´ì„ ë§¤í•‘\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE_DIR = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\")\n",
    "XLSX_PATH = BASE_DIR / \"qgis_export_uk.xlsx\"\n",
    "DEP_JSON  = BASE_DIR / \"dep_time_uk.json\"\n",
    "OUT_JSON  = BASE_DIR / \"routes_nodes_uk.json\"\n",
    "TRAIN_SHEET = \"TRAIN\"   # ì‹œíŠ¸ëª…ì´ ë‹¤ë¥´ë©´ ìˆ˜ì •\n",
    "\n",
    "# â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def to_nodes_list(node_str: str):\n",
    "    \"\"\"\n",
    "    \"46-35-31\" -> [\"n46\",\"n35\",\"n31\"]\n",
    "    ìˆ«ì/ê³µë°±/ì†Œìˆ˜í‘œê¸° ë“± ìœ ì—° ì²˜ë¦¬\n",
    "    \"\"\"\n",
    "    if node_str is None:\n",
    "        return []\n",
    "    parts = str(node_str).strip().split(\"-\")\n",
    "    nodes = []\n",
    "    for p in parts:\n",
    "        p = str(p).strip()\n",
    "        if p == \"\":\n",
    "            continue\n",
    "        # \"46.0\" ê°™ì€ ê²½ìš° ì •ìˆ˜ ë¬¸ìì—´ë¡œ\n",
    "        try:\n",
    "            if re.match(r\"^\\d+(\\.0+)?$\", p):\n",
    "                p_int = int(float(p))\n",
    "                nodes.append(f\"n{p_int}\")\n",
    "            else:\n",
    "                # ìˆ«ì ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë¶™ì´ë˜ ì ‘ë‘ 'n'ë§Œ\n",
    "                # (í•„ìš”ì‹œ ë” ì—„ê²©íˆ í•„í„°ë§)\n",
    "                nodes.append(f\"n{p}\")\n",
    "        except Exception:\n",
    "            nodes.append(f\"n{p}\")\n",
    "    return nodes\n",
    "\n",
    "def base_name(dep_key: str) -> str:\n",
    "    \"\"\"'1EMR1_3' -> '1EMR1'\"\"\"\n",
    "    return dep_key.rsplit(\"_\", 1)[0] if \"_\" in dep_key else dep_key\n",
    "\n",
    "# â”€â”€ ì…ë ¥ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if not XLSX_PATH.exists():\n",
    "    raise FileNotFoundError(f\"ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {XLSX_PATH}\")\n",
    "if not DEP_JSON.exists():\n",
    "    raise FileNotFoundError(f\"dep_time_uk.jsonì´ ì—†ìŠµë‹ˆë‹¤: {DEP_JSON}\")\n",
    "\n",
    "# TRAIN ì‹œíŠ¸ ì½ê¸°\n",
    "train_df = pd.read_excel(XLSX_PATH, sheet_name=TRAIN_SHEET, dtype=str, engine=\"openpyxl\")\n",
    "train_df.columns = [c.strip() for c in train_df.columns]\n",
    "\n",
    "need_cols = {\"RLWAY_NM\", \"NODE\"}\n",
    "missing = need_cols - set(train_df.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"TRAIN ì‹œíŠ¸ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}\")\n",
    "\n",
    "# RLWAY_NM -> [\"n..\",\"n..\",...] ë§¤í•‘\n",
    "map_line_to_nodes = {}\n",
    "for line, nodes in train_df[[\"RLWAY_NM\",\"NODE\"]].itertuples(index=False):\n",
    "    if line is None:\n",
    "        continue\n",
    "    line_key = str(line).strip()\n",
    "    nodes_list = to_nodes_list(nodes)\n",
    "    if nodes_list:\n",
    "        map_line_to_nodes[line_key] = nodes_list\n",
    "\n",
    "# dep_time_uk.json ì½ê¸°\n",
    "with DEP_JSON.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    dep_map = json.load(f)  # {\"1AWC1_1\": 0, ...}\n",
    "\n",
    "# â”€â”€ ë³€í™˜: dep í‚¤ë§ˆë‹¤ ë…¸ë“œ ì‹œí€€ìŠ¤ ë¶€ì—¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "routes_nodes = {}\n",
    "missing_bases = set()\n",
    "\n",
    "# dep_mapì˜ í‚¤ ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ë„ë¡ ë°˜ë³µ\n",
    "for dep_key in dep_map.keys():\n",
    "    b = base_name(dep_key)\n",
    "    nodes = map_line_to_nodes.get(b)\n",
    "    if nodes is None:\n",
    "        missing_bases.add(b)\n",
    "        continue\n",
    "    # ì–•ì€ ë³µì‚¬(ë¦¬ìŠ¤íŠ¸ ë³´í˜¸)\n",
    "    routes_nodes[dep_key] = list(nodes)\n",
    "\n",
    "# â”€â”€ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUT_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(routes_nodes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {OUT_JSON}\")\n",
    "print(f\"ì´ ìƒì„± í•­ëª© ìˆ˜: {len(routes_nodes):,}\")\n",
    "if missing_bases:\n",
    "    print(\"âš  ë‹¤ìŒ ë² ì´ìŠ¤ ë¼ì¸ì€ TRAIN ì‹œíŠ¸ì—ì„œ ì°¾ì§€ ëª»í•´ ìŠ¤í‚µí–ˆìŠµë‹ˆë‹¤:\")\n",
    "    for b in sorted(missing_bases):\n",
    "        print(\"  -\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de54e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lineë³„ ìµœëŒ€ timestep10 ===\n",
      "1AWC1\t23\n",
      "1AWC2\t23\n",
      "1EMR1\t17\n",
      "1EMR2\t17\n",
      "1GA1\t32\n",
      "1GA2\t32\n",
      "1GWR1\t23\n",
      "1GWR2\t23\n",
      "1LNER1\t49\n",
      "1LNER2\t49\n",
      "1SR1\t47\n",
      "1SR2\t47\n",
      "1TPE1\t26\n",
      "1TPE2\t26\n",
      "1TW1\t27\n",
      "1TW2\t27\n",
      "2AWC1\t43\n",
      "2AWC2\t43\n",
      "2EMR1\t28\n",
      "2EMR2\t28\n",
      "2GWR1\t37\n",
      "2GWR2\t37\n",
      "2LNER1\t67\n",
      "2LNER2\t67\n",
      "2SR1\t21\n",
      "2SR2\t21\n",
      "2TW1\t35\n",
      "2TW2\t35\n",
      "3EMR1\t23\n",
      "3EMR2\t23\n",
      "3GWR1\t29\n",
      "3GWR2\t29\n",
      "3SR1\t20\n",
      "3SR2\t20\n",
      "4EMR1\t28\n",
      "4EMR2\t28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) íŒŒì¼Â·ì‹œíŠ¸ ì„¤ì •\n",
    "xlsx_fp = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\qgis_export_uk.xlsx\")\n",
    "sheet   = \"DEMAND\"  # ì‹œíŠ¸ëª…ì´ ë‹¤ë¥´ë©´ ìˆ˜ì •\n",
    "\n",
    "# 2) í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì½ê¸°\n",
    "usecols = [\"Line\", \"timestep10\"]\n",
    "df = pd.read_excel(xlsx_fp, sheet_name=sheet, usecols=usecols, dtype=str, engine=\"openpyxl\")\n",
    "\n",
    "# 3) ì „ì²˜ë¦¬: ê³µë°± ì œê±° + ìˆ˜ì¹˜í™”\n",
    "df[\"Line\"] = df[\"Line\"].astype(str).str.strip()\n",
    "df[\"timestep10\"] = pd.to_numeric(df[\"timestep10\"], errors=\"coerce\")\n",
    "\n",
    "# 4) Lineë³„ ìµœëŒ€ timestep10 ê³„ì‚°\n",
    "max_by_line = (df.groupby(\"Line\", dropna=True)[\"timestep10\"]\n",
    "                 .max()\n",
    "                 .reset_index()\n",
    "                 .rename(columns={\"timestep10\": \"max_timestep10\"}))\n",
    "\n",
    "# 5) ì •ë ¬(ì›í•˜ë©´ ë‚´ë¦¼ì°¨ìˆœ)\n",
    "max_by_line = max_by_line.sort_values([\"Line\"]).reset_index(drop=True)\n",
    "\n",
    "# 6) ì¶œë ¥\n",
    "print(\"=== Lineë³„ ìµœëŒ€ timestep10 ===\")\n",
    "for line, vmax in max_by_line.itertuples(index=False):\n",
    "    print(f\"{line}\\t{vmax}\")\n",
    "\n",
    "# (ì„ íƒ) í‘œ í˜•íƒœë¡œ í•œ ë²ˆì— ë³´ê¸°\n",
    "# print(max_by_line.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc6d082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ë…¸ì„ ë³„ ìµœëŒ€ Edge íƒ‘ìŠ¹Â·í•„ìš” ì—´ì°¨ ëŒ€ìˆ˜ ===\n",
      "1AWC1            ìµœëŒ€íƒ‘ìŠ¹ = 1880444ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 4587ëŒ€\n",
      "1AWC2            ìµœëŒ€íƒ‘ìŠ¹ = 72209ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 177ëŒ€\n",
      "1EMR1            ìµœëŒ€íƒ‘ìŠ¹ = 1607277ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 3921ëŒ€\n",
      "1EMR2            ìµœëŒ€íƒ‘ìŠ¹ = 28712ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 71ëŒ€\n",
      "1GA1             ìµœëŒ€íƒ‘ìŠ¹ = 1890145ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 4611ëŒ€\n",
      "1GA2             ìµœëŒ€íƒ‘ìŠ¹ = 48215ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 118ëŒ€\n",
      "1GWR1            ìµœëŒ€íƒ‘ìŠ¹ = 1296469ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 3163ëŒ€\n",
      "1GWR2            ìµœëŒ€íƒ‘ìŠ¹ = 79402ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 194ëŒ€\n",
      "1LNER1           ìµœëŒ€íƒ‘ìŠ¹ = 2149512ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 5243ëŒ€\n",
      "1LNER2           ìµœëŒ€íƒ‘ìŠ¹ = 127608ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 312ëŒ€\n",
      "1SR1             ìµœëŒ€íƒ‘ìŠ¹ = 1019061ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 2486ëŒ€\n",
      "1SR2             ìµœëŒ€íƒ‘ìŠ¹ = 48037ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 118ëŒ€\n",
      "1TPE1            ìµœëŒ€íƒ‘ìŠ¹ = 42688ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 105ëŒ€\n",
      "1TPE2            ìµœëŒ€íƒ‘ìŠ¹ = 52438ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 128ëŒ€\n",
      "1TW1             ìµœëŒ€íƒ‘ìŠ¹ = 23066ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 57ëŒ€\n",
      "1TW2             ìµœëŒ€íƒ‘ìŠ¹ = 60456ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 148ëŒ€\n",
      "2AWC1            ìµœëŒ€íƒ‘ìŠ¹ = 2385564ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 5819ëŒ€\n",
      "2AWC2            ìµœëŒ€íƒ‘ìŠ¹ = 34697ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 85ëŒ€\n",
      "2EMR1            ìµœëŒ€íƒ‘ìŠ¹ = 2119415ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 5170ëŒ€\n",
      "2EMR2            ìµœëŒ€íƒ‘ìŠ¹ = 77191ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 189ëŒ€\n",
      "2GWR1            ìµœëŒ€íƒ‘ìŠ¹ = 1128237ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 2752ëŒ€\n",
      "2GWR2            ìµœëŒ€íƒ‘ìŠ¹ = 54403ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 133ëŒ€\n",
      "2LNER1           ìµœëŒ€íƒ‘ìŠ¹ = 2172892ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 5300ëŒ€\n",
      "2LNER2           ìµœëŒ€íƒ‘ìŠ¹ = 131791ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 322ëŒ€\n",
      "2SR1             ìµœëŒ€íƒ‘ìŠ¹ = 673920ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 1644ëŒ€\n",
      "2SR2             ìµœëŒ€íƒ‘ìŠ¹ = 33952ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 83ëŒ€\n",
      "2TW1             ìµœëŒ€íƒ‘ìŠ¹ = 53571ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 131ëŒ€\n",
      "2TW2             ìµœëŒ€íƒ‘ìŠ¹ = 29068ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 71ëŒ€\n",
      "3EMR1            ìµœëŒ€íƒ‘ìŠ¹ = 1594730ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 3890ëŒ€\n",
      "3EMR2            ìµœëŒ€íƒ‘ìŠ¹ = 26323ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 65ëŒ€\n",
      "3GWR1            ìµœëŒ€íƒ‘ìŠ¹ = 829176ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 2023ëŒ€\n",
      "3GWR2            ìµœëŒ€íƒ‘ìŠ¹ = 21031ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 52ëŒ€\n",
      "3SR1             ìµœëŒ€íƒ‘ìŠ¹ = 1397191ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 3408ëŒ€\n",
      "3SR2             ìµœëŒ€íƒ‘ìŠ¹ = 30700ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 75ëŒ€\n",
      "4EMR1            ìµœëŒ€íƒ‘ìŠ¹ = 1713670ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 4180ëŒ€\n",
      "4EMR2            ìµœëŒ€íƒ‘ìŠ¹ = 41794ëª…  /  ìš©ëŸ‰ = 410  â†’  ì—´ì°¨ 102ëŒ€\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\MINJI\\\\NETWORK RELIABILITY\\\\QGIS\\\\8.UK\\\\json\\\\json\\\\demand.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mln\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m15s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  ìµœëŒ€íƒ‘ìŠ¹ = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_load[ln]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m5.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mëª…  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/  ìš©ëŸ‰ = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcap_map[ln]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  â†’  ì—´ì°¨ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_cnt[ln]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mëŒ€\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. demand.json ë¶ˆëŸ¬ì™€ì„œ ìª¼ê°œê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     50\u001b[39m     demand_js = json.load(f)\n\u001b[32m     52\u001b[39m split_js = {}   \u001b[38;5;66;03m# ìƒˆë¡œìš´ dict\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\mjkang\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'D:\\\\MINJI\\\\NETWORK RELIABILITY\\\\QGIS\\\\8.UK\\\\json\\\\json\\\\demand.json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd, json, math\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE     = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\")\n",
    "xlsx_fp  = BASE / \"qgis_export_uk.xlsx\"\n",
    "json_in  = BASE / \"json\" / \"demand.json\"\n",
    "json_out = BASE / \"json\" / \"demand_split.json\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. DEMAND ì‹œíŠ¸ â†’ Edge ìµœëŒ€ íƒ‘ìŠ¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_excel(xlsx_fp, sheet_name=\"DEMAND\",\n",
    "                   usecols=[\"Line\", \"Route\", \"journeys\"])\n",
    "\n",
    "edge_load = defaultdict(int)          # {(Line, u, v): ìŠ¹ê° ëˆ„ì }\n",
    "\n",
    "for line, route, pax in df.itertuples(index=False):\n",
    "    nodes = route.split(\"-\")\n",
    "    for u, v in zip(nodes[:-1], nodes[1:]):\n",
    "        edge_load[(line, u, v)] += pax\n",
    "\n",
    "edge_df = (pd.DataFrame([(l,u,v,c) for (l,u,v),c in edge_load.items()],\n",
    "                        columns=[\"Line\",\"From\",\"To\",\"Load\"]))\n",
    "\n",
    "max_load = (edge_df.groupby(\"Line\")[\"Load\"]\n",
    "                     .max()\n",
    "                     .to_dict())      # {Line: ìµœëŒ€ Edge íƒ‘ìŠ¹}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ë…¸ì„ ë³„ ì—´ì°¨ 1í¸ì„± ìµœëŒ€ ì¢Œì„ìˆ˜ ë§¤í•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def train_capacity(line: str) -> int:\n",
    "    \"\"\"ë…¸ì„  ì´ë¦„ì„ ë°›ì•„ ë³´ìˆ˜ì  1í¸ì„± ìµœëŒ€ ì¢Œì„ìˆ˜ë¥¼ ë°˜í™˜\"\"\"\n",
    "    if line.startswith(\"KTXê°•ë¦‰ì„ \"):\n",
    "        return 381           # KTXâ€‘Eum\n",
    "    if line.startswith(\"ê²½ë¶€ê³ ì†ì² ë„\"):\n",
    "        return 515           # KTXâ€‘ì²­ë£¡ / ì‚°ì²œ\n",
    "    # ê·¸ ì™¸ ì¼ë°˜ì„ Â·í˜¸ë‚¨ê³ ì†ì² ë„ ë“± â†’ ì‚°ì²œ ë³´ìˆ˜ì¹˜\n",
    "    return 410               # KTXâ€‘Sancheon\n",
    "\n",
    "cap_map   = {ln: train_capacity(ln) for ln in max_load}\n",
    "train_cnt = {ln: max(1, math.ceil(max_load[ln] / cap_map[ln]))\n",
    "             for ln in max_load}\n",
    "\n",
    "print(\"\\n=== ë…¸ì„ ë³„ ìµœëŒ€ Edge íƒ‘ìŠ¹Â·í•„ìš” ì—´ì°¨ ëŒ€ìˆ˜ ===\")\n",
    "for ln in sorted(train_cnt):\n",
    "    print(f\"{ln:15s}  ìµœëŒ€íƒ‘ìŠ¹ = {max_load[ln]:5.0f}ëª…  \"\n",
    "          f\"/  ìš©ëŸ‰ = {cap_map[ln]}  â†’  ì—´ì°¨ {train_cnt[ln]}ëŒ€\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. demand.json ë¶ˆëŸ¬ì™€ì„œ ìª¼ê°œê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(json_in, \"r\", encoding=\"utf-8\") as f:\n",
    "    demand_js = json.load(f)\n",
    "\n",
    "split_js = {}   # ìƒˆë¡œìš´ dict\n",
    "\n",
    "for ln, od_list in demand_js.items():\n",
    "    n_train = train_cnt.get(ln, 1)          # ë§¤í•‘ ì•ˆ ë˜ë©´ 1ëŒ€\n",
    "    for idx in range(n_train):\n",
    "        key = f\"{ln}_{idx+1}\"\n",
    "        split_js[key] = []\n",
    "    for o, d, j in od_list:\n",
    "        share = round(j / n_train, 2)\n",
    "        for idx in range(n_train):\n",
    "            split_js[f\"{ln}_{idx+1}\"].append([o, d, share])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. JSON ì €ì¥ (í•œ ë…¸ì„  = í•œ ì¤„) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(json_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"{\\n\")\n",
    "    for i, (line, trips) in enumerate(split_js.items()):\n",
    "        line_str = json.dumps(line, ensure_ascii=False)\n",
    "        trips_str = json.dumps(trips, ensure_ascii=False, separators=(\",\", \":\"))\n",
    "        comma = \",\" if i < len(split_js) - 1 else \"\"\n",
    "        f.write(f\"  {line_str}: {trips_str}{comma}\\n\")\n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"\\nâœ… ì¤„ë°”ê¿ˆ ìµœì†Œí™”ëœ JSON ì €ì¥ ì™„ë£Œ â†’ {json_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14966d",
   "metadata": {},
   "source": [
    "ì¶œë°œ timestep ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46944202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 'time_step_15' ì—´ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "file_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export.xlsx\"\n",
    "sheet_name = \"EDGE\"\n",
    "\n",
    "# ì—‘ì…€ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# 'time(min)' ì—´ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "if \"time(min)\" not in df.columns:\n",
    "    raise KeyError(\"'time(min)' ì—´ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì†Œìˆ«ì  ì˜¬ë¦¼í•˜ì—¬ time_step_15 ê³„ì‚°\n",
    "df[\"time_step_15\"] = df[\"time(min)\"].apply(lambda x: math.ceil(x / 15))\n",
    "\n",
    "# ë®ì–´ì“°ê¸° ì €ì¥\n",
    "with pd.ExcelWriter(file_path, mode=\"a\", if_sheet_exists=\"overlay\", engine=\"openpyxl\") as writer:\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"âœ… 'time_step_15' ì—´ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6fe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… dep_time.json trimmed and saved (cutoff by rounded threshold).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "base_dir = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\"\n",
    "dep_path = os.path.join(base_dir, \"dep_time.json\")\n",
    "demand_path = os.path.join(base_dir, \"demand.json\")\n",
    "routes_path = os.path.join(base_dir, \"routes_nodes.json\")\n",
    "\n",
    "# 1. dep_time.json ë¡œë“œ\n",
    "with open(dep_path, encoding=\"utf-8\") as f:\n",
    "    dep_data = json.load(f)\n",
    "valid_trains = set(dep_data.keys())\n",
    "\n",
    "# 2. demand.json í•„í„°ë§\n",
    "with open(demand_path, encoding=\"utf-8\") as f:\n",
    "    demand_data = json.load(f)\n",
    "filtered_demand = {k: v for k, v in demand_data.items() if k in valid_trains}\n",
    "\n",
    "# 3. routes_nodes.json í•„í„°ë§\n",
    "with open(routes_path, encoding=\"utf-8\") as f:\n",
    "    routes_data = json.load(f)\n",
    "filtered_routes = {k: v for k, v in routes_data.items() if k in valid_trains}\n",
    "\n",
    "# 4. ì €ì¥ (ë®ì–´ì“°ê¸°)\n",
    "with open(demand_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_demand, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(routes_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_routes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… demand.jsonê³¼ routes_nodes.jsonì´ dep_time.json ê¸°ì¤€ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99a754a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì¤„ë°”ê¿ˆ í¬ë§· ì •ë¦¬ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "base_dir = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\"\n",
    "demand_path = os.path.join(base_dir, \"demand_filtered.json\")\n",
    "routes_path = os.path.join(base_dir, \"routes_nodes_filtered.json\")\n",
    "\n",
    "def save_single_line_json(data, filepath):\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"{\\n\")\n",
    "        for i, (k, v) in enumerate(data.items()):\n",
    "            line = f'  \"{k}\": {json.dumps(v, ensure_ascii=False)}'\n",
    "            if i < len(data) - 1:\n",
    "                line += \",\"\n",
    "            f.write(line + \"\\n\")\n",
    "        f.write(\"}\")\n",
    "\n",
    "# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(demand_path, encoding=\"utf-8\") as f:\n",
    "    demand_data = json.load(f)\n",
    "with open(routes_path, encoding=\"utf-8\") as f:\n",
    "    routes_data = json.load(f)\n",
    "\n",
    "# í•œ ì¤„ë‹¹ ê¸°ì°¨ í•˜ë‚˜ì”© ì €ì¥\n",
    "save_single_line_json(demand_data, demand_path)\n",
    "save_single_line_json(routes_data, routes_path)\n",
    "\n",
    "print(\"âœ… ì¤„ë°”ê¿ˆ í¬ë§· ì •ë¦¬ ì™„ë£Œ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d2e672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ í•„í„°ë§ ì „ ODìŒ ê°œìˆ˜: 12426\n",
      "ğŸ”¹ í•„í„°ë§ í›„ ODìŒ ê°œìˆ˜: 3607\n",
      "ğŸ”¹ ì œê±°ëœ ODìŒ ê°œìˆ˜: 8819\n",
      "âœ… ì €ì¥ ì™„ë£Œ: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\\demand_filtered.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "file_path = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\\demand.json\")\n",
    "output_path = file_path.parent / \"demand_filtered.json\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    demand_data = json.load(f)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. í•„í„°ë§ ë° í†µê³„\n",
    "original_count = 0\n",
    "filtered_count = 0\n",
    "\n",
    "filtered_data = {}\n",
    "\n",
    "for train_id, od_list in demand_data.items():\n",
    "    original_count += len(od_list)\n",
    "    \n",
    "    # ìˆ˜ìš” 10 ì´ìƒì¸ ODìŒë§Œ ë‚¨ê¸°ê¸°\n",
    "    new_od_list = [od for od in od_list if od[2] >= 10]\n",
    "    filtered_count += len(new_od_list)\n",
    "    \n",
    "    # ê²°ê³¼ê°€ ë‚¨ì•„ìˆëŠ” ê²½ìš°ì—ë§Œ ì €ì¥\n",
    "    if new_od_list:\n",
    "        filtered_data[train_id] = new_od_list\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. í•„í„°ë§ ê²°ê³¼ ì €ì¥\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ğŸ”¹ í•„í„°ë§ ì „ ODìŒ ê°œìˆ˜: {original_count}\")\n",
    "print(f\"ğŸ”¹ í•„í„°ë§ í›„ ODìŒ ê°œìˆ˜: {filtered_count}\")\n",
    "print(f\"ğŸ”¹ ì œê±°ëœ ODìŒ ê°œìˆ˜: {original_count - filtered_count}\")\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15255623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… dep_time í•„í„°ë§: 122 â†’ 106ê°œ ë‚¨ìŒ\n",
      "âœ… routes_nodes í•„í„°ë§: 122 â†’ 106ê°œ ë‚¨ìŒ\n",
      "\n",
      "ğŸ“ ì €ì¥ ì™„ë£Œ:\n",
      "â†’ dep_time_filtered.json\n",
      "â†’ routes_nodes_filtered.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. ê²½ë¡œ ì„¤ì •\n",
    "base_dir = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\")\n",
    "dep_time_path = base_dir / \"dep_time.json\"\n",
    "routes_nodes_path = base_dir / \"routes_nodes.json\"\n",
    "dep_time_out = base_dir / \"dep_time_filtered.json\"\n",
    "routes_nodes_out = base_dir / \"routes_nodes_filtered.json\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. dep_time.json ë¶ˆëŸ¬ì˜¤ê¸° ë° í•„í„°ë§ (ê°’ì´ 13 ë¯¸ë§Œì¸ ê²ƒë§Œ)\n",
    "with open(dep_time_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dep_time = json.load(f)\n",
    "\n",
    "filtered_dep_time = {k: v for k, v in dep_time.items() if v < 12}\n",
    "print(f\"âœ… dep_time í•„í„°ë§: {len(dep_time)} â†’ {len(filtered_dep_time)}ê°œ ë‚¨ìŒ\")\n",
    "\n",
    "# ì €ì¥\n",
    "with open(dep_time_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_dep_time, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. routes_nodes.json ë¶ˆëŸ¬ì˜¤ê¸° ë° í•´ë‹¹ í‚¤ë§Œ ë‚¨ê¸°ê¸°\n",
    "with open(routes_nodes_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    routes_nodes = json.load(f)\n",
    "\n",
    "# dep_timeì—ì„œ ë‚¨ì€ í‚¤ë§Œ ìœ ì§€\n",
    "filtered_routes_nodes = {k: v for k, v in routes_nodes.items() if k in filtered_dep_time}\n",
    "print(f\"âœ… routes_nodes í•„í„°ë§: {len(routes_nodes)} â†’ {len(filtered_routes_nodes)}ê°œ ë‚¨ìŒ\")\n",
    "\n",
    "# ì €ì¥\n",
    "with open(routes_nodes_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_routes_nodes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. ê²°ê³¼ ìš”ì•½\n",
    "print(f\"\\nğŸ“ ì €ì¥ ì™„ë£Œ:\")\n",
    "print(f\"â†’ {dep_time_out.name}\")\n",
    "print(f\"â†’ {routes_nodes_out.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb134c89",
   "metadata": {},
   "source": [
    "#### Capacity\n",
    "- Demand_dataì´ìš©í•´ì„œ ì—£ì§€ë§ˆë‹¤ íë¥´ëŠ” demandì–‘ ê³„ì‚°í•œ íŒŒì¼: 6.edge_journeys_summary.xlsx\n",
    "- e1 n1-n2 1502241\n",
    "- e2 n2-n1 389459.5\n",
    "- e1,e2,e3,e4, .. ë„˜ë²„ë§ qgisì™€ ë‹¤ë¦„ (ì–‘ë°©í–¥ ê³ ë ¤)\n",
    "- ì–‘ë°©í–¥ ì—£ì§€ì— ëŒ€í•´ì„œëŠ” ì„œë¡œ ë™ì¼í•œ capacityë¥¼ ê°–ë„ë¡ í•´ì•¼í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "060b668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capacity summary saved â†’ D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\7.edge_capacity_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "base_dir = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\")\n",
    "input_path = base_dir / \"6.edge_journeys_summary.xlsx\"\n",
    "df = pd.read_excel(input_path)\n",
    "\n",
    "# 2. ì—¬ìœ  ê³„ìˆ˜ ì„¤ì • (ì˜ˆ: 1.2ë°°)\n",
    "buffer_ratio = 1.2\n",
    "\n",
    "# 3. (u, v) / (v, u) í˜•íƒœë¥¼ ê³ ë ¤í•´ ì–‘ë°©í–¥ í‚¤ ë§Œë“¤ê¸°\n",
    "df[\"pair_key\"] = df.apply(lambda row: tuple(sorted([row[\"from_node_name\"], row[\"to_node_name\"]])), axis=1)\n",
    "\n",
    "# 4. ê°™ì€ pair_key ê·¸ë£¹ ë‚´ì—ì„œ ìµœëŒ€ journeys ì„ íƒ\n",
    "grouped = df.groupby(\"pair_key\", as_index=False).agg({\n",
    "    \"journeys\": \"max\"\n",
    "})\n",
    "grouped[\"capacity\"] = grouped[\"journeys\"] * buffer_ratio\n",
    "\n",
    "# 5. ë‹¤ì‹œ ì›ë˜ ë°©í–¥ì„± edgeì— capacity ì—°ê²°\n",
    "df = df.merge(grouped[[\"pair_key\", \"capacity\"]], on=\"pair_key\", how=\"left\")\n",
    "\n",
    "# 6. ì»¬ëŸ¼ ì •ë¦¬ ë° ì €ì¥\n",
    "df = df.drop(columns=[\"pair_key\"])\n",
    "output_path = base_dir / \"7.edge_capacity_summary.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Capacity summary saved â†’ {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc815f",
   "metadata": {},
   "source": [
    "#### ë…¸ì„ ë³„ ìŠ¹ê° ìˆ˜ìš” ë°ì´í„° -> í•„ìš”í•œ ê¸°ì°¨ìˆ˜ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01187308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      line  cycles_day  train_sets_needed\n",
      "0   KTXê°•ë¦‰ì„           24                  1\n",
      "1   KTXë™í•´ì„           39                  1\n",
      "2      ê²½ê°•ì„           23                  2\n",
      "3   ê²½ë¶€ê³ ì†ì² ë„           8                 23\n",
      "4      ê²½ë¶€ì„            1                 65\n",
      "5      ê²½ë¶ì„           18                  1\n",
      "6      ê²½ì˜ì„           46                  1\n",
      "7      ê²½ì¸ì„           35                  1\n",
      "8      ê²½ì „ì„            5                  3\n",
      "9     ê³µí•­ì² ë„          20                  2\n",
      "10     ëŒ€êµ¬ì„           44                  1\n",
      "11   ë™í•´ë‚¨ë¶€ì„           12                  3\n",
      "12     ë™í•´ì„           16                  1\n",
      "13     ì‚¼ì²™ì„           51                  1\n",
      "14     ì˜ë™ì„           12                  2\n",
      "15     ì¥í•­ì„           11                  2\n",
      "16     ì „ë¼ì„           17                  2\n",
      "17   ì¤‘ë¶€ë‚´ë¥™ì„           36                  1\n",
      "18     ì¤‘ì•™ì„            2                 23\n",
      "19     ì¶©ë¶ì„           11                  1\n",
      "20     íƒœë°±ì„           19                  1\n",
      "21  í˜¸ë‚¨ê³ ì†ì² ë„          20                  1\n",
      "22     í˜¸ë‚¨ì„           10                  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minji Kang\\AppData\\Local\\Temp\\ipykernel_38976\\2930921966.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby(\"line\").apply(train_stats).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import json, math, pandas as pd, os\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. íŒŒì¼ ì½ê¸°\n",
    "file_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\5.demand_data_kofull.json\"\n",
    "df = pd.read_json(file_path)\n",
    "df[\"journeys_day\"] = df[\"journeys\"] / 365\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. íŒŒë¼ë¯¸í„°\n",
    "TARGET_LOAD    = 1.0          # 100 % íƒ‘ìŠ¹ë¥ \n",
    "TURNAROUND_MIN = 15           # ì¢…ì  ë²„í¼(ë¶„)\n",
    "HOURS_PER_DAY  = 17\n",
    "DEFAULT_SEAT   = 400          # seat_map ì— ì—†ì„ ë•Œ ì„ì‹œê°’\n",
    "\n",
    "# ë…¸ì„ ë³„ ì¢Œì„ìˆ˜ (ì• ê¸€ì ë§¤ì¹­ìš©)\n",
    "seat_map = {\n",
    "    # ê³ ì†ì—´ì°¨\n",
    "    \"KTXê°•ë¦‰ì„ \":    381,   # KTX-ì´ìŒ (KTX-Eum) 381ì„ :contentReference[oaicite:2]{index=2}\n",
    "    \"KTXê²½ë¶€ì„ \":    955,   # KTX-I 955ì„(935~955) :contentReference[oaicite:3]{index=3}\n",
    "    \"KTXí˜¸ë‚¨ì„ \":    955,\n",
    "    \"ê²½ë¶€ê³ ì†ì² ë„\":   955,   # KTX-ì‚°ì²œ\n",
    "    \"ê²½ë¶€ì„ \":         900,   # SRT í¸ì„±(ì˜ˆì‹œ)\n",
    "    # ITX ê³„ì—´\n",
    "    \"ITX-ìƒˆë§ˆì„\":   376,   # ITX-ìƒˆë§ˆì„\n",
    "    \"ITX-ì²­ì¶˜\":     402,   # ITX-ì²­ì¶˜\n",
    "    # ì¼ë°˜ì—´ì°¨\n",
    "    \"ë¬´ê¶í™”í˜¸\":     920,   # ì¢Œì„+ì…ì„ í—ˆìš©, í¸ì„± ì¢Œì„ 920 ë¶€ê·¼\n",
    "}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def train_stats(group):\n",
    "    line = group.name\n",
    "    seats = next((v for k, v in seat_map.items() if line.startswith(k)), DEFAULT_SEAT)\n",
    "\n",
    "    total_pax = group[\"journeys_day\"].sum()\n",
    "    single_run = group[\"time_min\"].max()\n",
    "    round_trip = single_run + TURNAROUND_MIN\n",
    "    cycles = max(1, (HOURS_PER_DAY * 60) // round_trip)\n",
    "\n",
    "    cap_per_set = seats * TARGET_LOAD * cycles\n",
    "    sets_needed = math.ceil(total_pax / cap_per_set) if cap_per_set else None\n",
    "\n",
    "    return pd.Series({\n",
    "        \"cycles_day\": cycles,\n",
    "        \"train_sets_needed\": sets_needed\n",
    "    })\n",
    "\n",
    "# 4. ê³„ì‚°\n",
    "result = df.groupby(\"line\").apply(train_stats).reset_index()\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjkang",
   "language": "python",
   "name": "mjkang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
