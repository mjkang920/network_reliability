{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c9612c",
   "metadata": {},
   "source": [
    "#### QGIS -> demand \n",
    "노선별로 경로 생성\n",
    "distance 데이터 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab9b4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 노선별 단일 경로 (방향 무시)\n",
      "KTX강릉선: [22, 23, 34, 51]\n",
      "경부고속철도: [4, 3, 1, 2, 13, 18, 28, 42, 60, 72, 75, 79]\n",
      "경부선: [79, 77, 76, 73, 70, 63, 60, 59, 46, 41, 28, 17, 14, 9, 7, 1, 4]\n",
      "경북선: [41, 38, 36, 40, 43]\n",
      "경전선: [48, 57, 64, 65, 62, 66, 71, 73]\n",
      "동해선: [55, 58, 69, 74, 72]\n",
      "영동선: [51, 55, 54, 50, 43]\n",
      "장항선: [14, 11, 10, 15, 16, 27, 30]\n",
      "전라선: [68, 62, 56, 47, 45, 33, 30]\n",
      "중앙선: [3, 5, 6, 8, 12, 22, 29, 31, 35, 39, 43, 49, 52, 67, 72, 78, 79]\n",
      "충북선: [17, 18, 19, 20, 26, 29]\n",
      "호남고속철도: [48, 37, 30, 21, 18, 13, 2, 1, 3]\n",
      "호남선: [61, 53, 48, 44, 37, 32, 30, 24, 25, 28]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# ──────────────────────────────\n",
    "# 1. 파일 경로\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "xlsx_path = base_path + r\"\\qgis_export.xlsx\"\n",
    "sheet_name = \"LINE\"\n",
    "# ──────────────────────────────\n",
    "\n",
    "df = pd.read_excel(xlsx_path, sheet_name=sheet_name)\n",
    "df = df.dropna(subset=[\"from_node_\", \"to_node_id\"])\n",
    "df[\"from_node_\"] = df[\"from_node_\"].astype(int)\n",
    "df[\"to_node_id\"] = df[\"to_node_id\"].astype(int)\n",
    "\n",
    "print(\"📌 노선별 단일 경로 (방향 무시)\")\n",
    "for line, group in df.groupby(\"RLWAY_NM\"):\n",
    "    # 양방향 그래프 생성\n",
    "    graph = defaultdict(list)\n",
    "    for u, v in zip(group[\"from_node_\"], group[\"to_node_id\"]):\n",
    "        graph[u].append(v)\n",
    "        graph[v].append(u)\n",
    "\n",
    "    # 노드 연결 상태 진단\n",
    "    degree = {n: len(adj) for n, adj in graph.items()}\n",
    "    endpoints = [n for n, d in degree.items() if d == 1]\n",
    "    if len(endpoints) != 2:\n",
    "        print(f\"⚠️ {line}: 선형 경로 아님 (끝 노드 {len(endpoints)}개)\")\n",
    "        continue\n",
    "\n",
    "    # 한쪽 끝에서부터 BFS로 경로 복원\n",
    "    start = endpoints[0]\n",
    "    visited = set()\n",
    "    path = []\n",
    "\n",
    "    def dfs(u):\n",
    "        visited.add(u)\n",
    "        path.append(u)\n",
    "        for v in graph[u]:\n",
    "            if v not in visited:\n",
    "                dfs(v)\n",
    "\n",
    "    dfs(start)\n",
    "    print(f\"{line}: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef1b54",
   "metadata": {},
   "source": [
    "#### EDGE timestep 데이터 만들기 위해\n",
    "- distance로 이동시간\n",
    "- 이동시간 기반 time step 계산\n",
    "- EDGE 데이터 .json 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f068c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 완료: time_step 열이 추가된 파일이 저장됨 →\n",
      "D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export_with_time.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ────────────── 설정 ──────────────\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "input_file  = base_path + r\"\\qgis_export.xlsx\"\n",
    "output_file = base_path + r\"\\qgis_export_with_time.xlsx\"\n",
    "sheet_name  = \"EDGE\"\n",
    "# ────────────────────────────────\n",
    "\n",
    "# 1. 원본 엑셀 전체 읽기 (모든 시트 보존 목적)\n",
    "xlsx_all = pd.read_excel(input_file, sheet_name=None)\n",
    "\n",
    "# 2. EDGE 시트만 수정\n",
    "df_edge = xlsx_all[sheet_name]\n",
    "\n",
    "# 3. time_step 계산\n",
    "if 'distance' not in df_edge.columns:\n",
    "    raise ValueError(\"'distance' 열이 EDGE 시트에 없습니다.\")\n",
    "df_edge['time_step'] = df_edge['distance'] / 150 * 60\n",
    "\n",
    "# 4. 수정한 EDGE 시트 다시 넣기\n",
    "xlsx_all[sheet_name] = df_edge\n",
    "\n",
    "# 5. 전체 시트를 새 파일로 저장\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    for sheet, df in xlsx_all.items():\n",
    "        df.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "print(f\"✅ 완료: time_step 열이 추가된 파일이 저장됨 →\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb45211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 완료: 'time_step' 계산 후 저장됨 →\n",
      "D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export_with_step.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# ────────────── 설정 ──────────────\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "input_file  = base_path + r\"\\qgis_export_with_time.xlsx\"\n",
    "output_file = base_path + r\"\\qgis_export_with_step.xlsx\"\n",
    "sheet_name  = \"EDGE\"\n",
    "# ────────────────────────────────\n",
    "\n",
    "# 1. 모든 시트 읽기\n",
    "xlsx_all = pd.read_excel(input_file, sheet_name=None)\n",
    "\n",
    "# 2. EDGE 시트 수정\n",
    "df_edge = xlsx_all[sheet_name]\n",
    "\n",
    "# 3. time_step 계산 (5로 나누고 올림)\n",
    "if 'time(min)' not in df_edge.columns:\n",
    "    raise ValueError(\"'time(min)' 열이 EDGE 시트에 없습니다.\")\n",
    "\n",
    "df_edge['time_step'] = df_edge['time(min)'].apply(lambda x: math.ceil(x / 5))\n",
    "\n",
    "# 4. 덮어쓰기\n",
    "xlsx_all[sheet_name] = df_edge\n",
    "\n",
    "# 5. 새 파일로 저장\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    for sheet, df in xlsx_all.items():\n",
    "        df.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "print(f\"✅ 완료: 'time_step' 계산 후 저장됨 →\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "072ab939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 정렬된 edges.json 저장 완료 →\n",
      "D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\edges.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "# ─────────────── 설정 ───────────────\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "input_file  = os.path.join(base_path, \"qgis_export_with_step.xlsx\")\n",
    "sheet_name  = \"EDGE\"\n",
    "output_json = os.path.join(base_path, \"edges.json\")\n",
    "# ───────────────────────────────────\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name)\n",
    "\n",
    "# 2. 필요한 열 확인\n",
    "required_cols = ['edge_id', 'from_node_id', 'to_node_id', 'time_step']\n",
    "missing = set(required_cols) - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"다음 열이 누락되었습니다: {missing}\")\n",
    "\n",
    "# 3. 정수형 edge_id 기준으로 정렬\n",
    "df = df.dropna(subset=['edge_id'])\n",
    "df['edge_id'] = df['edge_id'].astype(int)\n",
    "df = df.sort_values(by='edge_id')\n",
    "\n",
    "# 4. edge 딕셔너리 생성 (OrderedDict로 순서 유지)\n",
    "edges = OrderedDict()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    eid = f\"e{int(row['edge_id'])}\"\n",
    "    n_from = f\"n{int(row['from_node_id'])}\"\n",
    "    n_to   = f\"n{int(row['to_node_id'])}\"\n",
    "    tstep  = int(row['time_step'])\n",
    "\n",
    "    edges[eid] = (n_from, n_to, tstep)\n",
    "\n",
    "# 5. JSON 저장\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(edges, f, indent=4)\n",
    "\n",
    "print(f\"✅ 정렬된 edges.json 저장 완료 →\\n{output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0837e648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\routes_nodes.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ─────────────── 설정 ───────────────\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "xlsx_file = os.path.join(base_path, \"qgis_export_with_step.xlsx\")\n",
    "sheet_name = \"TRAIN\"\n",
    "output_json = os.path.join(base_path, \"routes_nodes.json\")\n",
    "# ───────────────────────────────────\n",
    "\n",
    "# 1. TRAIN 시트 읽기\n",
    "df = pd.read_excel(xlsx_file, sheet_name=sheet_name)\n",
    "\n",
    "# 2. routes_nodes 딕셔너리 생성\n",
    "routes_nodes = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    name = str(row[\"RLWAY_NM\"]).strip()\n",
    "    nodes_str = str(row[\"NODE\"]).strip()\n",
    "\n",
    "    # 노드 문자열 파싱 → ['n4', 'n3', 'n1', ...]\n",
    "    node_list = [f\"n{int(n)}\" for n in nodes_str.split(\"-\") if n.isdigit()]\n",
    "\n",
    "    # 정방향 및 역방향 저장\n",
    "    routes_nodes[f\"{name}1\"] = node_list\n",
    "    routes_nodes[f\"{name}2\"] = node_list[::-1]\n",
    "\n",
    "# 3. JSON 저장\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(routes_nodes, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ 저장 완료: {output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ea2954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 양방향 edges 저장 완료: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\edges_bidirectional.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "# ─────────────── 설정 ───────────────\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "input_json = os.path.join(base_path, \"edges.json\")\n",
    "output_json = os.path.join(base_path, \"edges_bidirectional.json\")\n",
    "# ───────────────────────────────────\n",
    "\n",
    "# 1. 기존 edges.json 불러오기\n",
    "with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    edges = json.load(f)\n",
    "\n",
    "# 2. 양방향 딕셔너리 생성 (순서 보존)\n",
    "edges_bidir = OrderedDict()\n",
    "\n",
    "for eid, (n1, n2, t) in edges.items():\n",
    "    # 정방향 edge 추가\n",
    "    edges_bidir[eid] = [n1, n2, t]\n",
    "    \n",
    "    # 역방향 edge 추가\n",
    "    eid_r = f\"{eid}r\"\n",
    "    edges_bidir[eid_r] = [n2, n1, t]\n",
    "\n",
    "# 3. 새 JSON 파일로 저장\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(edges_bidir, f, indent=4)\n",
    "\n",
    "print(f\"✅ 양방향 edges 저장 완료: {output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9416e8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ demand_template.json 저장 완료 →\n",
      "D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\demand_template.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# ─────────────── 설정 ───────────────\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "routes_json = os.path.join(base_path, \"routes_nodes.json\")\n",
    "output_demand_json = os.path.join(base_path, \"demand_template.json\")\n",
    "# ───────────────────────────────────\n",
    "\n",
    "# 1. routes_nodes.json 불러오기\n",
    "with open(routes_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    routes_nodes = json.load(f)\n",
    "\n",
    "# 2. demand 생성\n",
    "demand = {}\n",
    "\n",
    "for tr, path in routes_nodes.items():\n",
    "    od_list = []\n",
    "    for i in range(len(path)-1):\n",
    "        for j in range(i+1, len(path)):\n",
    "            o, d = path[i], path[j]\n",
    "            od_list.append((o, d, None))  # 수요는 아직 없음\n",
    "    demand[tr] = od_list\n",
    "\n",
    "# 3. 저장\n",
    "with open(output_demand_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(demand, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ demand_template.json 저장 완료 →\\n{output_demand_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42781bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 성공적으로 저장됨 → D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\demand_template_compact.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ─────────────── 설정 ───────────────\n",
    "base_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\"\n",
    "input_file  = os.path.join(base_path, \"demand_template.json\")\n",
    "output_file = os.path.join(base_path, \"demand_template_compact.json\")\n",
    "# ───────────────────────────────────\n",
    "\n",
    "# 1. 불러오기\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    demand = json.load(f)\n",
    "\n",
    "# 2. 저장 (수동 문자열 포맷)\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"{\\n\")\n",
    "    for i, (k, od_list) in enumerate(demand.items()):\n",
    "        f.write(f'    \"{k}\": [\\n')\n",
    "        line = \"        \"  # 들여쓰기 시작\n",
    "        for j, od in enumerate(od_list):\n",
    "            # JSON 값으로 인코딩 (ensure null/quote 등 맞춤)\n",
    "            od_json = json.dumps(od, ensure_ascii=False)\n",
    "            line += od_json\n",
    "            if j < len(od_list) - 1:\n",
    "                line += \", \"\n",
    "            if len(line) > 120:\n",
    "                f.write(line + \"\\n\")\n",
    "                line = \"        \"\n",
    "        if line.strip():\n",
    "            f.write(line + \"\\n\")\n",
    "        f.write(\"    ]\")\n",
    "        if i < len(demand) - 1:\n",
    "            f.write(\",\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")\n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"✅ 성공적으로 저장됨 → {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a2836b",
   "metadata": {},
   "source": [
    "#### 역사별 승하차데이터로 임시 journeys demand data 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "618ed86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\data\\od-matrix\\network-od-matrix-with-journeys-fl.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === 경로 설정 ===\n",
    "BASE_DIR = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\data\\od-matrix\"\n",
    "in_xlsx  = os.path.join(BASE_DIR, \"network-od-matrix-with-journeys.xlsx\")\n",
    "out_xlsx = os.path.join(BASE_DIR, \"network-od-matrix-with-journeys-fl.xlsx\")\n",
    "\n",
    "# === 1) 엑셀 읽기 ===\n",
    "df = pd.read_excel(in_xlsx)\n",
    "\n",
    "# === 2) node_path 처리 ===\n",
    "def get_first_last(path_str):\n",
    "    if pd.isna(path_str):\n",
    "        return None, None\n",
    "    parts = [p.strip() for p in str(path_str).split(\",\") if p.strip() != \"\"]\n",
    "    if not parts:\n",
    "        return None, None\n",
    "    if len(parts) == 1:\n",
    "        return parts[0], parts[0]\n",
    "    return parts[0], parts[-1]\n",
    "\n",
    "df[[\"start_node\", \"end_node\"]] = df[\"node_path\"].apply(\n",
    "    lambda x: pd.Series(get_first_last(x))\n",
    ")\n",
    "\n",
    "# === 3) 저장 (원본 열 + 추가 열 같이) ===\n",
    "df.to_excel(out_xlsx, index=False)\n",
    "\n",
    "print(f\"저장 완료: {out_xlsx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f5c43cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링 완료: 105813개 행 남음 → D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\data\\od-matrix\\network-od-matrix-with-journeys-fl-filtered.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === 경로 설정 ===\n",
    "BASE_DIR = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\"\n",
    "od_xlsx   = os.path.join(BASE_DIR, \"data\", \"od-matrix\", \"network-od-matrix-with-journeys-fl.xlsx\")\n",
    "node_xlsx = os.path.join(BASE_DIR, \"json\", \"qgis_export_uk.xlsx\")\n",
    "out_xlsx  = os.path.join(BASE_DIR, \"data\", \"od-matrix\", \"network-od-matrix-with-journeys-fl-filtered.xlsx\")\n",
    "\n",
    "# === 1) 데이터 읽기 ===\n",
    "df = pd.read_excel(od_xlsx)\n",
    "nodes = pd.read_excel(node_xlsx)\n",
    "\n",
    "# === 2) wgs84_node_id 집합 ===\n",
    "valid_nodes = set(nodes[\"wgs84_node_id\"].astype(str))  # 문자열 비교를 안전하게\n",
    "\n",
    "# === 3) start_node / end_node 값 비교 후 필터링 ===\n",
    "# start_node, end_node도 문자열로 변환\n",
    "df[\"start_node\"] = df[\"start_node\"].astype(str)\n",
    "df[\"end_node\"]   = df[\"end_node\"].astype(str)\n",
    "\n",
    "filtered = df[\n",
    "    (df[\"start_node\"].isin(valid_nodes)) | (df[\"end_node\"].isin(valid_nodes))\n",
    "]\n",
    "\n",
    "# === 4) 저장 ===\n",
    "filtered.to_excel(out_xlsx, index=False)\n",
    "\n",
    "print(f\"필터링 완료: {len(filtered)}개 행 남음 → {out_xlsx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77df676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "매핑 완료: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\data\\od-matrix\\network-od-matrix-with-journeys-fl-filtered-mapped.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === 경로 설정 ===\n",
    "BASE_DIR   = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\"\n",
    "od_xlsx    = os.path.join(BASE_DIR, \"data\", \"od-matrix\", \"network-od-matrix-with-journeys-fl-filtered.xlsx\")\n",
    "node_xlsx  = os.path.join(BASE_DIR, \"json\", \"qgis_export_uk.xlsx\")\n",
    "out_xlsx   = os.path.join(BASE_DIR, \"data\", \"od-matrix\", \"network-od-matrix-with-journeys-fl-filtered-mapped.xlsx\")\n",
    "\n",
    "# === 1) 데이터 읽기 ===\n",
    "df = pd.read_excel(od_xlsx)\n",
    "nodes = pd.read_excel(node_xlsx, sheet_name=\"NODE\")\n",
    "\n",
    "# === 2) 매핑 딕셔너리 (wgs84_node_id -> node_id) ===\n",
    "mapping = dict(zip(nodes[\"wgs84_node_id\"].astype(str), nodes[\"node_id\"]))\n",
    "\n",
    "# === 3) start_node/end_node 변환 ===\n",
    "df[\"o_node_id\"] = df[\"start_node\"].astype(str).map(mapping)\n",
    "df[\"d_node_id\"] = df[\"end_node\"].astype(str).map(mapping)\n",
    "\n",
    "# === 4) 저장 ===\n",
    "df.to_excel(out_xlsx, index=False)\n",
    "\n",
    "print(f\"매핑 완료: {out_xlsx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6238ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 둘 다 포함된 역 (matched):\n",
      " - 경산역\n",
      " - 계룡역\n",
      " - 곡성역\n",
      " - 공주역\n",
      " - 광명역\n",
      " - 광주송정역\n",
      " - 광천역\n",
      " - 구례구역\n",
      " - 구미역\n",
      " - 구포역\n",
      " - 기장역\n",
      " - 김제역\n",
      " - 김천구미역\n",
      " - 김천역\n",
      " - 나주역\n",
      " - 남원역\n",
      " - 논산역\n",
      " - 능주역\n",
      " - 단양역\n",
      " - 대구역\n",
      " - 대전역\n",
      " - 대천역\n",
      " - 덕소역\n",
      " - 동대구역\n",
      " - 동해역\n",
      " - 마산역\n",
      " - 목포역\n",
      " - 물금역\n",
      " - 밀양역\n",
      " - 벌교역\n",
      " - 보성역\n",
      " - 봉양역\n",
      " - 부산역\n",
      " - 삼랑진역\n",
      " - 상봉역\n",
      " - 상주역\n",
      " - 서울역\n",
      " - 서창역\n",
      " - 수원역\n",
      " - 순천역\n",
      " - 안동역\n",
      " - 양평역\n",
      " - 여수엑스포역\n",
      " - 영덕역\n",
      " - 영등포역\n",
      " - 영주역\n",
      " - 영천역\n",
      " - 예산역\n",
      " - 예천역\n",
      " - 오송역\n",
      " - 온양온천역\n",
      " - 용산역\n",
      " - 울산역\n",
      " - 원주역\n",
      " - 의성역\n",
      " - 익산역\n",
      " - 장성역\n",
      " - 장항역\n",
      " - 전주역\n",
      " - 점촌역\n",
      " - 정동진역\n",
      " - 정읍역\n",
      " - 제천역\n",
      " - 진주역\n",
      " - 천안역\n",
      " - 청량리역\n",
      " - 청주역\n",
      " - 춘양역\n",
      " - 충주역\n",
      " - 태화강역\n",
      " - 평창역\n",
      " - 평택역\n",
      " - 포항역\n",
      " - 풍기역\n",
      " - 횡성역\n",
      "\n",
      "📌 QGIS에는 있으나 승하차 데이터에는 없는 역 (only_in_qgis):\n",
      " - 백산역\n",
      " - 보천역\n",
      " - 삼척역\n",
      " - 신경주역\n",
      " - 천안아산역(온양온천)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ─────────────── 경로 설정 ───────────────\n",
    "base_dir = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\")\n",
    "qgis_path = base_dir / \"qgis_export_with_step.xlsx\"\n",
    "ridership_path = base_dir / \"station_ridership.xlsx\"\n",
    "# ────────────────────────────────────────\n",
    "\n",
    "# 1. QGIS NODE 데이터에서 RLNODE_NM 추출\n",
    "qgis_df = pd.read_excel(qgis_path, sheet_name=\"NODE\")\n",
    "qgis_nodes = qgis_df[\"RLNODE_NM\"].astype(str).str.strip()\n",
    "\n",
    "# 2. 승하차 데이터에서 RLNODE_NM 추출 (E열)\n",
    "ridership_df = pd.read_excel(ridership_path)\n",
    "ridership_nodes = ridership_df[\"RLNODE_NM\"].astype(str).str.strip()\n",
    "\n",
    "# 3. 비교\n",
    "matched = sorted(set(qgis_nodes) & set(ridership_nodes))\n",
    "only_in_qgis = sorted(set(qgis_nodes) - set(ridership_nodes))\n",
    "\n",
    "# 4. 출력\n",
    "print(\"📌 둘 다 포함된 역 (matched):\")\n",
    "for name in matched:\n",
    "    print(f\" - {name}\")\n",
    "\n",
    "print(\"\\n📌 QGIS에는 있으나 승하차 데이터에는 없는 역 (only_in_qgis):\")\n",
    "for name in only_in_qgis:\n",
    "    print(f\" - {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3eb01c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\demand_with_route_filled.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 파일 경로\n",
    "base_path = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\")\n",
    "file_path = base_path / \"qgis_export.xlsx\"\n",
    "\n",
    "# 엑셀 파일 읽기\n",
    "train_df = pd.read_excel(file_path, sheet_name=\"TRAIN\")\n",
    "demand_df = pd.read_excel(file_path, sheet_name=\"DEMAND\")\n",
    "\n",
    "# 노선 이름 → 노드 경로 dict (n 접두어 붙이기)\n",
    "line_routes = {}\n",
    "for _, row in train_df.iterrows():\n",
    "    line_name = row[\"RLWAY_NM\"].strip()\n",
    "    node_seq = [f\"n{n.strip()}\" for n in str(row[\"NODE\"]).split(\"-\")]\n",
    "    line_routes[line_name] = node_seq\n",
    "\n",
    "# 노선 이름에서 숫자 제거해서 base name으로 매핑\n",
    "def get_base_line(line):\n",
    "    return ''.join(filter(lambda x: not x.isdigit(), line)).strip()\n",
    "\n",
    "# OD에 대해 Route 생성 함수\n",
    "def extract_route(line, origin, destination):\n",
    "    base_line = get_base_line(line)\n",
    "    nodes = line_routes.get(base_line)\n",
    "    if not nodes:\n",
    "        return None\n",
    "    if origin not in nodes or destination not in nodes:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        idx_o = nodes.index(origin)\n",
    "        idx_d = nodes.index(destination)\n",
    "        # 정방향 또는 역방향 슬라이싱\n",
    "        if idx_o <= idx_d:\n",
    "            path = nodes[idx_o:idx_d + 1]\n",
    "        else:\n",
    "            path = nodes[idx_o:idx_d - 1:-1]  # 역방향\n",
    "        return \"-\".join(path)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Route 열 추가\n",
    "demand_df[\"Route\"] = demand_df.apply(\n",
    "    lambda row: extract_route(row[\"Line\"], row[\"Origin\"], row[\"Destination\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 저장\n",
    "output_path = base_path / \"demand_with_route_filled.xlsx\"\n",
    "demand_df.to_excel(output_path, index=False)\n",
    "print(f\"✅ 저장 완료: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fe683ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ 저장 완료: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export_with_node_id.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ───────────── 경로 설정 ─────────────\n",
    "base_dir = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\")\n",
    "xlsx_path = base_dir / \"qgis_export.xlsx\"\n",
    "output_path = base_dir / \"qgis_export_with_node_id.xlsx\"\n",
    "# ─────────────────────────────────────\n",
    "\n",
    "# 1. NODE 시트 읽기 (참조용)\n",
    "node_df = pd.read_excel(xlsx_path, sheet_name=\"NODE\")\n",
    "node_df = node_df[[\"RLNODE_NM\", \"node_id\"]].dropna()\n",
    "node_df[\"RLNODE_NM\"] = node_df[\"RLNODE_NM\"].astype(str).str.strip()\n",
    "\n",
    "# 2. BOARD_ALIGHT 시트에 node_id 매칭\n",
    "df = pd.read_excel(xlsx_path, sheet_name=\"BOARD_ALIGHT\")\n",
    "df[\"RLNODE_NM\"] = df[\"RLNODE_NM\"].astype(str).str.strip()\n",
    "\n",
    "# 매칭 수행\n",
    "merged_df = df.merge(node_df, on=\"RLNODE_NM\", how=\"left\")\n",
    "\n",
    "# 3. 저장\n",
    "with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
    "    merged_df.to_excel(writer, sheet_name=\"BOARD_ALIGHT\", index=False)\n",
    "\n",
    "print(f\"✔️ 저장 완료: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "639d0058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료: NODE 시트에 board/alight 갱신\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === 경로 설정 ===\n",
    "BASE_DIR = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\"\n",
    "od_fp    = os.path.join(BASE_DIR, \"data\", \"od-matrix\", \"network-od-matrix-with-journeys-fl-filtered-mapped.xlsx\")\n",
    "node_fp  = os.path.join(BASE_DIR, \"json\", \"qgis_export_uk.xlsx\")\n",
    "\n",
    "# === 1) 데이터 읽기 ===\n",
    "od = pd.read_excel(od_fp)                       # o_node_id, d_node_id, journeys 포함\n",
    "nodes = pd.read_excel(node_fp, sheet_name=\"NODE\")  # node_id 포함\n",
    "\n",
    "# 안전한 dtype 정리\n",
    "od[\"o_node_id\"] = pd.to_numeric(od[\"o_node_id\"], errors=\"coerce\")\n",
    "od[\"d_node_id\"] = pd.to_numeric(od[\"d_node_id\"], errors=\"coerce\")\n",
    "od[\"journeys\"]  = pd.to_numeric(od[\"journeys\"],  errors=\"coerce\").fillna(0)\n",
    "\n",
    "# === 2) 집계: o_node_id/d_node_id별 journeys 합 ===\n",
    "board_sum  = od.dropna(subset=[\"o_node_id\"]).groupby(\"o_node_id\")[\"journeys\"].sum()\n",
    "alight_sum = od.dropna(subset=[\"d_node_id\"]).groupby(\"d_node_id\")[\"journeys\"].sum()\n",
    "\n",
    "# === 3) NODE 시트에 매핑 (없으면 0) ===\n",
    "nodes[\"board\"]  = nodes[\"node_id\"].map(board_sum).fillna(0)\n",
    "nodes[\"alight\"] = nodes[\"node_id\"].map(alight_sum).fillna(0)\n",
    "\n",
    "# 정수로 원하면 아래 주석 해제\n",
    "# nodes[\"board\"]  = nodes[\"board\"].round(0).astype(int)\n",
    "# nodes[\"alight\"] = nodes[\"alight\"].round(0).astype(int)\n",
    "\n",
    "# === 4) 저장: NODE 시트만 교체, 다른 시트 보존 ===\n",
    "try:\n",
    "    # pandas>=1.4 권장\n",
    "    with pd.ExcelWriter(node_fp, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as w:\n",
    "        nodes.to_excel(w, sheet_name=\"NODE\", index=False)\n",
    "except TypeError:\n",
    "    # if_sheet_exists 미지원 pandas 대응: 전체를 새 파일에 써서 교체\n",
    "    xls = pd.ExcelFile(node_fp)\n",
    "    sheets = {sn: pd.read_excel(node_fp, sheet_name=sn) for sn in xls.sheet_names}\n",
    "    sheets[\"NODE\"] = nodes\n",
    "    with pd.ExcelWriter(node_fp, engine=\"openpyxl\", mode=\"w\") as w:\n",
    "        for sn, df in sheets.items():\n",
    "            df.to_excel(w, sheet_name=sn, index=False)\n",
    "\n",
    "print(\"완료: NODE 시트에 board/alight 갱신\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f8a6fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료. 결과 저장: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\qgis_export_uk_with_journeys.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "INPUT_XLSX = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\qgis_export_uk.xlsx\"\n",
    "OUTPUT_XLSX = os.path.splitext(INPUT_XLSX)[0] + \"_with_journeys.xlsx\"\n",
    "\n",
    "# === helper: 시트 자동 탐지 ===\n",
    "def find_sheet_with_columns(sheets_dict, required_cols):\n",
    "    for name, df in sheets_dict.items():\n",
    "        cols = {c.strip().lower() for c in df.columns}\n",
    "        if required_cols.issubset(cols):\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "def find_col(df, target):\n",
    "    t = target.strip().lower()\n",
    "    for c in df.columns:\n",
    "        if c.strip().lower() == t:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def norm_node(tok):\n",
    "    # 'n46' -> '46' ; if already numeric string, keep it\n",
    "    if pd.isna(tok): return \"\"\n",
    "    s = str(tok).strip()\n",
    "    if s.lower().startswith('n'):\n",
    "        return s[1:].strip()\n",
    "    return s\n",
    "\n",
    "# === 1) 읽기(모든 시트) ===\n",
    "sheets = pd.read_excel(INPUT_XLSX, sheet_name=None, dtype=str, engine='openpyxl')\n",
    "\n",
    "# === 2) 시트 찾기 ===\n",
    "demand_sheet = find_sheet_with_columns(sheets, {'route','origin','destination'})\n",
    "node_sheet = find_sheet_with_columns(sheets, {'rlnode_nm','node_id','board','alight'})\n",
    "\n",
    "if demand_sheet is None or node_sheet is None:\n",
    "    raise RuntimeError(\"DEMAND 시트 또는 NODE 시트를 찾을 수 없음. 컬럼 이름을 확인하세요.\")\n",
    "\n",
    "df_demand = sheets[demand_sheet].copy()\n",
    "df_node = sheets[node_sheet].copy()\n",
    "\n",
    "# 컬럼 실제 이름 찾기\n",
    "route_col = find_col(df_demand, 'route')\n",
    "origin_col = find_col(df_demand, 'origin')\n",
    "dest_col = find_col(df_demand, 'destination')\n",
    "\n",
    "node_id_col = find_col(df_node, 'node_id')\n",
    "board_col = find_col(df_node, 'board')\n",
    "alight_col = find_col(df_node, 'alight')\n",
    "\n",
    "# 숫자형으로 변환(가능한 경우)\n",
    "df_node[node_id_col] = df_node[node_id_col].astype(str).str.strip()\n",
    "df_node[board_col] = pd.to_numeric(df_node[board_col], errors='coerce').fillna(0.0)\n",
    "df_node[alight_col] = pd.to_numeric(df_node[alight_col], errors='coerce').fillna(0.0)\n",
    "\n",
    "# node dicts\n",
    "board_by_node = { str(r[node_id_col]).strip(): float(r[board_col]) for _, r in df_node.iterrows() }\n",
    "alight_by_node = { str(r[node_id_col]).strip(): float(r[alight_col]) for _, r in df_node.iterrows() }\n",
    "\n",
    "# === 옵션: station_line_share 매핑 (node_id -> fraction for this route)\n",
    "# 만약 제공 가능하면 route별로 (예: route identifier(또는 Line))에 대해 station별 분배비를 주면 더 정확함.\n",
    "# 기본: 모든 board/alight를 해당 route에 전부 사용 (즉 fraction=1.0)\n",
    "# 사용자가 제공하면 아래 dict 형태: station_share = {'46':0.3, '35':0.5, ...}\n",
    "# 예: station_share = {}  # 비워두면 1.0으로 처리\n",
    "station_share = {}  # 사용자가 채워 넣으면 됨\n",
    "\n",
    "def get_station_share(node_id):\n",
    "    # 반환: 0..1\n",
    "    return float(station_share.get(str(node_id).strip(), 1.0))\n",
    "\n",
    "# === 방법 A: 비례할당 함수 ===\n",
    "def journeys_proportional(route_tokens, origin_node, dest_node):\n",
    "    # route_tokens: ['n46','n35','n31',...]\n",
    "    nodes_after_origin = []\n",
    "    found_origin = False\n",
    "    for tok in route_tokens:\n",
    "        nid = norm_node(tok)\n",
    "        if not found_origin:\n",
    "            if nid == origin_node:\n",
    "                found_origin = True\n",
    "            continue\n",
    "        # origin found previously -> collect downstream nodes including first after origin\n",
    "        nodes_after_origin.append(nid)\n",
    "    # if destination not in downstream, return NaN\n",
    "    if dest_node not in nodes_after_origin or not nodes_after_origin:\n",
    "        return np.nan\n",
    "    # sum of alight over downstream nodes, with optional station_share\n",
    "    sum_alights = sum(alight_by_node.get(nid, 0.0) * get_station_share(nid) for nid in nodes_after_origin)\n",
    "    if sum_alights == 0:\n",
    "        return np.nan\n",
    "    board_origin = board_by_node.get(origin_node, 0.0) * get_station_share(origin_node)\n",
    "    dest_alight = alight_by_node.get(dest_node, 0.0) * get_station_share(dest_node)\n",
    "    return board_origin * (dest_alight / sum_alights)\n",
    "\n",
    "# === 방법 B: 경로 흐름 시뮬레이션 함수 ===\n",
    "def simulate_route_od(route_tokens):\n",
    "    \"\"\"\n",
    "    route_tokens: ['n46','n35','n31', ...] (order of stops along the route)\n",
    "    returns: od_counts dict keyed by ('46','35') -> float journeys\n",
    "    \"\"\"\n",
    "    # normalize tokens to node ids\n",
    "    nodes = [norm_node(tok) for tok in route_tokens]\n",
    "    # active pools: list of (origin_node, remaining_count)\n",
    "    active = []  # list of dicts {'origin':id, 'rem':float}\n",
    "    od_counts = {}  # (origin, dest) -> float\n",
    "\n",
    "    for idx, nid in enumerate(nodes):\n",
    "        # 1) boarding at this station (scaled by station_share)\n",
    "        b = board_by_node.get(nid, 0.0) * get_station_share(nid)\n",
    "        if b > 0:\n",
    "            active.append({'origin': nid, 'rem': float(b)})\n",
    "\n",
    "        # 2) alighting at this station\n",
    "        al = alight_by_node.get(nid, 0.0) * get_station_share(nid)\n",
    "        if al <= 0 or not active:\n",
    "            continue\n",
    "\n",
    "        total_onboard = sum(a['rem'] for a in active)\n",
    "        # guard\n",
    "        if total_onboard <= 0:\n",
    "            continue\n",
    "\n",
    "        # allocate alighting proportionally to each active origin pool\n",
    "        for a in active:\n",
    "            if a['rem'] <= 0:\n",
    "                continue\n",
    "            share = a['rem'] / total_onboard\n",
    "            al_from_origin = share * al\n",
    "            # decrement\n",
    "            a['rem'] -= al_from_origin\n",
    "            key = (a['origin'], nid)\n",
    "            od_counts[key] = od_counts.get(key, 0.0) + al_from_origin\n",
    "\n",
    "        # drop any active pools that are now near-zero\n",
    "        active = [a for a in active if a['rem'] > 1e-9]\n",
    "\n",
    "    return od_counts\n",
    "\n",
    "# === 3) DEMAND 표에 두 방법 적용 ===\n",
    "journeys_prop = []\n",
    "journeys_flow = []\n",
    "missing_info_prop = []\n",
    "missing_info_flow = []\n",
    "\n",
    "for _, row in df_demand.iterrows():\n",
    "    route = row.get(route_col, \"\")\n",
    "    if pd.isna(route) or str(route).strip() == \"\":\n",
    "        journeys_prop.append(np.nan)\n",
    "        journeys_flow.append(np.nan)\n",
    "        missing_info_prop.append(\"no route\")\n",
    "        missing_info_flow.append(\"no route\")\n",
    "        continue\n",
    "\n",
    "    tokens = [t.strip() for t in str(route).split('-') if t.strip() != \"\"]\n",
    "    origin_tok = norm_node(row.get(origin_col, \"\"))\n",
    "    dest_tok = norm_node(row.get(dest_col, \"\"))\n",
    "\n",
    "    # --- proportional ---\n",
    "    try:\n",
    "        j_prop = journeys_proportional(tokens, origin_tok, dest_tok)\n",
    "        if pd.isna(j_prop):\n",
    "            missing_info_prop.append(\"insufficient alight/board or dest not downstream\")\n",
    "        else:\n",
    "            missing_info_prop.append(\"\")\n",
    "    except Exception as e:\n",
    "        j_prop = np.nan\n",
    "        missing_info_prop.append(str(e))\n",
    "    journeys_prop.append(j_prop)\n",
    "\n",
    "    # --- flow simulation ---\n",
    "    try:\n",
    "        od_counts = simulate_route_od(tokens)\n",
    "        j_flow = od_counts.get((origin_tok, dest_tok), 0.0)\n",
    "        # if result is 0 but data suggests missing nodes, mark note\n",
    "        if j_flow == 0.0:\n",
    "            # detect if origin or dest absent in nodes\n",
    "            if origin_tok not in [norm_node(t) for t in tokens] or dest_tok not in [norm_node(t) for t in tokens]:\n",
    "                missing_info_flow.append(\"origin/dest missing in route\")\n",
    "            else:\n",
    "                # zero might be truly zero (alight zero etc.)\n",
    "                missing_info_flow.append(\"\")\n",
    "        else:\n",
    "            missing_info_flow.append(\"\")\n",
    "    except Exception as e:\n",
    "        j_flow = np.nan\n",
    "        missing_info_flow.append(str(e))\n",
    "    journeys_flow.append(j_flow)\n",
    "\n",
    "# attach to df_demand\n",
    "df_demand['journeys_prop'] = journeys_prop\n",
    "df_demand['journeys_flow'] = journeys_flow\n",
    "df_demand['note_prop'] = missing_info_prop\n",
    "df_demand['note_flow'] = missing_info_flow\n",
    "\n",
    "# write back (preserve other sheets)\n",
    "sheets_out = sheets.copy()\n",
    "sheets_out[demand_sheet] = df_demand\n",
    "\n",
    "with pd.ExcelWriter(OUTPUT_XLSX, engine='openpyxl', mode='w') as writer:\n",
    "    for name, df in sheets_out.items():\n",
    "        sheetname = name if len(name) <= 31 else name[:31]\n",
    "        df.to_excel(writer, sheet_name=sheetname, index=False)\n",
    "\n",
    "print(\"완료. 결과 저장:\", OUTPUT_XLSX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d34b2",
   "metadata": {},
   "source": [
    "#### Journeys data 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df60ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Worksheet named 'BOARD_ALIGHT' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m eps_conv    = \u001b[32m1e-6\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ──────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 1. 데이터 읽기 -----------------------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m bal    = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBOARD_ALIGHT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m            \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnode_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mboard_1d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43malight_1d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m demand = pd.read_excel(file_path, sheet_name=\u001b[33m\"\u001b[39m\u001b[33mDEMAND\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 1‑a. Origin / Destination ⇒ 정수 node_id\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\mjkang\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:508\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     data = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\mjkang\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1616\u001b[39m, in \u001b[36mExcelFile.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\n\u001b[32m   1577\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1578\u001b[39m     sheet_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1596\u001b[39m     **kwds,\n\u001b[32m   1597\u001b[39m ) -> DataFrame | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[32m   1598\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[33;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[32m   1600\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1614\u001b[39m \u001b[33;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[32m   1615\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\mjkang\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:773\u001b[39m, in \u001b[36mBaseExcelReader.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReading sheet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masheetname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(asheetname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     sheet = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43masheetname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume an integer if not a string\u001b[39;00m\n\u001b[32m    775\u001b[39m     sheet = \u001b[38;5;28mself\u001b[39m.get_sheet_by_index(asheetname)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\mjkang\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:582\u001b[39m, in \u001b[36mOpenpyxlReader.get_sheet_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_if_bad_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.book[name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\mjkang\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:624\u001b[39m, in \u001b[36mBaseExcelReader.raise_if_bad_sheet_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_if_bad_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sheet_names:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWorksheet named \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Worksheet named 'BOARD_ALIGHT' not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ────── CONFIG ────────────────────────────────────────────────────────────\n",
    "file_path   = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export.xlsx\")\n",
    "output_path = file_path.with_stem(file_path.stem + \"_with_journeys\")\n",
    "\n",
    "min_board   = 1      # 승차가 0/누락인 역에 부여할 최소 승차 인원\n",
    "min_alight  = 1      # 하차가 0/누락인 역에 부여할 최소 하차 인원\n",
    "epsilon_od  = 1      # 유효 OD(승·하차>0)에 심을 최소 통행량\n",
    "max_iter    = 50\n",
    "eps_conv    = 1e-6\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1. 데이터 읽기 -----------------------------------------------------------\n",
    "bal    = pd.read_excel(\n",
    "            file_path, sheet_name=\"BOARD_ALIGHT\",\n",
    "            usecols=[\"node_id\", \"board_1d\", \"alight_1d\"]\n",
    "        )\n",
    "demand = pd.read_excel(file_path, sheet_name=\"DEMAND\")\n",
    "\n",
    "# 1‑a. Origin / Destination ⇒ 정수 node_id\n",
    "demand[\"O_id\"] = demand[\"Origin\"].str.lstrip(\"n\").astype(int)\n",
    "demand[\"D_id\"] = demand[\"Destination\"].str.lstrip(\"n\").astype(int)\n",
    "\n",
    "# 2. BOARD_ALIGHT 누락/중복 처리 ------------------------------------------\n",
    "#   2‑a. 중복 node_id 합산\n",
    "bal_agg = (bal.groupby(\"node_id\", as_index=False)\n",
    "               .agg(board=(\"board_1d\", \"sum\"),\n",
    "                    alight=(\"alight_1d\", \"sum\")))\n",
    "\n",
    "#   2‑b. DEMAND 에 등장하지만 bal 에 없는 노드 추가\n",
    "all_nodes = pd.unique(demand[[\"O_id\", \"D_id\"]].values.ravel())\n",
    "bal_full  = pd.DataFrame({\"node_id\": all_nodes}).merge(\n",
    "                bal_agg, on=\"node_id\", how=\"left\"\n",
    "            )\n",
    "\n",
    "#   2‑c. 승·하차 0/NaN → 최소값 주입\n",
    "bal_full[\"board\"]  = pd.to_numeric(bal_full[\"board\"],  errors=\"coerce\").fillna(0)\n",
    "bal_full[\"alight\"] = pd.to_numeric(bal_full[\"alight\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "bal_full.loc[bal_full[\"board\"]  == 0, \"board\"]  = min_board\n",
    "bal_full.loc[bal_full[\"alight\"] == 0, \"alight\"] = min_alight\n",
    "\n",
    "# 3. 총 승·하차량 매핑 ------------------------------------------------------\n",
    "board_map  = bal_full.set_index(\"node_id\")[\"board\"].to_dict()\n",
    "alight_map = bal_full.set_index(\"node_id\")[\"alight\"].to_dict()\n",
    "\n",
    "demand[\"o_total\"] = demand[\"O_id\"].map(board_map)\n",
    "demand[\"d_total\"] = demand[\"D_id\"].map(alight_map)\n",
    "\n",
    "# 4. 초기 OD 행렬 + ε 주입 ---------------------------------------------------\n",
    "f = 1.0 / demand[\"Timestep\"].replace(0, np.nan)\n",
    "demand[\"T\"] = (demand[\"o_total\"] * demand[\"d_total\"] * f).fillna(0.0)\n",
    "\n",
    "valid_od = (demand[\"o_total\"] > 0) & (demand[\"d_total\"] > 0)\n",
    "demand.loc[valid_od, \"T\"] += epsilon_od      # 최소 통행량 심기\n",
    "\n",
    "# 5. IPF (Iterative Proportional Fitting) ----------------------------------\n",
    "for _ in range(max_iter):\n",
    "    row_sum = demand.groupby(\"O_id\")[\"T\"].transform(\"sum\").replace(0, np.nan)\n",
    "    demand[\"T\"] *= demand[\"o_total\"] / row_sum\n",
    "\n",
    "    col_sum = demand.groupby(\"D_id\")[\"T\"].transform(\"sum\").replace(0, np.nan)\n",
    "    demand[\"T\"] *= demand[\"d_total\"] / col_sum\n",
    "\n",
    "    if max((row_sum - demand[\"o_total\"]).abs().max(),\n",
    "           (col_sum - demand[\"d_total\"]).abs().max()) < eps_conv:\n",
    "        break\n",
    "\n",
    "# 6. journeys 계산 및 후처리 -------------------------------------------------\n",
    "demand[\"journeys\"] = demand[\"T\"].round(0).astype(int)   # 정수(명)로 반올림\n",
    "out = demand.drop(columns=[\"O_id\", \"D_id\", \"o_total\", \"d_total\", \"T\"])\n",
    "\n",
    "# 7. 결과 저장 --------------------------------------------------------------\n",
    "if output_path.exists():\n",
    "    mode, sheet_opt = \"a\", \"overlay\"\n",
    "else:\n",
    "    mode, sheet_opt = \"w\", None                 # 새 파일 — 시트 존재 안 하므로 옵션 불필요\n",
    "\n",
    "with pd.ExcelWriter(output_path,\n",
    "                    engine=\"openpyxl\",\n",
    "                    mode=mode,\n",
    "                    if_sheet_exists=sheet_opt) as wr:\n",
    "    out.to_excel(wr, sheet_name=\"DEMAND\", index=False)\n",
    "\n",
    "print(\"✅ journeys 열이 채워진 파일:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b83a59b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modified file saved to: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export_journeys_fixed.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 파일 경로\n",
    "file_path = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export.xlsx\")\n",
    "output_path = file_path.with_stem(file_path.stem + \"_journeys_fixed\")\n",
    "\n",
    "# DEMAND 시트 읽기\n",
    "demand = pd.read_excel(file_path, sheet_name=\"DEMAND\")\n",
    "\n",
    "# journeys 열에서 0인 값을 1로 변경\n",
    "demand[\"journeys\"] = demand[\"journeys\"].apply(lambda x: 1 if x == 0 else x)\n",
    "\n",
    "# 새 파일로 저장\n",
    "with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
    "    demand.to_excel(writer, sheet_name=\"DEMAND\", index=False)\n",
    "\n",
    "print(f\"✅ Modified file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d9a7c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 노선별 최대 탑승 인원 =====\n",
      "  Line      MaxLoad\n",
      " 1AWC1 62681.455319\n",
      " 1AWC2  2406.977529\n",
      " 1EMR1 53575.899146\n",
      " 1EMR2   957.062056\n",
      "  1GA1 63004.822775\n",
      "  1GA2  1607.179558\n",
      " 1GWR1 43215.627992\n",
      " 1GWR2  2646.727970\n",
      "1LNER1 71650.412764\n",
      "1LNER2  4253.587918\n",
      "  1SR1 33968.707205\n",
      "  1SR2  1601.238405\n",
      " 1TPE1  1422.917239\n",
      " 1TPE2  1747.935970\n",
      "  1TW1   768.871046\n",
      "  1TW2  2015.183499\n",
      " 2AWC1 79518.803233\n",
      " 2AWC2  1156.568444\n",
      " 2EMR1 70647.169439\n",
      " 2EMR2  2573.045776\n",
      " 2GWR1 37607.893574\n",
      " 2GWR2  1813.418767\n",
      "2LNER1 72429.739161\n",
      "2LNER2  4393.017267\n",
      "  2SR1 22464.011084\n",
      "  2SR2  1131.721169\n",
      "  2TW1  1785.686930\n",
      "  2TW2   968.922909\n",
      " 3EMR1 53157.661529\n",
      " 3EMR2   877.425438\n",
      " 3GWR1 27639.195702\n",
      " 3GWR2   701.036900\n",
      "  3SR1 46573.040225\n",
      "  3SR2  1023.326345\n",
      " 4EMR1 57122.331480\n",
      " 4EMR2  1393.145636\n",
      "\n",
      "===== 최대 탑승 Edge 상세(선택) =====\n",
      "  Line From  To         Load\n",
      " 1AWC1  n46 n35 62681.455319\n",
      " 1AWC2  n15 n20  2406.977529\n",
      " 1EMR1  n46 n27 53575.899146\n",
      " 1EMR2  n22 n27   957.062056\n",
      "  1GA1  n46 n39 63004.822775\n",
      "  1GA2  n33 n36  1607.179558\n",
      " 1GWR1  n46 n45 43215.627992\n",
      " 1GWR2  n41 n42  2646.727970\n",
      "1LNER1  n46 n28 71650.412764\n",
      "1LNER2   n6  n8  4253.587918\n",
      "  1SR1  n46 n55 33968.707205\n",
      "  1SR2  n55 n46  1601.238405\n",
      " 1TPE1  n14 n13  1422.917239\n",
      " 1TPE2  n15 n13  1747.935970\n",
      "  1TW1  n41 n40   768.871046\n",
      "  1TW2  n41 n42  2015.183499\n",
      " 2AWC1  n46 n35 79518.803233\n",
      " 2AWC2  n21 n24  1156.568444\n",
      " 2EMR1  n46 n27 70647.169439\n",
      " 2EMR2  n15 n20  2573.045776\n",
      " 2GWR1  n46 n45 37607.893574\n",
      " 2GWR2  n48 n44  1813.418767\n",
      "2LNER1  n46 n28 72429.739161\n",
      "2LNER2   n6  n8  4393.017267\n",
      "  2SR1  n46 n55 22464.011084\n",
      "  2SR2  n55 n46  1131.721169\n",
      "  2TW1  n15 n25  1785.686930\n",
      "  2TW2  n43 n37   968.922909\n",
      " 3EMR1  n46 n27 53157.661529\n",
      " 3EMR2  n22 n27   877.425438\n",
      " 3GWR1  n46 n45 27639.195702\n",
      " 3GWR2  n52 n51   701.036900\n",
      "  3SR1  n46 n49 46573.040225\n",
      "  3SR2  n57 n51  1023.326345\n",
      " 4EMR1  n46 n27 57122.331480\n",
      " 4EMR2  n23 n22  1393.145636\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# ────────── 1. 입력 경로 및 시트 읽기 ──────────\n",
    "file_path = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\qgis_export_uk.xlsx\")\n",
    "df = pd.read_excel(file_path, sheet_name=\"DEMAND\",\n",
    "                   usecols=[\"Line\", \"Route\", \"journeys30\"])\n",
    "\n",
    "# ────────── 2. Edge‑별 탑승 인원 누적 ──────────\n",
    "edge_load = defaultdict(int)           # {(Line, from, to): 총탑승}\n",
    "\n",
    "for line, route, pax in df.itertuples(index=False):\n",
    "    nodes = route.split(\"-\")           # ex) [\"n4\",\"n3\",\"n1\",...]\n",
    "    for frm, to in zip(nodes[:-1], nodes[1:]):\n",
    "        edge_load[(line, frm, to)] += pax   # 방향 구분 O (필요 없으면 정렬해서 key 통일)\n",
    "\n",
    "# DataFrame 화\n",
    "edge_df = (pd.DataFrame([(l,f,t,c) for (l,f,t),c in edge_load.items()],\n",
    "                        columns=[\"Line\",\"From\",\"To\",\"Load\"]))\n",
    "\n",
    "# ────────── 3. 노선별 최대 탑승 인원 계산 ──────────\n",
    "max_by_line = (edge_df.groupby(\"Line\")[\"Load\"]\n",
    "                        .agg(MaxLoad=\"max\")\n",
    "                        .reset_index())\n",
    "\n",
    "# (선택) 어느 Edge 에서 최대가 나왔는지도 보고 싶다면:\n",
    "idx = edge_df.groupby(\"Line\")[\"Load\"].idxmax()\n",
    "edge_peak = edge_df.loc[idx].reset_index(drop=True)   # Line, From, To, Load\n",
    "\n",
    "# ────────── 4. 결과 저장 or 출력 ──────────\n",
    "print(\"\\n===== 노선별 최대 탑승 인원 =====\")\n",
    "print(max_by_line.to_string(index=False))\n",
    "\n",
    "print(\"\\n===== 최대 탑승 Edge 상세(선택) =====\")\n",
    "print(edge_peak.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24b6a5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 필요 편성 수 요약 ===\n",
      "Line         Op          MaxLoad    Cap  #Trains Note\n",
      "1AWC1        AWC        62681.46    607      104 \n",
      "1AWC2        AWC         2406.98    607        4 \n",
      "1EMR1        EMR        53575.90    301      178 \n",
      "1EMR2        EMR          957.06    301        4 \n",
      "1GA1         GA         63004.82    704       90 \n",
      "1GA2         GA          1607.18    704        3 \n",
      "1GWR1        GWR        43215.63    650       67 \n",
      "1GWR2        GWR         2646.73    650        5 \n",
      "1LNER1       LNER       71650.41    600      120 \n",
      "1LNER2       LNER        4253.59    600        8 \n",
      "1SR1         SR         33968.71    330      103 \n",
      "1SR2         SR          1601.24    330        5 \n",
      "1TPE1        TPE         1422.92    647        3 \n",
      "1TPE2        TPE         1747.94    647        3 \n",
      "1TW1         TW           768.87    200        4 \n",
      "1TW2         TW          2015.18    200       11 \n",
      "2AWC1        AWC        79518.80    607      132 \n",
      "2AWC2        AWC         1156.57    607        2 \n",
      "2EMR1        EMR        70647.17    301      235 \n",
      "2EMR2        EMR         2573.05    301        9 \n",
      "2GWR1        GWR        37607.89    650       58 \n",
      "2GWR2        GWR         1813.42    650        3 \n",
      "2LNER1       LNER       72429.74    600      121 \n",
      "2LNER2       LNER        4393.02    600        8 \n",
      "2SR1         SR         22464.01    330       69 \n",
      "2SR2         SR          1131.72    330        4 \n",
      "2TW1         TW          1785.69    200        9 \n",
      "2TW2         TW           968.92    200        5 \n",
      "3EMR1        EMR        53157.66    301      177 \n",
      "3EMR2        EMR          877.43    301        3 \n",
      "3GWR1        GWR        27639.20    650       43 \n",
      "3GWR2        GWR          701.04    650        2 \n",
      "3SR1         SR         46573.04    330      142 \n",
      "3SR2         SR          1023.33    330        4 \n",
      "4EMR1        EMR        57122.33    301      190 \n",
      "4EMR2        EMR         1393.15    301        5 \n",
      "\n",
      "✅ demand split JSON 저장 완료 → D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\\demand_split_by_capacity.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "노선별 최대탑승값(주어진 리스트) + capacity_map -> 필요한 편성 수 계산\n",
    "그리고 demand.json이 있으면 노선별로 편성 수만큼 균등분할하여 demand_split.json으로 저장 (옵션).\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------------- user inputs ----------------\n",
    "# capacity_map: 사용자가 제공한 값(수정 금지)\n",
    "capacity_map = {\n",
    "    \"AWC\": 607,   # Avanti West Coast (Class 390 Pendolino 최대형 기준 예시)\n",
    "    \"EMR\": 301,   # East Midlands Railway (Class 810 등 예시)\n",
    "    \"GA\" : 704,   # Greater Anglia (Class 745 12-car 예시)\n",
    "    \"GWR\": 650,   # Great Western Railway (Class 800/802 대형 편성 예시)\n",
    "    \"LNER\":600,   # LNER (Azuma 대형 편성 예시)\n",
    "    \"SR\": 330,    # Southern (Class 377 등 4~5-car 기준 예시)\n",
    "    \"TPE\":647,    # TransPennine Express (대형 편성 예시)\n",
    "    \"TW\": 200,    # Transport for Wales (TfW, 2~3-car 예시)\n",
    "    # 필요하면 여기에 추가 매핑\n",
    "}\n",
    "DEFAULT_CAPACITY = 600   # 매핑 없을 때 보수적 기본값\n",
    "\n",
    "# 주어진 노선별 최대탑승값 (사용자가 제공한 리스트를 그대로 dict로 만듦)\n",
    "# 문자열 키는 앞/뒤 숫자를 포함한 원본 노선명(예: '1AWC1')\n",
    "max_loads_raw = {\n",
    "\"1AWC1\": 62681.455319,\n",
    "\"1AWC2\": 2406.977529,\n",
    "\"1EMR1\": 53575.899146,\n",
    "\"1EMR2\": 957.062056,\n",
    "\"1GA1\": 63004.822775,\n",
    "\"1GA2\": 1607.179558,\n",
    "\"1GWR1\": 43215.627992,\n",
    "\"1GWR2\": 2646.727970,\n",
    "\"1LNER1\": 71650.412764,\n",
    "\"1LNER2\": 4253.587918,\n",
    "\"1SR1\": 33968.707205,\n",
    "\"1SR2\": 1601.238405,\n",
    "\"1TPE1\": 1422.917239,\n",
    "\"1TPE2\": 1747.935970,\n",
    "\"1TW1\": 768.871046,\n",
    "\"1TW2\": 2015.183499,\n",
    "\"2AWC1\": 79518.803233,\n",
    "\"2AWC2\": 1156.568444,\n",
    "\"2EMR1\": 70647.169439,\n",
    "\"2EMR2\": 2573.045776,\n",
    "\"2GWR1\": 37607.893574,\n",
    "\"2GWR2\": 1813.418767,\n",
    "\"2LNER1\": 72429.739161,\n",
    "\"2LNER2\": 4393.017267,\n",
    "\"2SR1\": 22464.011084,\n",
    "\"2SR2\": 1131.721169,\n",
    "\"2TW1\": 1785.686930,\n",
    "\"2TW2\": 968.922909,\n",
    "\"3EMR1\": 53157.661529,\n",
    "\"3EMR2\": 877.425438,\n",
    "\"3GWR1\": 27639.195702,\n",
    "\"3GWR2\": 701.036900,\n",
    "\"3SR1\": 46573.040225,\n",
    "\"3SR2\": 1023.326345,\n",
    "\"4EMR1\": 57122.331480,\n",
    "\"4EMR2\": 1393.145636,\n",
    "}\n",
    "\n",
    "# 파일 경로 (demand.json 불러와 분할하려면 경로를 올바르게 설정)\n",
    "JSON_IN  = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\\demand.json\")\n",
    "JSON_OUT = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\\demand_split_by_capacity.json\")\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def extract_letters(s: str) -> str:\n",
    "    \"\"\"'1AWC1' -> 'AWC' 처럼 연속된 영문자 그룹 중 첫번째를 대문자로 반환\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    m = re.search(r\"[A-Za-z]+\", s)\n",
    "    return m.group(0).upper() if m else \"\"\n",
    "\n",
    "# ---------------- main 계산 ----------------\n",
    "results = []\n",
    "train_count_map = {}   # key: original line string -> n_trains\n",
    "capacity_used_map = {} # key: original line -> capacity used\n",
    "\n",
    "for line_key, maxload in max_loads_raw.items():\n",
    "    op = extract_letters(line_key)          # e.g., '1AWC1' -> 'AWC'\n",
    "    if not op:\n",
    "        cap = DEFAULT_CAPACITY\n",
    "        note = \"no_letters_found -> default cap used\"\n",
    "    else:\n",
    "        cap = capacity_map.get(op, DEFAULT_CAPACITY)\n",
    "        note = \"\" if op in capacity_map else f\"operator '{op}' not in capacity_map -> default cap used\"\n",
    "    # 필요한 편성 수 (올림), 최소 1\n",
    "    n_trains = max(1, math.ceil(float(maxload) / float(cap))) if cap > 0 else 1\n",
    "    results.append((line_key, op, float(maxload), int(cap), int(n_trains), note))\n",
    "    train_count_map[line_key] = int(n_trains)\n",
    "    capacity_used_map[line_key] = int(cap)\n",
    "\n",
    "# 출력(정렬해서 보기 좋게)\n",
    "print(\"\\n=== 필요 편성 수 요약 ===\")\n",
    "print(f\"{'Line':12s} {'Op':6s} {'MaxLoad':>12s} {'Cap':>6s} {'#Trains':>8s} {'Note'}\")\n",
    "for t in sorted(results, key=lambda x: x[0]):\n",
    "    line_key, op, maxload, cap, n_trains, note = t\n",
    "    print(f\"{line_key:12s} {op:6s} {maxload:12.2f} {cap:6d} {n_trains:8d} {note}\")\n",
    "\n",
    "# ---------------- optional: demand.json 분할 ----------------\n",
    "split_js = {}\n",
    "if JSON_IN.exists():\n",
    "    with open(JSON_IN, \"r\", encoding=\"utf-8\") as f:\n",
    "        demand_js = json.load(f)\n",
    "    for ln, od_list in demand_js.items():\n",
    "        # 사용자가 원하면 ln 자체(예: '1AWC1')를 키로 사용\n",
    "        n_train = train_count_map.get(ln, 1)  # 매핑 없으면 1대\n",
    "        # create keys\n",
    "        for idx in range(n_train):\n",
    "            split_js[f\"{ln}_{idx+1}\"] = []\n",
    "        # distribute OD trips evenly (rounded to 2 decimals)\n",
    "        for od in od_list:\n",
    "            # od expected format: [o, d, j]\n",
    "            if len(od) < 3:\n",
    "                continue\n",
    "            o, d, j = od[0], od[1], float(od[2])\n",
    "            share = round(j / n_train, 2)\n",
    "            for idx in range(n_train):\n",
    "                split_js[f\"{ln}_{idx+1}\"].append([o, d, share])\n",
    "\n",
    "    # 저장\n",
    "    JSON_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(JSON_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"{\\n\")\n",
    "        for i, (line, trips) in enumerate(split_js.items()):\n",
    "            line_str = json.dumps(line, ensure_ascii=False)\n",
    "            trips_str = json.dumps(trips, ensure_ascii=False, separators=(\",\", \":\"))\n",
    "            comma = \",\" if i < len(split_js) - 1 else \"\"\n",
    "            f.write(f\"  {line_str}: {trips_str}{comma}\\n\")\n",
    "        f.write(\"}\\n\")\n",
    "    print(f\"\\n✅ demand split JSON 저장 완료 → {JSON_OUT}\")\n",
    "else:\n",
    "    print(f\"\\n주의: {JSON_IN} 파일이 없어서 demand 분할은 수행하지 않았습니다.\")\n",
    "\n",
    "# ---------------- 끝 ----------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82cda35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\dep_time_uk.json\n",
      "총 출발 이벤트 수: 229\n",
      "1AWC1_1 : 0\n",
      "1AWC1_2 : 3\n",
      "1AWC1_3 : 6\n",
      "1AWC1_4 : 9\n",
      "1AWC1_5 : 12\n",
      "1AWC1_6 : 15\n",
      "1AWC1_7 : 18\n",
      "1AWC1_8 : 21\n",
      "1AWC1_9 : 24\n",
      "1AWC1_10 : 27\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 0) 저장 경로\n",
    "OUT_DIR = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\")\n",
    "OUT_PATH = OUT_DIR / \"dep_time_uk.json\"\n",
    "\n",
    "# 1) 라인별 배차간격(분)\n",
    "headway = {\n",
    "  \"1AWC1\": 3, \"1AWC2\": 18, \"1EMR1\": 3, \"1EMR2\": 18, \"1GA1\": 3, \"1GA2\": 18,\n",
    "  \"1GWR1\": 3, \"1GWR2\": 9, \"1LNER1\": 3, \"1LNER2\": 9, \"1SR1\": 3, \"1SR2\": 9,\n",
    "  \"1TPE1\": 18, \"1TPE2\": 18, \"1TW1\": 18, \"1TW2\": 9, \"2AWC1\":3, \"2AWC2\": 18,\n",
    "  \"2EMR1\": 3, \"2EMR2\": 9, \"2GWR1\": 3, \"2GWR2\": 18, \"2LNER1\": 3, \"2LNER2\": 9,\n",
    "  \"2SR1\": 3, \"2SR2\": 18, \"2TW1\": 9, \"2TW2\": 9, \"3EMR1\": 3, \"3EMR2\": 18,\n",
    "  \"3GWR1\": 9, \"3GWR2\": 18, \"3SR1\": 3, \"3SR2\": 18, \"4EMR1\": 3, \"4EMR2\": 9\n",
    "}\n",
    "\n",
    "# 2) 라인별 총 소요시간(분)\n",
    "trip_time = {\n",
    "  \"1AWC1\":31, \"1AWC2\":31, \"1EMR1\":22, \"1EMR2\":22, \"1GA1\":38, \"1GA2\":38,\n",
    "  \"1GWR1\":31, \"1GWR2\":31, \"1LNER1\":59, \"1LNER2\":59, \"1SR1\":54, \"1SR2\":54,\n",
    "  \"1TPE1\":31, \"1TPE2\":31, \"1TW1\":34, \"1TW2\":34, \"2AWC1\":52, \"2AWC2\":52,\n",
    "  \"2EMR1\":35, \"2EMR2\":35, \"2GWR1\":46, \"2GWR2\":46, \"2LNER1\":79, \"2LNER2\":79,\n",
    "  \"2SR1\":25, \"2SR2\":25, \"2TW1\":43, \"2TW2\":43, \"3EMR1\":29, \"3EMR2\":29,\n",
    "  \"3GWR1\":35, \"3GWR2\":35, \"3SR1\":25, \"3SR2\":25, \"4EMR1\":34, \"4EMR2\":34\n",
    "}\n",
    "\n",
    "# 3) 총 시뮬레이션 길이(분 단위 timestep)\n",
    "TOTAL = 72\n",
    "\n",
    "# 4) 출발시각 생성 (도착 제한: t + trip_time[line] <= TOTAL)\n",
    "dep_map = {}\n",
    "for line in sorted(headway.keys()):\n",
    "    h = int(headway[line])\n",
    "    tt = int(trip_time.get(line, 0))   # trip_time에 없으면 0으로 간주(=제약 없음)\n",
    "    if h <= 0:\n",
    "        continue\n",
    "\n",
    "    # 출발 가능한 마지막 시각(포함): t <= TOTAL - tt\n",
    "    last_start = TOTAL - tt\n",
    "    if last_start < 0:\n",
    "        # 이동시간이 TOTAL보다 길면 해당 라인은 출발 없음\n",
    "        continue\n",
    "\n",
    "    # 0, h, 2h, ... <= last_start\n",
    "    times = list(range(0, last_start + 1, h))\n",
    "    for i, t in enumerate(times, start=1):\n",
    "        dep_map[f\"{line}_{i}\"] = t\n",
    "\n",
    "# 5) 저장\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dep_map, f, ensure_ascii=False, indent=2, separators=(\",\", \": \"))\n",
    "\n",
    "print(f\"✅ 저장 완료: {OUT_PATH}\")\n",
    "print(f\"총 출발 이벤트 수: {len(dep_map):,}\")\n",
    "# 샘플 몇 개 확인\n",
    "for k in list(dep_map.keys())[:10]:\n",
    "    print(k, \":\", dep_map[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1354bb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\routes_nodes_uk.json\n",
      "총 생성 항목 수: 229\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TRAIN 시트(RLWAY_NM, NODE) + dep_time_uk.json -> routes_nodes_uk.json\n",
    "- NODE: \"1-2-3-4\"  -> [\"n1\",\"n2\",\"n3\",\"n4\"] 로 변환\n",
    "- dep_time_uk의 각 키(예: \"1ABC1_2\")에 대해 베이스(\"1ABC1\")를 뽑아 해당 노드열을 매핑\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ── 경로 설정 ─────────────────────────────────────────\n",
    "BASE_DIR = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\")\n",
    "XLSX_PATH = BASE_DIR / \"qgis_export_uk.xlsx\"\n",
    "DEP_JSON  = BASE_DIR / \"dep_time_uk.json\"\n",
    "OUT_JSON  = BASE_DIR / \"routes_nodes_uk.json\"\n",
    "TRAIN_SHEET = \"TRAIN\"   # 시트명이 다르면 수정\n",
    "\n",
    "# ── 유틸 ─────────────────────────────────────────────\n",
    "def to_nodes_list(node_str: str):\n",
    "    \"\"\"\n",
    "    \"46-35-31\" -> [\"n46\",\"n35\",\"n31\"]\n",
    "    숫자/공백/소수표기 등 유연 처리\n",
    "    \"\"\"\n",
    "    if node_str is None:\n",
    "        return []\n",
    "    parts = str(node_str).strip().split(\"-\")\n",
    "    nodes = []\n",
    "    for p in parts:\n",
    "        p = str(p).strip()\n",
    "        if p == \"\":\n",
    "            continue\n",
    "        # \"46.0\" 같은 경우 정수 문자열로\n",
    "        try:\n",
    "            if re.match(r\"^\\d+(\\.0+)?$\", p):\n",
    "                p_int = int(float(p))\n",
    "                nodes.append(f\"n{p_int}\")\n",
    "            else:\n",
    "                # 숫자 아니면 그대로 붙이되 접두 'n'만\n",
    "                # (필요시 더 엄격히 필터링)\n",
    "                nodes.append(f\"n{p}\")\n",
    "        except Exception:\n",
    "            nodes.append(f\"n{p}\")\n",
    "    return nodes\n",
    "\n",
    "def base_name(dep_key: str) -> str:\n",
    "    \"\"\"'1EMR1_3' -> '1EMR1'\"\"\"\n",
    "    return dep_key.rsplit(\"_\", 1)[0] if \"_\" in dep_key else dep_key\n",
    "\n",
    "# ── 입력 로드 ────────────────────────────────────────\n",
    "if not XLSX_PATH.exists():\n",
    "    raise FileNotFoundError(f\"엑셀 파일이 없습니다: {XLSX_PATH}\")\n",
    "if not DEP_JSON.exists():\n",
    "    raise FileNotFoundError(f\"dep_time_uk.json이 없습니다: {DEP_JSON}\")\n",
    "\n",
    "# TRAIN 시트 읽기\n",
    "train_df = pd.read_excel(XLSX_PATH, sheet_name=TRAIN_SHEET, dtype=str, engine=\"openpyxl\")\n",
    "train_df.columns = [c.strip() for c in train_df.columns]\n",
    "\n",
    "need_cols = {\"RLWAY_NM\", \"NODE\"}\n",
    "missing = need_cols - set(train_df.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"TRAIN 시트에 필요한 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "# RLWAY_NM -> [\"n..\",\"n..\",...] 매핑\n",
    "map_line_to_nodes = {}\n",
    "for line, nodes in train_df[[\"RLWAY_NM\",\"NODE\"]].itertuples(index=False):\n",
    "    if line is None:\n",
    "        continue\n",
    "    line_key = str(line).strip()\n",
    "    nodes_list = to_nodes_list(nodes)\n",
    "    if nodes_list:\n",
    "        map_line_to_nodes[line_key] = nodes_list\n",
    "\n",
    "# dep_time_uk.json 읽기\n",
    "with DEP_JSON.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    dep_map = json.load(f)  # {\"1AWC1_1\": 0, ...}\n",
    "\n",
    "# ── 변환: dep 키마다 노드 시퀀스 부여 ────────────────\n",
    "routes_nodes = {}\n",
    "missing_bases = set()\n",
    "\n",
    "# dep_map의 키 순서를 그대로 따르도록 반복\n",
    "for dep_key in dep_map.keys():\n",
    "    b = base_name(dep_key)\n",
    "    nodes = map_line_to_nodes.get(b)\n",
    "    if nodes is None:\n",
    "        missing_bases.add(b)\n",
    "        continue\n",
    "    # 얕은 복사(리스트 보호)\n",
    "    routes_nodes[dep_key] = list(nodes)\n",
    "\n",
    "# ── 저장 ────────────────────────────────────────────\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUT_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(routes_nodes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ 저장 완료: {OUT_JSON}\")\n",
    "print(f\"총 생성 항목 수: {len(routes_nodes):,}\")\n",
    "if missing_bases:\n",
    "    print(\"⚠ 다음 베이스 라인은 TRAIN 시트에서 찾지 못해 스킵했습니다:\")\n",
    "    for b in sorted(missing_bases):\n",
    "        print(\"  -\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "43022e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\demand_uk_2.json\n",
      "총 생성 키 수: 270\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\")\n",
    "DEP_PATH    = BASE / \"dep_time_uk.json\"   # {\"1AWC1_1\": 0, \"1AWC1_2\": 3, ...}\n",
    "DEMAND_PATH = BASE / \"demand_uk.json\"     # {\"1AWC1\": [[o,d,val], ...], ...}\n",
    "OUT_PATH    = BASE / \"demand_uk_2.json\"   # 결과 저장\n",
    "\n",
    "# ---- 입력 로드 ----\n",
    "if not DEP_PATH.exists():\n",
    "    raise FileNotFoundError(f\"dep_time_uk.json이 없습니다: {DEP_PATH}\")\n",
    "if not DEMAND_PATH.exists():\n",
    "    raise FileNotFoundError(f\"demand_uk.json이 없습니다: {DEMAND_PATH}\")\n",
    "\n",
    "with DEP_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    dep_map = json.load(f)\n",
    "with DEMAND_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    demand_map = json.load(f)\n",
    "\n",
    "# ---- dep 키를 (베이스, 번호)로 분해 후 정렬 ----\n",
    "# 예: \"1AWC1_3\" -> base=\"1AWC1\", idx=3\n",
    "def split_base_idx(k: str):\n",
    "    if \"_\" not in k:\n",
    "        return k, None\n",
    "    base, idx = k.rsplit(\"_\", 1)\n",
    "    # 숫자 추출 실패 시 None\n",
    "    try:\n",
    "        idx_num = int(idx)\n",
    "    except Exception:\n",
    "        idx_num = None\n",
    "    return base, idx_num\n",
    "\n",
    "# 베이스별로 키 모으고, 번호로 정렬\n",
    "grouped = defaultdict(list)\n",
    "for k in dep_map.keys():\n",
    "    base, idx = split_base_idx(k)\n",
    "    grouped[base].append((k, idx))\n",
    "\n",
    "for base in grouped:\n",
    "    grouped[base].sort(key=lambda x: (x[1] is None, x[1]))  # 숫자 있는 것 우선, 숫자 오름차순\n",
    "\n",
    "# ---- 결과 out 생성: 각 dep 키마다 demand의 base OD 복사 ----\n",
    "out = {}\n",
    "missing_bases = []\n",
    "for base, items in sorted(grouped.items(), key=lambda x: x[0]):\n",
    "    od_list = demand_map.get(base)\n",
    "    if od_list is None:\n",
    "        missing_bases.append(base)\n",
    "        continue\n",
    "    for k, _ in items:\n",
    "        # 얕은 복사로 충분 (원소는 리스트 of [o,d,val])\n",
    "        out[k] = [list(od) for od in od_list]\n",
    "\n",
    "# ---- 저장: 키마다 한 줄(엔터) ----\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "all_keys_ordered = []\n",
    "for base, items in sorted(grouped.items(), key=lambda x: x[0]):\n",
    "    if base in missing_bases:\n",
    "        continue\n",
    "    for k, _ in items:\n",
    "        if k in out:\n",
    "            all_keys_ordered.append(k)\n",
    "\n",
    "with OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"{\\n\")\n",
    "    for i, k in enumerate(all_keys_ordered):\n",
    "        k_json = json.dumps(k, ensure_ascii=False)\n",
    "        v_json = json.dumps(out[k], ensure_ascii=False, separators=(\",\", \":\"))\n",
    "        comma = \",\" if i < len(all_keys_ordered) - 1 else \"\"\n",
    "        # 키마다 한 줄씩 기록(숫자 다른 것끼리는 자연스럽게 엔터)\n",
    "        f.write(f\"  {k_json}: {v_json}{comma}\\n\")\n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"✅ 저장 완료: {OUT_PATH}\")\n",
    "print(f\"총 생성 키 수: {len(all_keys_ordered):,}\")\n",
    "if missing_bases:\n",
    "    print(\"⚠ 다음 베이스 라인은 demand_uk.json에 없어 스킵되었습니다:\")\n",
    "    for b in sorted(missing_bases):\n",
    "        print(\"  -\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a88894f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\routes_nodes_uk.json\n",
      "총 생성 항목 수: 270\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TRAIN 시트(RLWAY_NM, NODE) + dep_time_uk.json -> routes_nodes_uk.json\n",
    "- NODE: \"1-2-3-4\"  -> [\"n1\",\"n2\",\"n3\",\"n4\"] 로 변환\n",
    "- dep_time_uk의 각 키(예: \"1ABC1_2\")에 대해 베이스(\"1ABC1\")를 뽑아 해당 노드열을 매핑\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ── 경로 설정 ─────────────────────────────────────────\n",
    "BASE_DIR = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\")\n",
    "XLSX_PATH = BASE_DIR / \"qgis_export_uk.xlsx\"\n",
    "DEP_JSON  = BASE_DIR / \"dep_time_uk.json\"\n",
    "OUT_JSON  = BASE_DIR / \"routes_nodes_uk.json\"\n",
    "TRAIN_SHEET = \"TRAIN\"   # 시트명이 다르면 수정\n",
    "\n",
    "# ── 유틸 ─────────────────────────────────────────────\n",
    "def to_nodes_list(node_str: str):\n",
    "    \"\"\"\n",
    "    \"46-35-31\" -> [\"n46\",\"n35\",\"n31\"]\n",
    "    숫자/공백/소수표기 등 유연 처리\n",
    "    \"\"\"\n",
    "    if node_str is None:\n",
    "        return []\n",
    "    parts = str(node_str).strip().split(\"-\")\n",
    "    nodes = []\n",
    "    for p in parts:\n",
    "        p = str(p).strip()\n",
    "        if p == \"\":\n",
    "            continue\n",
    "        # \"46.0\" 같은 경우 정수 문자열로\n",
    "        try:\n",
    "            if re.match(r\"^\\d+(\\.0+)?$\", p):\n",
    "                p_int = int(float(p))\n",
    "                nodes.append(f\"n{p_int}\")\n",
    "            else:\n",
    "                # 숫자 아니면 그대로 붙이되 접두 'n'만\n",
    "                # (필요시 더 엄격히 필터링)\n",
    "                nodes.append(f\"n{p}\")\n",
    "        except Exception:\n",
    "            nodes.append(f\"n{p}\")\n",
    "    return nodes\n",
    "\n",
    "def base_name(dep_key: str) -> str:\n",
    "    \"\"\"'1EMR1_3' -> '1EMR1'\"\"\"\n",
    "    return dep_key.rsplit(\"_\", 1)[0] if \"_\" in dep_key else dep_key\n",
    "\n",
    "# ── 입력 로드 ────────────────────────────────────────\n",
    "if not XLSX_PATH.exists():\n",
    "    raise FileNotFoundError(f\"엑셀 파일이 없습니다: {XLSX_PATH}\")\n",
    "if not DEP_JSON.exists():\n",
    "    raise FileNotFoundError(f\"dep_time_uk.json이 없습니다: {DEP_JSON}\")\n",
    "\n",
    "# TRAIN 시트 읽기\n",
    "train_df = pd.read_excel(XLSX_PATH, sheet_name=TRAIN_SHEET, dtype=str, engine=\"openpyxl\")\n",
    "train_df.columns = [c.strip() for c in train_df.columns]\n",
    "\n",
    "need_cols = {\"RLWAY_NM\", \"NODE\"}\n",
    "missing = need_cols - set(train_df.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"TRAIN 시트에 필요한 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "# RLWAY_NM -> [\"n..\",\"n..\",...] 매핑\n",
    "map_line_to_nodes = {}\n",
    "for line, nodes in train_df[[\"RLWAY_NM\",\"NODE\"]].itertuples(index=False):\n",
    "    if line is None:\n",
    "        continue\n",
    "    line_key = str(line).strip()\n",
    "    nodes_list = to_nodes_list(nodes)\n",
    "    if nodes_list:\n",
    "        map_line_to_nodes[line_key] = nodes_list\n",
    "\n",
    "# dep_time_uk.json 읽기\n",
    "with DEP_JSON.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    dep_map = json.load(f)  # {\"1AWC1_1\": 0, ...}\n",
    "\n",
    "# ── 변환: dep 키마다 노드 시퀀스 부여 ────────────────\n",
    "routes_nodes = {}\n",
    "missing_bases = set()\n",
    "\n",
    "# dep_map의 키 순서를 그대로 따르도록 반복\n",
    "for dep_key in dep_map.keys():\n",
    "    b = base_name(dep_key)\n",
    "    nodes = map_line_to_nodes.get(b)\n",
    "    if nodes is None:\n",
    "        missing_bases.add(b)\n",
    "        continue\n",
    "    # 얕은 복사(리스트 보호)\n",
    "    routes_nodes[dep_key] = list(nodes)\n",
    "\n",
    "# ── 저장 ────────────────────────────────────────────\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUT_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(routes_nodes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ 저장 완료: {OUT_JSON}\")\n",
    "print(f\"총 생성 항목 수: {len(routes_nodes):,}\")\n",
    "if missing_bases:\n",
    "    print(\"⚠ 다음 베이스 라인은 TRAIN 시트에서 찾지 못해 스킵했습니다:\")\n",
    "    for b in sorted(missing_bases):\n",
    "        print(\"  -\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de54e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Line별 최대 timestep10 ===\n",
      "1AWC1\t23\n",
      "1AWC2\t23\n",
      "1EMR1\t17\n",
      "1EMR2\t17\n",
      "1GA1\t32\n",
      "1GA2\t32\n",
      "1GWR1\t23\n",
      "1GWR2\t23\n",
      "1LNER1\t49\n",
      "1LNER2\t49\n",
      "1SR1\t47\n",
      "1SR2\t47\n",
      "1TPE1\t26\n",
      "1TPE2\t26\n",
      "1TW1\t27\n",
      "1TW2\t27\n",
      "2AWC1\t43\n",
      "2AWC2\t43\n",
      "2EMR1\t28\n",
      "2EMR2\t28\n",
      "2GWR1\t37\n",
      "2GWR2\t37\n",
      "2LNER1\t67\n",
      "2LNER2\t67\n",
      "2SR1\t21\n",
      "2SR2\t21\n",
      "2TW1\t35\n",
      "2TW2\t35\n",
      "3EMR1\t23\n",
      "3EMR2\t23\n",
      "3GWR1\t29\n",
      "3GWR2\t29\n",
      "3SR1\t20\n",
      "3SR2\t20\n",
      "4EMR1\t28\n",
      "4EMR2\t28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) 파일·시트 설정\n",
    "xlsx_fp = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\\qgis_export_uk.xlsx\")\n",
    "sheet   = \"DEMAND\"  # 시트명이 다르면 수정\n",
    "\n",
    "# 2) 필요한 컬럼만 읽기\n",
    "usecols = [\"Line\", \"timestep10\"]\n",
    "df = pd.read_excel(xlsx_fp, sheet_name=sheet, usecols=usecols, dtype=str, engine=\"openpyxl\")\n",
    "\n",
    "# 3) 전처리: 공백 제거 + 수치화\n",
    "df[\"Line\"] = df[\"Line\"].astype(str).str.strip()\n",
    "df[\"timestep10\"] = pd.to_numeric(df[\"timestep10\"], errors=\"coerce\")\n",
    "\n",
    "# 4) Line별 최대 timestep10 계산\n",
    "max_by_line = (df.groupby(\"Line\", dropna=True)[\"timestep10\"]\n",
    "                 .max()\n",
    "                 .reset_index()\n",
    "                 .rename(columns={\"timestep10\": \"max_timestep10\"}))\n",
    "\n",
    "# 5) 정렬(원하면 내림차순)\n",
    "max_by_line = max_by_line.sort_values([\"Line\"]).reset_index(drop=True)\n",
    "\n",
    "# 6) 출력\n",
    "print(\"=== Line별 최대 timestep10 ===\")\n",
    "for line, vmax in max_by_line.itertuples(index=False):\n",
    "    print(f\"{line}\\t{vmax}\")\n",
    "\n",
    "# (선택) 표 형태로 한 번에 보기\n",
    "# print(max_by_line.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc6d082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 노선별 최대 Edge 탑승·필요 열차 대수 ===\n",
      "1AWC1            최대탑승 = 1880444명  /  용량 = 410  →  열차 4587대\n",
      "1AWC2            최대탑승 = 72209명  /  용량 = 410  →  열차 177대\n",
      "1EMR1            최대탑승 = 1607277명  /  용량 = 410  →  열차 3921대\n",
      "1EMR2            최대탑승 = 28712명  /  용량 = 410  →  열차 71대\n",
      "1GA1             최대탑승 = 1890145명  /  용량 = 410  →  열차 4611대\n",
      "1GA2             최대탑승 = 48215명  /  용량 = 410  →  열차 118대\n",
      "1GWR1            최대탑승 = 1296469명  /  용량 = 410  →  열차 3163대\n",
      "1GWR2            최대탑승 = 79402명  /  용량 = 410  →  열차 194대\n",
      "1LNER1           최대탑승 = 2149512명  /  용량 = 410  →  열차 5243대\n",
      "1LNER2           최대탑승 = 127608명  /  용량 = 410  →  열차 312대\n",
      "1SR1             최대탑승 = 1019061명  /  용량 = 410  →  열차 2486대\n",
      "1SR2             최대탑승 = 48037명  /  용량 = 410  →  열차 118대\n",
      "1TPE1            최대탑승 = 42688명  /  용량 = 410  →  열차 105대\n",
      "1TPE2            최대탑승 = 52438명  /  용량 = 410  →  열차 128대\n",
      "1TW1             최대탑승 = 23066명  /  용량 = 410  →  열차 57대\n",
      "1TW2             최대탑승 = 60456명  /  용량 = 410  →  열차 148대\n",
      "2AWC1            최대탑승 = 2385564명  /  용량 = 410  →  열차 5819대\n",
      "2AWC2            최대탑승 = 34697명  /  용량 = 410  →  열차 85대\n",
      "2EMR1            최대탑승 = 2119415명  /  용량 = 410  →  열차 5170대\n",
      "2EMR2            최대탑승 = 77191명  /  용량 = 410  →  열차 189대\n",
      "2GWR1            최대탑승 = 1128237명  /  용량 = 410  →  열차 2752대\n",
      "2GWR2            최대탑승 = 54403명  /  용량 = 410  →  열차 133대\n",
      "2LNER1           최대탑승 = 2172892명  /  용량 = 410  →  열차 5300대\n",
      "2LNER2           최대탑승 = 131791명  /  용량 = 410  →  열차 322대\n",
      "2SR1             최대탑승 = 673920명  /  용량 = 410  →  열차 1644대\n",
      "2SR2             최대탑승 = 33952명  /  용량 = 410  →  열차 83대\n",
      "2TW1             최대탑승 = 53571명  /  용량 = 410  →  열차 131대\n",
      "2TW2             최대탑승 = 29068명  /  용량 = 410  →  열차 71대\n",
      "3EMR1            최대탑승 = 1594730명  /  용량 = 410  →  열차 3890대\n",
      "3EMR2            최대탑승 = 26323명  /  용량 = 410  →  열차 65대\n",
      "3GWR1            최대탑승 = 829176명  /  용량 = 410  →  열차 2023대\n",
      "3GWR2            최대탑승 = 21031명  /  용량 = 410  →  열차 52대\n",
      "3SR1             최대탑승 = 1397191명  /  용량 = 410  →  열차 3408대\n",
      "3SR2             최대탑승 = 30700명  /  용량 = 410  →  열차 75대\n",
      "4EMR1            최대탑승 = 1713670명  /  용량 = 410  →  열차 4180대\n",
      "4EMR2            최대탑승 = 41794명  /  용량 = 410  →  열차 102대\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\MINJI\\\\NETWORK RELIABILITY\\\\QGIS\\\\8.UK\\\\json\\\\json\\\\demand.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mln\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m15s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  최대탑승 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_load[ln]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m5.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m명  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/  용량 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcap_map[ln]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  →  열차 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_cnt[ln]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m대\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# ───────────────── 3. demand.json 불러와서 쪼개기 ─────────────────\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     50\u001b[39m     demand_js = json.load(f)\n\u001b[32m     52\u001b[39m split_js = {}   \u001b[38;5;66;03m# 새로운 dict\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\anaconda3\\envs\\mjkang\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'D:\\\\MINJI\\\\NETWORK RELIABILITY\\\\QGIS\\\\8.UK\\\\json\\\\json\\\\demand.json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd, json, math\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# ──────────────────────── 경로 설정 ────────────────────────\n",
    "BASE     = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\8.UK\\json\")\n",
    "xlsx_fp  = BASE / \"qgis_export_uk.xlsx\"\n",
    "json_in  = BASE / \"json\" / \"demand.json\"\n",
    "json_out = BASE / \"json\" / \"demand_split.json\"\n",
    "\n",
    "# ───────────────── 1. DEMAND 시트 → Edge 최대 탑승 ─────────────────\n",
    "df = pd.read_excel(xlsx_fp, sheet_name=\"DEMAND\",\n",
    "                   usecols=[\"Line\", \"Route\", \"journeys\"])\n",
    "\n",
    "edge_load = defaultdict(int)          # {(Line, u, v): 승객 누적}\n",
    "\n",
    "for line, route, pax in df.itertuples(index=False):\n",
    "    nodes = route.split(\"-\")\n",
    "    for u, v in zip(nodes[:-1], nodes[1:]):\n",
    "        edge_load[(line, u, v)] += pax\n",
    "\n",
    "edge_df = (pd.DataFrame([(l,u,v,c) for (l,u,v),c in edge_load.items()],\n",
    "                        columns=[\"Line\",\"From\",\"To\",\"Load\"]))\n",
    "\n",
    "max_load = (edge_df.groupby(\"Line\")[\"Load\"]\n",
    "                     .max()\n",
    "                     .to_dict())      # {Line: 최대 Edge 탑승}\n",
    "\n",
    "# ───────────────── 2. 노선별 열차 1편성 최대 좌석수 매핑 ────────────────\n",
    "def train_capacity(line: str) -> int:\n",
    "    \"\"\"노선 이름을 받아 보수적 1편성 최대 좌석수를 반환\"\"\"\n",
    "    if line.startswith(\"KTX강릉선\"):\n",
    "        return 381           # KTX‑Eum\n",
    "    if line.startswith(\"경부고속철도\"):\n",
    "        return 515           # KTX‑청룡 / 산천\n",
    "    # 그 외 일반선·호남고속철도 등 → 산천 보수치\n",
    "    return 410               # KTX‑Sancheon\n",
    "\n",
    "cap_map   = {ln: train_capacity(ln) for ln in max_load}\n",
    "train_cnt = {ln: max(1, math.ceil(max_load[ln] / cap_map[ln]))\n",
    "             for ln in max_load}\n",
    "\n",
    "print(\"\\n=== 노선별 최대 Edge 탑승·필요 열차 대수 ===\")\n",
    "for ln in sorted(train_cnt):\n",
    "    print(f\"{ln:15s}  최대탑승 = {max_load[ln]:5.0f}명  \"\n",
    "          f\"/  용량 = {cap_map[ln]}  →  열차 {train_cnt[ln]}대\")\n",
    "\n",
    "# ───────────────── 3. demand.json 불러와서 쪼개기 ─────────────────\n",
    "with open(json_in, \"r\", encoding=\"utf-8\") as f:\n",
    "    demand_js = json.load(f)\n",
    "\n",
    "split_js = {}   # 새로운 dict\n",
    "\n",
    "for ln, od_list in demand_js.items():\n",
    "    n_train = train_cnt.get(ln, 1)          # 매핑 안 되면 1대\n",
    "    for idx in range(n_train):\n",
    "        key = f\"{ln}_{idx+1}\"\n",
    "        split_js[key] = []\n",
    "    for o, d, j in od_list:\n",
    "        share = round(j / n_train, 2)\n",
    "        for idx in range(n_train):\n",
    "            split_js[f\"{ln}_{idx+1}\"].append([o, d, share])\n",
    "\n",
    "# ───────────────── 4. JSON 저장 (한 노선 = 한 줄) ─────────────────\n",
    "with open(json_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"{\\n\")\n",
    "    for i, (line, trips) in enumerate(split_js.items()):\n",
    "        line_str = json.dumps(line, ensure_ascii=False)\n",
    "        trips_str = json.dumps(trips, ensure_ascii=False, separators=(\",\", \":\"))\n",
    "        comma = \",\" if i < len(split_js) - 1 else \"\"\n",
    "        f.write(f\"  {line_str}: {trips_str}{comma}\\n\")\n",
    "    f.write(\"}\\n\")\n",
    "\n",
    "print(f\"\\n✅ 줄바꿈 최소화된 JSON 저장 완료 → {json_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14966d",
   "metadata": {},
   "source": [
    "출발 timestep 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46944202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'time_step_15' 열이 성공적으로 추가되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# 파일 경로\n",
    "file_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\qgis_export.xlsx\"\n",
    "sheet_name = \"EDGE\"\n",
    "\n",
    "# 엑셀 불러오기\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# 'time(min)' 열이 있는지 확인\n",
    "if \"time(min)\" not in df.columns:\n",
    "    raise KeyError(\"'time(min)' 열이 존재하지 않습니다.\")\n",
    "\n",
    "# 소숫점 올림하여 time_step_15 계산\n",
    "df[\"time_step_15\"] = df[\"time(min)\"].apply(lambda x: math.ceil(x / 15))\n",
    "\n",
    "# 덮어쓰기 저장\n",
    "with pd.ExcelWriter(file_path, mode=\"a\", if_sheet_exists=\"overlay\", engine=\"openpyxl\") as writer:\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"✅ 'time_step_15' 열이 성공적으로 추가되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6fe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ dep_time.json trimmed and saved (cutoff by rounded threshold).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# 파일 경로\n",
    "base_dir = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\"\n",
    "dep_path = os.path.join(base_dir, \"dep_time.json\")\n",
    "demand_path = os.path.join(base_dir, \"demand.json\")\n",
    "routes_path = os.path.join(base_dir, \"routes_nodes.json\")\n",
    "\n",
    "# 1. dep_time.json 로드\n",
    "with open(dep_path, encoding=\"utf-8\") as f:\n",
    "    dep_data = json.load(f)\n",
    "valid_trains = set(dep_data.keys())\n",
    "\n",
    "# 2. demand.json 필터링\n",
    "with open(demand_path, encoding=\"utf-8\") as f:\n",
    "    demand_data = json.load(f)\n",
    "filtered_demand = {k: v for k, v in demand_data.items() if k in valid_trains}\n",
    "\n",
    "# 3. routes_nodes.json 필터링\n",
    "with open(routes_path, encoding=\"utf-8\") as f:\n",
    "    routes_data = json.load(f)\n",
    "filtered_routes = {k: v for k, v in routes_data.items() if k in valid_trains}\n",
    "\n",
    "# 4. 저장 (덮어쓰기)\n",
    "with open(demand_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_demand, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(routes_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_routes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ demand.json과 routes_nodes.json이 dep_time.json 기준으로 성공적으로 필터링되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99a754a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 줄바꿈 포맷 정리 완료.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# 경로 설정\n",
    "base_dir = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\"\n",
    "demand_path = os.path.join(base_dir, \"demand_filtered.json\")\n",
    "routes_path = os.path.join(base_dir, \"routes_nodes_filtered.json\")\n",
    "\n",
    "def save_single_line_json(data, filepath):\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"{\\n\")\n",
    "        for i, (k, v) in enumerate(data.items()):\n",
    "            line = f'  \"{k}\": {json.dumps(v, ensure_ascii=False)}'\n",
    "            if i < len(data) - 1:\n",
    "                line += \",\"\n",
    "            f.write(line + \"\\n\")\n",
    "        f.write(\"}\")\n",
    "\n",
    "# 파일 불러오기\n",
    "with open(demand_path, encoding=\"utf-8\") as f:\n",
    "    demand_data = json.load(f)\n",
    "with open(routes_path, encoding=\"utf-8\") as f:\n",
    "    routes_data = json.load(f)\n",
    "\n",
    "# 한 줄당 기차 하나씩 저장\n",
    "save_single_line_json(demand_data, demand_path)\n",
    "save_single_line_json(routes_data, routes_path)\n",
    "\n",
    "print(\"✅ 줄바꿈 포맷 정리 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d2e672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 필터링 전 OD쌍 개수: 12426\n",
      "🔹 필터링 후 OD쌍 개수: 3607\n",
      "🔹 제거된 OD쌍 개수: 8819\n",
      "✅ 저장 완료: D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\\demand_filtered.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 1. 파일 경로 설정\n",
    "file_path = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\\demand.json\")\n",
    "output_path = file_path.parent / \"demand_filtered.json\"\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 2. 데이터 불러오기\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    demand_data = json.load(f)\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 3. 필터링 및 통계\n",
    "original_count = 0\n",
    "filtered_count = 0\n",
    "\n",
    "filtered_data = {}\n",
    "\n",
    "for train_id, od_list in demand_data.items():\n",
    "    original_count += len(od_list)\n",
    "    \n",
    "    # 수요 10 이상인 OD쌍만 남기기\n",
    "    new_od_list = [od for od in od_list if od[2] >= 10]\n",
    "    filtered_count += len(new_od_list)\n",
    "    \n",
    "    # 결과가 남아있는 경우에만 저장\n",
    "    if new_od_list:\n",
    "        filtered_data[train_id] = new_od_list\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 4. 필터링 결과 저장\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 5. 결과 출력\n",
    "print(f\"🔹 필터링 전 OD쌍 개수: {original_count}\")\n",
    "print(f\"🔹 필터링 후 OD쌍 개수: {filtered_count}\")\n",
    "print(f\"🔹 제거된 OD쌍 개수: {original_count - filtered_count}\")\n",
    "print(f\"✅ 저장 완료: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15255623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ dep_time 필터링: 122 → 106개 남음\n",
      "✅ routes_nodes 필터링: 122 → 106개 남음\n",
      "\n",
      "📁 저장 완료:\n",
      "→ dep_time_filtered.json\n",
      "→ routes_nodes_filtered.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 1. 경로 설정\n",
    "base_dir = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\json\")\n",
    "dep_time_path = base_dir / \"dep_time.json\"\n",
    "routes_nodes_path = base_dir / \"routes_nodes.json\"\n",
    "dep_time_out = base_dir / \"dep_time_filtered.json\"\n",
    "routes_nodes_out = base_dir / \"routes_nodes_filtered.json\"\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 2. dep_time.json 불러오기 및 필터링 (값이 13 미만인 것만)\n",
    "with open(dep_time_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dep_time = json.load(f)\n",
    "\n",
    "filtered_dep_time = {k: v for k, v in dep_time.items() if v < 12}\n",
    "print(f\"✅ dep_time 필터링: {len(dep_time)} → {len(filtered_dep_time)}개 남음\")\n",
    "\n",
    "# 저장\n",
    "with open(dep_time_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_dep_time, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 3. routes_nodes.json 불러오기 및 해당 키만 남기기\n",
    "with open(routes_nodes_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    routes_nodes = json.load(f)\n",
    "\n",
    "# dep_time에서 남은 키만 유지\n",
    "filtered_routes_nodes = {k: v for k, v in routes_nodes.items() if k in filtered_dep_time}\n",
    "print(f\"✅ routes_nodes 필터링: {len(routes_nodes)} → {len(filtered_routes_nodes)}개 남음\")\n",
    "\n",
    "# 저장\n",
    "with open(routes_nodes_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_routes_nodes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 4. 결과 요약\n",
    "print(f\"\\n📁 저장 완료:\")\n",
    "print(f\"→ {dep_time_out.name}\")\n",
    "print(f\"→ {routes_nodes_out.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb134c89",
   "metadata": {},
   "source": [
    "#### Capacity\n",
    "- Demand_data이용해서 엣지마다 흐르는 demand양 계산한 파일: 6.edge_journeys_summary.xlsx\n",
    "- e1 n1-n2 1502241\n",
    "- e2 n2-n1 389459.5\n",
    "- e1,e2,e3,e4, .. 넘버링 qgis와 다름 (양방향 고려)\n",
    "- 양방향 엣지에 대해서는 서로 동일한 capacity를 갖도록 해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "060b668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capacity summary saved → D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\7.edge_capacity_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. 파일 불러오기\n",
    "base_dir = Path(r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\")\n",
    "input_path = base_dir / \"6.edge_journeys_summary.xlsx\"\n",
    "df = pd.read_excel(input_path)\n",
    "\n",
    "# 2. 여유 계수 설정 (예: 1.2배)\n",
    "buffer_ratio = 1.2\n",
    "\n",
    "# 3. (u, v) / (v, u) 형태를 고려해 양방향 키 만들기\n",
    "df[\"pair_key\"] = df.apply(lambda row: tuple(sorted([row[\"from_node_name\"], row[\"to_node_name\"]])), axis=1)\n",
    "\n",
    "# 4. 같은 pair_key 그룹 내에서 최대 journeys 선택\n",
    "grouped = df.groupby(\"pair_key\", as_index=False).agg({\n",
    "    \"journeys\": \"max\"\n",
    "})\n",
    "grouped[\"capacity\"] = grouped[\"journeys\"] * buffer_ratio\n",
    "\n",
    "# 5. 다시 원래 방향성 edge에 capacity 연결\n",
    "df = df.merge(grouped[[\"pair_key\", \"capacity\"]], on=\"pair_key\", how=\"left\")\n",
    "\n",
    "# 6. 컬럼 정리 및 저장\n",
    "df = df.drop(columns=[\"pair_key\"])\n",
    "output_path = base_dir / \"7.edge_capacity_summary.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Capacity summary saved → {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc815f",
   "metadata": {},
   "source": [
    "#### 노선별 승객 수요 데이터 -> 필요한 기차수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01187308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      line  cycles_day  train_sets_needed\n",
      "0   KTX강릉선          24                  1\n",
      "1   KTX동해선          39                  1\n",
      "2      경강선          23                  2\n",
      "3   경부고속철도           8                 23\n",
      "4      경부선           1                 65\n",
      "5      경북선          18                  1\n",
      "6      경의선          46                  1\n",
      "7      경인선          35                  1\n",
      "8      경전선           5                  3\n",
      "9     공항철도          20                  2\n",
      "10     대구선          44                  1\n",
      "11   동해남부선          12                  3\n",
      "12     동해선          16                  1\n",
      "13     삼척선          51                  1\n",
      "14     영동선          12                  2\n",
      "15     장항선          11                  2\n",
      "16     전라선          17                  2\n",
      "17   중부내륙선          36                  1\n",
      "18     중앙선           2                 23\n",
      "19     충북선          11                  1\n",
      "20     태백선          19                  1\n",
      "21  호남고속철도          20                  1\n",
      "22     호남선          10                  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minji Kang\\AppData\\Local\\Temp\\ipykernel_38976\\2930921966.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby(\"line\").apply(train_stats).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import json, math, pandas as pd, os\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 1. 파일 읽기\n",
    "file_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\7.Korea_Full\\5.demand_data_kofull.json\"\n",
    "df = pd.read_json(file_path)\n",
    "df[\"journeys_day\"] = df[\"journeys\"] / 365\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 2. 파라미터\n",
    "TARGET_LOAD    = 1.0          # 100 % 탑승률\n",
    "TURNAROUND_MIN = 15           # 종점 버퍼(분)\n",
    "HOURS_PER_DAY  = 17\n",
    "DEFAULT_SEAT   = 400          # seat_map 에 없을 때 임시값\n",
    "\n",
    "# 노선별 좌석수 (앞 글자 매칭용)\n",
    "seat_map = {\n",
    "    # 고속열차\n",
    "    \"KTX강릉선\":    381,   # KTX-이음 (KTX-Eum) 381석 :contentReference[oaicite:2]{index=2}\n",
    "    \"KTX경부선\":    955,   # KTX-I 955석(935~955) :contentReference[oaicite:3]{index=3}\n",
    "    \"KTX호남선\":    955,\n",
    "    \"경부고속철도\":   955,   # KTX-산천\n",
    "    \"경부선\":         900,   # SRT 편성(예시)\n",
    "    # ITX 계열\n",
    "    \"ITX-새마을\":   376,   # ITX-새마을\n",
    "    \"ITX-청춘\":     402,   # ITX-청춘\n",
    "    # 일반열차\n",
    "    \"무궁화호\":     920,   # 좌석+입석 허용, 편성 좌석 920 부근\n",
    "}\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "def train_stats(group):\n",
    "    line = group.name\n",
    "    seats = next((v for k, v in seat_map.items() if line.startswith(k)), DEFAULT_SEAT)\n",
    "\n",
    "    total_pax = group[\"journeys_day\"].sum()\n",
    "    single_run = group[\"time_min\"].max()\n",
    "    round_trip = single_run + TURNAROUND_MIN\n",
    "    cycles = max(1, (HOURS_PER_DAY * 60) // round_trip)\n",
    "\n",
    "    cap_per_set = seats * TARGET_LOAD * cycles\n",
    "    sets_needed = math.ceil(total_pax / cap_per_set) if cap_per_set else None\n",
    "\n",
    "    return pd.Series({\n",
    "        \"cycles_day\": cycles,\n",
    "        \"train_sets_needed\": sets_needed\n",
    "    })\n",
    "\n",
    "# 4. 계산\n",
    "result = df.groupby(\"line\").apply(train_stats).reset_index()\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjkang",
   "language": "python",
   "name": "mjkang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
