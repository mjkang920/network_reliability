{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91780aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "from shapely.geometry import LineString\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time() \n",
    "toy_node_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\Korea\\KOREARL_NODE.shp\"\n",
    "toy_edge_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\Korea\\KOREARL_EDGE.shp\"\n",
    "\n",
    "toy_node = gpd.read_file(toy_node_path)\n",
    "toy_edge = gpd.read_file(toy_edge_path)\n",
    "\n",
    "toy_node = toy_node.to_crs(epsg=3857)\n",
    "toy_edge = toy_edge.to_crs(epsg=3857)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "toy_edge.plot(ax=ax, color=\"grey\", linewidth=1.8, label=\"Edges\")\n",
    "\n",
    "node_coords = toy_node.geometry.apply(lambda pt: (pt.x, pt.y)).tolist()\n",
    "x_coords, y_coords = zip(*node_coords)\n",
    "ax.scatter(x_coords, y_coords, color=\"black\", s=2.3, label=\"Nodes\", zorder=3, alpha=0.5)\n",
    "\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron, zorder=0)\n",
    "plt.legend()\n",
    "plt.title(\"Scotland Railway Network\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "# 1. Node numbering (n1, n2, ...)\n",
    "round_coord = lambda coord: (round(coord[0], 6), round(coord[1], 6))\n",
    "\n",
    "# 1. node_id ‚Üí geometry\n",
    "node_info_list = [\n",
    "    (row.node_id, round_coord((row.geometry.x, row.geometry.y)))\n",
    "    for row in toy_node.itertuples()\n",
    "]\n",
    "\n",
    "nodes = {}\n",
    "node_meta = []\n",
    "\n",
    "for node_id, coord in node_info_list:\n",
    "    node_name = f\"n{int(node_id)}\" \n",
    "    nodes[node_name] = coord\n",
    "    node_meta.append({\n",
    "        \"node_id\": int(node_id),\n",
    "        \"node_name\": node_name,\n",
    "        \"geometry\": coord\n",
    "    })\n",
    "\n",
    "coord_to_node = {coord: node_name for node_name, coord in nodes.items()}\n",
    "\n",
    "print(\"====== Node info (ÏßÅÏ†ë Îß§Ìïë) ======\")\n",
    "for meta in node_meta:\n",
    "    print(f\"{meta['node_name']} (ID: {meta['node_id']}): {meta['geometry']}\")\n",
    "\n",
    "\n",
    "\n",
    "# 2. Edge numbering (e1, e2, ...)\n",
    "nodeid_to_nodename = {meta[\"node_id\"]: meta[\"node_name\"] for meta in node_meta}\n",
    "toy_edge['from_node_name'] = toy_edge['from_node_'].map(nodeid_to_nodename)\n",
    "toy_edge['to_node_name'] = toy_edge['to_node_id'].map(nodeid_to_nodename)\n",
    "\n",
    "edge_pairs = set()\n",
    "\n",
    "for _, row in toy_edge.iterrows():\n",
    "    from_node = row['from_node_name']\n",
    "    to_node = row['to_node_name']\n",
    "    sorted_pair = tuple(sorted([from_node, to_node]))\n",
    "    edge_pairs.add(sorted_pair)\n",
    "\n",
    "sorted_edge_pairs = sorted(edge_pairs, key=lambda x: (x[0], x[1]))\n",
    "print(sorted_edge_pairs)\n",
    "\n",
    "edges = {}\n",
    "edge_number = 1\n",
    "\n",
    "for from_node, to_node in sorted_edge_pairs:\n",
    "    edges[f\"e{edge_number}\"] = (from_node, to_node)\n",
    "    edges[f\"e{edge_number + 1}\"] = (to_node, from_node)\n",
    "    edge_number += 2\n",
    "\n",
    "edge_records = []\n",
    "for edge_name, (from_node, to_node) in edges.items():\n",
    "    match = toy_edge[((toy_edge['from_node_name'] == from_node) & (toy_edge['to_node_name'] == to_node)) |\n",
    "                     ((toy_edge['from_node_name'] == to_node) & (toy_edge['to_node_name'] == from_node))]\n",
    "    \n",
    "    if not match.empty:\n",
    "        journeys = match.iloc[0]['journeys']\n",
    "        geometry = match.iloc[0]['geometry']\n",
    "        edge_id = match.iloc[0]['edge_id']\n",
    "    else:\n",
    "        journeys = None\n",
    "        geometry = None\n",
    "        edge_id = None\n",
    "\n",
    "    edge_records.append({\n",
    "        'edge_id': edge_id,\n",
    "        'edge_name': edge_name,\n",
    "        'from_node_name': from_node,\n",
    "        'to_node_name': to_node,\n",
    "        'journeys': journeys,\n",
    "        'geometry': geometry\n",
    "    })\n",
    "\n",
    "toy_edge_bidirectional = pd.DataFrame(edge_records)\n",
    "\n",
    "toy_edge_bidirectional['edge_num'] = toy_edge_bidirectional['edge_name'].str.extract(r'e(\\d+)').astype(int)\n",
    "toy_edge_bidirectional = toy_edge_bidirectional.sort_values(by='edge_num').reset_index(drop=True)\n",
    "toy_edge_bidirectional = toy_edge_bidirectional.drop(columns='edge_num')\n",
    "\n",
    "print(\"\\n====== Edge info ======\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(toy_edge_bidirectional)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Convert edges to arc format\n",
    "arcs = [(u, v) for _, (u, v) in edges.items()]\n",
    "\n",
    "\n",
    "\n",
    "# 4. Compute Euclidean distances\n",
    "def euclidean_distance(node1, node2):\n",
    "    x1, y1 = nodes[node1]\n",
    "    x2, y2 = nodes[node2]\n",
    "    return round(((x2 - x1)**2 + (y2 - y1)**2)**0.5, 2)\n",
    "\n",
    "arc_distance = {edge_name: euclidean_distance(u, v) for edge_name, (u, v) in edges.items()}\n",
    "\n",
    "\n",
    "\n",
    "# 5. Create a network graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges\n",
    "for node, position in nodes.items():\n",
    "    G.add_node(node, pos=position)\n",
    "\n",
    "for edge_name, (u, v) in edges.items(): \n",
    "    G.add_edge(u, v, weight=arc_distance[edge_name])\n",
    "\n",
    "# Remove duplicate edges\n",
    "unique_edges = set()\n",
    "edge_name_map = {}\n",
    "\n",
    "for edge_name, (u, v) in edges.items():\n",
    "    if (v, u) not in unique_edges:\n",
    "        unique_edges.add((u, v))\n",
    "        \n",
    "        reverse_edge_name = [k for k, (a, b) in edges.items() if (a, b) == (v, u)]\n",
    "        if reverse_edge_name:\n",
    "            journeys = toy_edge_bidirectional[\n",
    "                (toy_edge_bidirectional['from_node_name'] == u) &\n",
    "                (toy_edge_bidirectional['to_node_name'] == v)\n",
    "            ]['journeys'].values\n",
    "\n",
    "            journeys_val = int(journeys[0]) if len(journeys) > 0 else \"?\"\n",
    "            edge_label = f\"{edge_name}, {reverse_edge_name[0]} ({journeys_val})\"\n",
    "        else:\n",
    "            edge_label = edge_name\n",
    "\n",
    "        edge_name_map[(u, v)] = edge_label\n",
    "\n",
    "\n",
    "# Plot the network topology\n",
    "plt.figure(figsize=(7, 7))\n",
    "pos = nx.get_node_attributes(G, 'pos')\n",
    "nx.draw_networkx_nodes(G, pos, node_size=250, node_color=\"lightblue\")\n",
    "nx.draw_networkx_edges(\n",
    "    G, pos,\n",
    "    edgelist=list(unique_edges),  \n",
    "    arrowstyle='-',\n",
    "    min_target_margin=10,\n",
    "    min_source_margin=10\n",
    ")\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_name_map, font_size=6)\n",
    "nx.draw_networkx_labels(G, pos, font_size=7)\n",
    "plt.title(\"Network with Unique Edge Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "\n",
    "# Scientific Libraries\n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "\n",
    "# Optimization\n",
    "from gurobipy import Model, GRB, quicksum\n",
    "\n",
    "# Graph & Plotting\\\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# MBNpy Modules\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "from mbnpy import brc, cpm, variable, operation, branch, config\n",
    "\n",
    "# Local Module\n",
    "import batch\n",
    "\n",
    "# Clean up memory\n",
    "gc.collect()\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4048d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = nodes  \n",
    "edges = edges  \n",
    "arcs = arcs\n",
    "arc_distance = arc_distance \n",
    "\n",
    "\n",
    "\n",
    "# 1. Generate arc failure probabilities based on arc distances (longer distance -> higher failure probability)\n",
    "min_dist = min(arc_distance.values())\n",
    "max_dist = max(arc_distance.values())\n",
    "\n",
    "min_prob = 0.01\n",
    "max_prob = 0.3\n",
    "\n",
    "def compute_failure_probability(distance, min_dist, max_dist, min_prob, max_prob):\n",
    "    normalized_dist = (distance - min_dist) / (max_dist - min_dist)\n",
    "    return round(min_prob + normalized_dist * (max_prob - min_prob), 4)\n",
    "\n",
    "probs_dynamic = {\n",
    "    edge: {\n",
    "        0: compute_failure_probability(dist, min_dist, max_dist, min_prob, max_prob),\n",
    "        1: round(1 - compute_failure_probability(dist, min_dist, max_dist, min_prob, max_prob), 4)\n",
    "    }\n",
    "    for edge, dist in arc_distance.items()\n",
    "}\n",
    "\n",
    "def numeric_sort(edge):\n",
    "    return int(edge[1:])\n",
    "\n",
    "probs_sorted = {k: probs_dynamic[k] for k in sorted(probs_dynamic, key=numeric_sort)}\n",
    "\n",
    "probs = probs_sorted\n",
    "probs_cpm = copy.deepcopy(probs)\n",
    "\n",
    "\n",
    "\n",
    "# 2. Assign capacities to each arcs\n",
    "intact_capacity = {}\n",
    "\n",
    "for edge_name, (u, v) in edges.items():\n",
    "    match = toy_edge[((toy_edge['from_node_name'] == u) & (toy_edge['to_node_name'] == v)) |\n",
    "                     ((toy_edge['from_node_name'] == v) & (toy_edge['to_node_name'] == u))]\n",
    "\n",
    "    if not match.empty:\n",
    "        intact_capacity[edge_name] = match.iloc[0]['journeys']\n",
    "    else:\n",
    "        intact_capacity[edge_name] = None  \n",
    "\n",
    "print(\"Intact Capacities:\", intact_capacity)\n",
    "\n",
    "def generate_comps_st(probs):\n",
    "    comps_st = {}\n",
    "\n",
    "    for edge, prob in probs.items():\n",
    "        if isinstance(prob, dict) and 0 in prob and 1 in prob:  \n",
    "            comps_st[edge] = np.random.choice([0, 1], p=[prob[0], prob[1]])\n",
    "        else:\n",
    "            print(f\"Warning: Invalid probability format for edge {edge}: {prob}\")  \n",
    "\n",
    "    return comps_st\n",
    "\n",
    "comps_st = generate_comps_st(probs_sorted)  \n",
    "\n",
    "\n",
    "\n",
    "# 3. Compute maximum allowable distance\n",
    "json_path = r\"D:\\MINJI\\NETWORK RELIABILITY\\QGIS\\Korea\\demand_data.json\"\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    demand = json.load(f)\n",
    "\n",
    "avg_velo = 149  # Speed in km/h\n",
    "max_distance = {}\n",
    "commodity_name_map = {}  \n",
    "\n",
    "for idx, item in enumerate(demand, start=1):\n",
    "    origin = item[\"origin\"]\n",
    "    destination = item[\"destination\"]\n",
    "    commodity_key = f\"{origin}->{destination}\"\n",
    "    commodity_name = f\"k{idx}\"  \n",
    "\n",
    "    distance = item[\"distance\"]\n",
    "    max_allowable_time = (distance * 60) / avg_velo + 180\n",
    "    max_distance[commodity_name] = max_allowable_time * avg_velo / 60\n",
    "\n",
    "    commodity_name_map[commodity_name] = {\n",
    "        \"key\": commodity_key,\n",
    "        \"origin\": origin,\n",
    "        \"destination\": destination\n",
    "    }\n",
    "\n",
    "    print(f\"\\nCommodity: {commodity_name}\")\n",
    "    print(f\"  OD Pair: {commodity_key}\")\n",
    "    print(f\"  Distance (from JSON): {distance:.2f} km\")\n",
    "    print(f\"  Maximum allowable time: {max_allowable_time:.2f} minutes\")\n",
    "    print(f\"  Maximum allowable distance: {max_distance[commodity_name]:.2f} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc791b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build directed graph with weighted arcs\n",
    "G = nx.DiGraph()\n",
    "for node, position in nodes.items():\n",
    "    G.add_node(node, pos=position)\n",
    "\n",
    "for edge_id, (u, v) in edges.items():\n",
    "    G.add_edge(u, v, weight=arc_distance[edge_id])\n",
    "\n",
    "# Initialize demand for each arc\n",
    "edge_demand = { (u, v): 0 for u, v in G.edges }\n",
    "\n",
    "# Accumulate demand from all shortest paths\n",
    "for info in demand:\n",
    "    origin = info[\"origin_name\"]        \n",
    "    destination = info[\"destination_name\"]\n",
    "    amount = info[\"journeys\"]            \n",
    "\n",
    "    try:\n",
    "        paths = list(nx.all_shortest_paths(G, source=origin, target=destination, weight=\"weight\"))\n",
    "        for path in paths:\n",
    "            for i in range(len(path) - 1):\n",
    "                edge = (path[i], path[i + 1])\n",
    "                if edge in edge_demand:\n",
    "                    edge_demand[edge] += amount\n",
    "                    print(f\"üü¶ OD {origin} ‚Üí {destination}, amount={amount} ‚ûú edge {edge} += {amount} (total: {edge_demand[edge]})\")\n",
    "    except nx.NetworkXNoPath:\n",
    "        print(f\"No path found for {origin} ‚Üí {destination}\")\n",
    "        continue\n",
    "\n",
    "# Log-scaled normalization\n",
    "normalized_demand = {\n",
    "    (u, v): np.log10(d + 1) for (u, v), d in edge_demand.items()\n",
    "}\n",
    "D_max = max(normalized_demand.values())\n",
    "D_min = min(normalized_demand.values())\n",
    "\n",
    "normalized_demand = {\n",
    "    (u, v): (val - D_min) / (D_max - D_min)\n",
    "    for (u, v), val in normalized_demand.items()\n",
    "}\n",
    "\n",
    "# Visualization\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "start_rgb = (245/255, 250/255, 254/255)    \n",
    "mid_rgb   = (100/255, 169/255, 211/255)       \n",
    "end_rgb   = (8/255, 50/255, 110/255)    \n",
    "cmap = LinearSegmentedColormap.from_list(\"custom_3color_gradient\", [start_rgb, mid_rgb, end_rgb])\n",
    "\n",
    "pos = nx.get_node_attributes(G, \"pos\")\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "\n",
    "edge_labels = {\n",
    "    (u, v): f\"{edge_demand[(u, v)]:.0f}\"\n",
    "    for (u, v) in G.edges\n",
    "    if edge_demand[(u, v)] > 0\n",
    "}\n",
    "\n",
    "nx.draw_networkx_edge_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    edge_labels=edge_labels,\n",
    "    font_size=8,\n",
    "    font_color=\"darkred\",\n",
    "    rotate=False,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "for (u, v), value in normalized_demand.items():\n",
    "    color = cmap(value)\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos, edgelist=[(u, v)],\n",
    "        width=1.8,\n",
    "        edge_color=[color],\n",
    "        arrowstyle=\"->\",\n",
    "        arrowsize=8,\n",
    "        alpha=1,\n",
    "        ax=ax,\n",
    "        connectionstyle=\"arc3,rad=0.07\"\n",
    "    )\n",
    "\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron, crs=\"EPSG:3857\", zorder=0)\n",
    "ax.tick_params(labelbottom=True, labelleft=True)\n",
    "ax.set_axis_on()\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "\n",
    "# Colorbar\n",
    "norm = mpl.colors.Normalize(vmin=D_min, vmax=D_max)\n",
    "sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label(\"Edge Demand Intensity (normalized)\")\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, which='major', linestyle='--', linewidth=0.5, color='gray', alpha=0.5)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14, direction='out')\n",
    "ax.set_axisbelow(True)  \n",
    "\n",
    "plt.title(\"OD Demands for Scotland Railway Network\", fontsize=18)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCNF_od_systemfunc(arcs, comps_st, edges, arc_capacity, demand,max_distance, arc_distance, target_od):\n",
    "    \n",
    "    from gurobipy import Model, GRB, quicksum\n",
    "    import networkx as nx\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for e, (i, j) in edges.items():\n",
    "        G.add_edge(i, j, weight=arc_distance.get(e, 1)) \n",
    "\n",
    "    model = Model(\"Network Flow Optimization\")\n",
    "    model.setParam('OutputFlag', 0) \n",
    "\n",
    "    # Define decision variables\n",
    "    flow = {}\n",
    "    unmet_demand = {}\n",
    "\n",
    "    for k, info in demand.items():\n",
    "        unmet_demand[k] = model.addVar(lb=0, vtype=GRB.CONTINUOUS, name=f\"unsatisfied_{k}\")\n",
    "        for i, j in arcs:\n",
    "            arc_key = next((e for e, v in edges.items() if v == (i, j) or v == (j, i)), None)\n",
    "            capacity = arc_capacity.get(arc_key, 0)\n",
    "            flow[k, i, j] = model.addVar(lb=0, ub=capacity, vtype=GRB.CONTINUOUS, name=f\"flow_{k}_{i}_{j}\")\n",
    "\n",
    "    # Objective: minimize total unmet demand\n",
    "    model.setObjective(\n",
    "        quicksum(unmet_demand[k] for k in demand),\n",
    "        GRB.MINIMIZE\n",
    "    )\n",
    "\n",
    "    nodes = set(node for edge in edges.values() for node in edge)\n",
    "\n",
    "    # Constraint 1: Flow conservation\n",
    "    for k, info in demand.items():\n",
    "        origin = info['origin']\n",
    "        destination = info['destination']\n",
    "        amount = info['amount']\n",
    "        for node in nodes: \n",
    "            inflow = quicksum(flow[k, i, j] for i, j in arcs if j == node)\n",
    "            outflow = quicksum(flow[k, i, j] for i, j in arcs if i == node)\n",
    "            if node == origin:\n",
    "                model.addConstr(outflow - inflow == amount - unmet_demand[k])\n",
    "            elif node == destination:\n",
    "                model.addConstr(outflow - inflow == - amount + unmet_demand[k])\n",
    "            else:\n",
    "                model.addConstr(outflow - inflow == 0)\n",
    "\n",
    "    # Constraint 2: Arc capacity limits\n",
    "    for i, j in arcs:\n",
    "        arc_key = next((e for e, v in edges.items() if v == (i, j) or v == (j, i)), None)\n",
    "        model.addConstr(quicksum(flow[k, i, j] for k in demand if (k, i, j) in flow) <= arc_capacity.get(arc_key, 0))\n",
    "\n",
    "    # Constraint 3: Distance constraints for each commodity\n",
    "    for k, info in demand.items():\n",
    "        origin = info['origin']\n",
    "        distance_expr = quicksum(arc_distance.get(e, 0) * flow[k, i, j] for e, (i, j) in edges.items() if (k, i, j) in flow)\n",
    "        total_flow = quicksum(flow[k, i, j] for i, j in arcs if (k, i, j) in flow and i == origin)\n",
    "        \n",
    "        model.addConstr(distance_expr <= max_distance[k] * total_flow)\n",
    "\n",
    "    model.optimize()\n",
    "    \n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        origin = demand[target_od]['origin']\n",
    "        destination = demand[target_od]['destination']\n",
    "        amount = demand[target_od]['amount']\n",
    "\n",
    "        # Sum of flow into destination\n",
    "        flow_to_dest = sum(\n",
    "            flow[target_od, i, destination].x\n",
    "            for i, j in arcs\n",
    "            if j == destination and (target_od, i, j) in flow\n",
    "        )\n",
    "\n",
    "        flow_ratio = flow_to_dest / amount if amount > 0 else 0\n",
    "        if flow_ratio > 0.9:\n",
    "            sys_st = 's'\n",
    "\n",
    "            min_comps_st = {}\n",
    "            for (k, i, j), var in flow.items():\n",
    "                if k == target_od and var.x > 0:\n",
    "                    link_name = next((e for e, v in edges.items() if v == (i, j) or v == (j, i)), None)\n",
    "                    if link_name:\n",
    "                        min_comps_st[link_name] = 1\n",
    "\n",
    "        else:\n",
    "            sys_st = 'f'\n",
    "            min_comps_st = None    \n",
    "        \n",
    "        print(f\"Used Components: {min_comps_st}\")\n",
    "\n",
    "        return flow_ratio, sys_st, min_comps_st\n",
    "    \n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e839ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCNF_systemfunc(arcs, comps_st, edges, arc_capacity, demand, max_distance, arc_distance):\n",
    "    from gurobipy import Model, GRB, quicksum\n",
    "    import networkx as nx\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for e, (i, j) in edges.items():\n",
    "        G.add_edge(i, j, weight=arc_distance.get(e, 1))  \n",
    "\n",
    "    model = Model(\"Network Flow Optimization\")\n",
    "    model.setParam('OutputFlag', 0) \n",
    "\n",
    "    # Define variables\n",
    "    flow = {}\n",
    "    unmet_demand = {}\n",
    "\n",
    "    for k, info in demand.items():\n",
    "        unmet_demand[k] = model.addVar(lb=0, vtype=GRB.CONTINUOUS, name=f\"unsatisfied_{k}\")\n",
    "        for i, j in arcs:\n",
    "            arc_key = next((e for e, v in edges.items() if v == (i, j) or v == (j, i)), None)\n",
    "            capacity = arc_capacity.get(arc_key, 0)\n",
    "            flow[k, i, j] = model.addVar(lb=0, ub=capacity, vtype=GRB.CONTINUOUS, name=f\"flow_{k}_{i}_{j}\")\n",
    "\n",
    "    # Objective function: Minimize expected loss\n",
    "    model.setObjective(\n",
    "        quicksum(unmet_demand[k] for k in demand),\n",
    "        GRB.MINIMIZE\n",
    "    )\n",
    "\n",
    "    # Extract all nodes from edge values\n",
    "    nodes = set(node for edge in edges.values() for node in edge)\n",
    "\n",
    "    # Constraint 1: Flow conservation\n",
    "    for k, info in demand.items():\n",
    "        origin = info['origin']\n",
    "        destination = info['destination']\n",
    "        amount = info['amount']\n",
    "        for node in nodes: \n",
    "            inflow = quicksum(flow[k, i, j] for i, j in arcs if j == node)\n",
    "            outflow = quicksum(flow[k, i, j] for i, j in arcs if i == node)\n",
    "            if node == origin:\n",
    "                model.addConstr(outflow - inflow == amount - unmet_demand[k])\n",
    "            elif node == destination:\n",
    "                model.addConstr(outflow - inflow == - amount + unmet_demand[k])\n",
    "            else:\n",
    "                model.addConstr(outflow - inflow == 0)\n",
    "\n",
    "    # Constraint 2: Arc capacity limits\n",
    "    for i, j in arcs:\n",
    "        arc_key = next((e for e, v in edges.items() if v == (i, j) or v == (j, i)), None)\n",
    "        model.addConstr(quicksum(flow[k, i, j] for k in demand if (k, i, j) in flow) <= arc_capacity.get(arc_key, 0))\n",
    "\n",
    "    # Constraint 3: CommodityÎ≥Ñ max_distance Ï†ÅÏö©\n",
    "    for k, info in demand.items():\n",
    "        origin = info['origin']\n",
    "        distance_expr = quicksum(arc_distance.get(e, 0) * flow[k, i, j] for e, (i, j) in edges.items() if (k, i, j) in flow)\n",
    "        total_flow = quicksum(flow[k, i, j] for i, j in arcs if (k, i, j) in flow and i == origin)\n",
    "        \n",
    "        model.addConstr(distance_expr <= max_distance[k] * total_flow)\n",
    "\n",
    "    # Perform optimization\n",
    "    model.optimize()\n",
    "\n",
    "    # Process results\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        expected_loss = model.objVal\n",
    "\n",
    "        total_demand = sum(info['amount'] for info in demand.values())\n",
    "        expected_loss = max(0.0, min(expected_loss, total_demand)) \n",
    "\n",
    "        if expected_loss < 127364:\n",
    "            sys_st = 's'\n",
    "\n",
    "            min_comps_st = {}\n",
    "\n",
    "            for (k, i, j), var in flow.items():\n",
    "                if var.x > 0:\n",
    "                    link_name = next((e for e, v in edges.items() if v == (i, j) or v == (j, i)), None)\n",
    "                    if link_name:\n",
    "                        min_comps_st[link_name] = 1 \n",
    "\n",
    "        else:\n",
    "            sys_st = 'f'\n",
    "            min_comps_st = None\n",
    "        \n",
    "        return expected_loss, sys_st, min_comps_st\n",
    "\n",
    "    else:\n",
    "        return None, None, None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms.flow import shortest_augmenting_path\n",
    "\n",
    "def shortestpath_systemfunc(arcs, comps_st, edges, arc_capacity, demand, max_distance, arc_distance):\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for e, (i, j) in edges.items():\n",
    "        if comps_st.get(e, 0) > 0:\n",
    "            cap = arc_capacity.get(e, 1)\n",
    "            dist = arc_distance.get(e, 1)\n",
    "            G.add_edge(i, j, capacity=cap, weight=dist, link_id=e)\n",
    "            G.add_edge(j, i, capacity=cap, weight=dist, link_id=e)\n",
    "\n",
    "    expected_loss = 0.0\n",
    "    used_links_set = set()\n",
    "\n",
    "    for k, info in demand.items():\n",
    "        origin = info[\"origin\"]\n",
    "        destination = info[\"destination\"]\n",
    "        amount = info[\"amount\"]\n",
    "        max_dist = max_distance.get(k, float('inf'))\n",
    "\n",
    "        if origin not in G.nodes or destination not in G.nodes:\n",
    "            expected_loss += amount\n",
    "            continue\n",
    "\n",
    "        G_temp = G.copy()\n",
    "        G_temp.add_edge(destination, 'sink', capacity=1) \n",
    "\n",
    "        try:\n",
    "            flow_value, flow_dict = nx.maximum_flow(\n",
    "                G_temp, origin, 'sink', capacity='capacity', flow_func=shortest_augmenting_path\n",
    "            )\n",
    "        except Exception as e:\n",
    "            flow_value = 0\n",
    "\n",
    "        if flow_value == 0:\n",
    "            expected_loss += amount\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            path = nx.shortest_path(G, source=origin, target=destination, weight='weight')\n",
    "            path_length = nx.shortest_path_length(G, source=origin, target=destination, weight='weight')\n",
    "\n",
    "            if path_length > max_dist:\n",
    "                expected_loss += amount\n",
    "            else:\n",
    "                for u, v in zip(path[:-1], path[1:]):\n",
    "                    edge_id = G[u][v]['link_id']\n",
    "                    used_links_set.add(edge_id)\n",
    "        except nx.NetworkXNoPath:\n",
    "            expected_loss += amount\n",
    "\n",
    "    total_demand = sum(info[\"amount\"] for info in demand.values())\n",
    "    expected_loss = max(0.0, min(expected_loss, total_demand))\n",
    "\n",
    "    if expected_loss < 127364:\n",
    "        sys_st = 's'\n",
    "        min_comps_st = {e: 1 for e in used_links_set}\n",
    "    else:\n",
    "        sys_st = 'f'\n",
    "        min_comps_st = None\n",
    "\n",
    "    return expected_loss, sys_st, total_demand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6628ff24",
   "metadata": {},
   "source": [
    "Î™®Îì† Ïª¥Ìè¨ÎÑåÌä∏Í∞Ä Ï†ïÏÉÅ ÏÉÅÌÉúÏùº Îïå expected loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe30a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_dict = {}\n",
    "\n",
    "for idx, item in enumerate(demand, start=1):\n",
    "    key = f\"k{idx}\"\n",
    "    demand_dict[key] = {\n",
    "        \"origin\": item[\"origin_name\"],\n",
    "        \"destination\": item[\"destination_name\"],\n",
    "        \"amount\": item[\"journeys\"],\n",
    "        \"distance\": item[\"distance\"]\n",
    "    }\n",
    "\n",
    "comps_st = {edge: 1 for edge in intact_capacity}\n",
    "arc_capacity = {edge: intact_capacity[edge] * comps_st[edge] for edge in intact_capacity}\n",
    "\n",
    "expected_loss, sys_st, total_demand = shortestpath_systemfunc(\n",
    "    arcs=arcs,\n",
    "    comps_st=comps_st,\n",
    "    edges=edges,\n",
    "    arc_capacity=arc_capacity,\n",
    "    demand=demand_dict,\n",
    "    max_distance=max_distance,\n",
    "    arc_distance=arc_distance\n",
    ")\n",
    "\n",
    "print(\"Expected Loss:\", expected_loss)\n",
    "print(\"System State:\", sys_st)\n",
    "print(\"Total Demand:\", total_demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3764dbe",
   "metadata": {},
   "source": [
    "Run MCNF system function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c2d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_fun_mcnf = lambda comps_st: MCNF_systemfunc(\n",
    "    arcs=arcs,\n",
    "    comps_st=comps_st,\n",
    "    edges=edges,\n",
    "    arc_capacity={e: intact_capacity[e] * comps_st[e] for e in comps_st},\n",
    "    demand=demand_dict,\n",
    "    max_distance=max_distance,\n",
    "    arc_distance=arc_distance\n",
    ")\n",
    "\n",
    "sys_fun_shortest = lambda comps_st: shortestpath_systemfunc(\n",
    "    arcs=arcs,\n",
    "    comps_st=comps_st,\n",
    "    edges=edges,\n",
    "    arc_capacity={e: intact_capacity[e] * comps_st[e] for e in comps_st},\n",
    "    demand=demand_dict,\n",
    "    max_distance=max_distance,\n",
    "    arc_distance=arc_distance\n",
    ")\n",
    "\n",
    "print(\"\\nüîπ Input Values:\")\n",
    "print(\"Component States (comps_st):\", comps_st)\n",
    "print(\"Edges:\", edges)\n",
    "print(\"Arc Capacity:\", arc_capacity)\n",
    "print(\"Demand:\", demand_dict)\n",
    "print(\"Max Distance:\", max_distance)\n",
    "print(\"Arc Distance:\", arc_distance)\n",
    "\n",
    "expected_loss_mcnf, sys_st_mcnf, min_comps_st_mcnf = sys_fun_mcnf(comps_st)\n",
    "expected_loss_shortest, sys_st_shortest, min_comps_st_shortest = sys_fun_shortest(comps_st)\n",
    "\n",
    "print(\"\\nüîπ Output Values:\")\n",
    "print(\"System State (MCNF):\", sys_st_mcnf)\n",
    "print(\"System State (Shortest):\", sys_st_shortest)\n",
    "print(\"Minimum component state (MCNF):\", min_comps_st_mcnf) \n",
    "print(\"Minimum component state (Shortest):\", min_comps_st_shortest) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4dcbd",
   "metadata": {},
   "source": [
    "# Expected Loss Evaluation\n",
    "By BRC algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7297c2e",
   "metadata": {},
   "source": [
    "### Shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "brs_shortest, rules_shortest, sys_res_shortest, monitor_shortest = brc.run(\n",
    "    probs=probs, \n",
    "    sys_fun=sys_fun_shortest, \n",
    "    max_sf=np.inf, \n",
    "    max_nb=np.inf, \n",
    "    pf_bnd_wr=0.0, \n",
    "    max_rules=100,  \n",
    "    active_decomp=10,\n",
    "    display_freq=5,\n",
    "    brs=[],\n",
    ")\n",
    "\n",
    "# Check if BRC stopped due to max_rules\n",
    "if len(rules_shortest['s'] + rules_shortest['f']) >= 100:\n",
    "    print(\"\\nüîπ BRC terminated because 100 rules have been found.\")\n",
    "\n",
    "# System failure probability\n",
    "Pf_shortest = sum(branch.p for branch in brs_shortest if branch.down_state == 'f')\n",
    "print(f\"System Failure Probability (P_f): {Pf_shortest}\")\n",
    "\n",
    "# Identify unknown branches (brs_u)\n",
    "brs_u_shortest = [branch for branch in brs_shortest if branch.down_state == 'u' or branch.up_state == 'u']\n",
    "\n",
    "# Print rules\n",
    "print(\"All Survival Rules:\")\n",
    "for i, rule in enumerate(rules_shortest['s'], 1):\n",
    "    print(f\"Rule {i}: {rule}\")\n",
    "\n",
    "print(\"\\nAll Failure Rules:\")\n",
    "for i, rule in enumerate(rules_shortest['f'], 1):\n",
    "    print(f\"Rule {i}: {rule}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf84285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print branches\n",
    "print(\"MCNF All Branches:\")\n",
    "for i, branch in enumerate(brs_shortest, 1):\n",
    "    print(f\"Branch {i}:\")\n",
    "    print(f\"  Down: {branch.down}\")\n",
    "    print(f\"  Up: {branch.up}\")\n",
    "    print(f\"  Down State: {branch.down_state}\")\n",
    "    print(f\"  Up State: {branch.up_state}\")\n",
    "    print(f\"  Probability: {branch.p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîπ Running Monte Carlo Simulation (MCS) after BRC termination.\")\n",
    "\n",
    "def sys_fun_rs_shortest(sample1):\n",
    "    val, sys_st, _ = sys_fun_shortest(sample1)\n",
    "    return val, sys_st\n",
    "\n",
    "# Define system variable\n",
    "varis = {}\n",
    "for k in edges:\n",
    "    varis[k] = variable.Variable(name=k, values=[0, 1]) \n",
    "varis['sys_event'] = variable.Variable(name='sys_event', values=['f', 's', 'u'])\n",
    "\n",
    "# Create CPMs for each component event\n",
    "cpms = {}  # Initialize CPMs\n",
    "for k, v in edges.items():\n",
    "    cpms[k] = cpm.Cpm(\n",
    "        variables=[varis[k]], \n",
    "        no_child=1, \n",
    "        C=np.array([[0], [1]]),  # Define states (0: fail, 1: survive)\n",
    "        p=np.array([probs[k][0], probs[k][1]])  # Assign probabilities\n",
    "    )\n",
    "\n",
    "def get_composite_state(vari, states):\n",
    "    added = set(states)\n",
    "    if added not in vari.B:\n",
    "        vari.B.append(added)\n",
    "    cst = vari.B.index(added)\n",
    "    return vari, cst\n",
    "\n",
    "import mbnpy.variable as variable\n",
    "variable.get_composite_state = get_composite_state\n",
    "\n",
    "# Generate system constraint matrix\n",
    "Csys, varis = brc.get_csys(brs_shortest, varis, {'f': 0, 's': 1, 'u': 2})\n",
    "Csys = np.array(Csys, dtype=np.int32)  # Ensure integer type\n",
    "\n",
    "# Ensure \"sys_event\" exists in CPMs\n",
    "if \"sys_event\" not in cpms:\n",
    "    cpms[\"sys_event\"] = cpm.Cpm(\n",
    "        variables=[varis[\"sys_event\"]] + [varis[e] for e in edges],\n",
    "        no_child=1,\n",
    "        C=np.zeros((1, len(edges) + 1), dtype=int),  # Initialize constraints matrix\n",
    "        p=np.array([1.0])  # Set initial probability\n",
    "    )\n",
    "\n",
    "# Adjust constraint matrix dimensions if needed\n",
    "expected_columns = len(cpms[\"sys_event\"].variables)\n",
    "actual_columns = Csys.shape[1]\n",
    "\n",
    "if actual_columns != expected_columns:\n",
    "    print(f\"‚ö† Warning: Adjusting Csys! Expected {expected_columns} columns, but got {actual_columns}.\")\n",
    "    if actual_columns > expected_columns:\n",
    "        Csys = Csys[:, :expected_columns]  # Trim excess columns\n",
    "    else:\n",
    "        missing_cols = expected_columns - actual_columns\n",
    "        extra_cols = np.full((Csys.shape[0], missing_cols), 0)  # Fill missing columns with 0\n",
    "        Csys = np.hstack((Csys, extra_cols))\n",
    "\n",
    "cpms[\"sys_event\"].Cs = Csys  # Assign constraint matrix to CPMs\n",
    "\n",
    "\n",
    "# Transform unknown branches for MCS\n",
    "brs_u_transformed_shortest = [\n",
    "    (\n",
    "        b.down,\n",
    "        b.up,\n",
    "        round(float(b.p), 20),  # Convert probability to float and round\n",
    "        b.down_state,\n",
    "        b.up_state,\n",
    "    )\n",
    "    for b in brs_u_shortest\n",
    "]\n",
    "\n",
    "# Run Monte Carlo Simulation for unknown branches\n",
    "cpms_shortest, mcs_result_shortest = batch.mcs_unknown(\n",
    "    brs_u=brs_u_transformed_shortest,\n",
    "    probs=probs,\n",
    "    sys_fun_rs=sys_fun_rs_shortest,\n",
    "    cpms=cpms,\n",
    "    sys_name=\"sys_event\",\n",
    "    cov_t=0.1,  # Convergence threshold\n",
    "    sys_st_monitor=0,  # System failure monitoring state\n",
    "    sys_st_prob=round(Pf_shortest, 20),  # Round system failure probability\n",
    "    rand_seed=1  # Set random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Print MCS results\n",
    "print(f\"\\nüîπ MCS Completed. Failure Probability (pf): {mcs_result_shortest['pf']:.10e}\")\n",
    "print(f\"   - COV: {mcs_result_shortest['cov']:.4e}\")\n",
    "print(f\"   - Confidence Interval: [{mcs_result_shortest['cint_low']:.10e}, {mcs_result_shortest['cint_up']:.10e}]\")\n",
    "print(f\"   - Number of Samples: {mcs_result_shortest['nsamp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd2f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import batch\n",
    "\n",
    "importlib.reload(batch)\n",
    "\n",
    "def sys_fun_rs_shortest(sample):\n",
    "    val, sys_st, _ = sys_fun_shortest(sample)\n",
    "    return val, sys_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store final results for each component\n",
    "final_results_1 = {}\n",
    "\n",
    "# Iterate over all components in probs_cpm\n",
    "for X_n in probs_cpm.keys():  \n",
    "    print(f\"... Computing for Component: {X_n}\")\n",
    "\n",
    "    # Step 1: Filter branches and apply system function (using P(X_n=1))\n",
    "    survival_known_branch1_shortest, unknown_branch1_shortest = batch.eventspace_x1_filter(brs_u_shortest, X_n, probs_cpm[X_n][1], sys_fun_shortest)\n",
    "\n",
    "    # Step 2: Compute probabilities\n",
    "    total_prob_survival1_shortest = batch.compute_total_probability(survival_known_branch1_shortest)\n",
    "    total_prob_unknown1_shortest = batch.compute_total_probability(unknown_branch1_shortest)\n",
    "    P_Xi_1 = probs_cpm[X_n][1]  # Extract P(X_i=1) from probs_cpm\n",
    "\n",
    "    # Step 3: Compute P(S=1, X_i=1) from known branches\n",
    "    B_s_shortest = [branch for branch in brs_shortest if branch.down_state == 's']\n",
    "    brc_survival_prob1_shortest = batch.survivalprob_xi1_brc100(brc_branches=B_s_shortest, probs=probs_cpm, target_xi=X_n)\n",
    "\n",
    "    # Step 4: Run Monte Carlo Simulation (MCS) for unknown branches\n",
    "    survival_prob1_shortest = brc_survival_prob1_shortest + total_prob_survival1_shortest\n",
    "\n",
    "    mcs_result_unknown_shortest = batch.run_mcs_for_unknown_branch(\n",
    "        brs_u=brs_u_shortest,\n",
    "        unknown_branch=unknown_branch1_shortest, \n",
    "        probs=probs_cpm,  \n",
    "        sys_fun_rs=sys_fun_rs_shortest, \n",
    "        cov_t=0.01,  \n",
    "        sys_st_monitor=1,\n",
    "        survival_prob=survival_prob1_shortest,\n",
    "        rand_seed=None\n",
    "    )\n",
    "\n",
    "    # Compute conditional probability P(S=1 | X_i=1)\n",
    "    P_S_given_Xi_1_shortest = mcs_result_unknown_shortest['ps'] / P_Xi_1\n",
    "    P_S_given_Xi_1_low_shortest = mcs_result_unknown_shortest['cint_low'] / P_Xi_1\n",
    "    P_S_given_Xi_1_up_shortest = mcs_result_unknown_shortest['cint_up'] / P_Xi_1\n",
    "    COV1 = mcs_result_unknown_shortest['cov']\n",
    "\n",
    "    # Compute alpha and beta for Beta distribution\n",
    "    alpha1, beta1 = batch.beta_parameters(P_S_given_Xi_1_shortest, COV1)\n",
    "\n",
    "    # Store results\n",
    "    final_results_1[X_n] = {\n",
    "        'P(X_i=1, S=1)': mcs_result_unknown_shortest['ps'],\n",
    "        'P(S=1 | X_i=1)': P_S_given_Xi_1_shortest,\n",
    "        'COV': COV1,\n",
    "        'Confidence Interval': [P_S_given_Xi_1_low_shortest, P_S_given_Xi_1_up_shortest],\n",
    "        'Number of Samples': mcs_result_unknown_shortest['nsamp'],\n",
    "        'Alpha': alpha1,\n",
    "        'Beta': beta1\n",
    "    }\n",
    "\n",
    "# Print final results for all components\n",
    "print(\"\\nüîπ **Final Conditional Probabilities for All Components** üîπ\")\n",
    "for comp, values in final_results_1.items():\n",
    "    print(f\"\\nComponent: {comp}\")\n",
    "    print(f\"   - Estimated P(S=1 | {comp}=1): {values['P(S=1 | X_i=1)']:.10e}\")\n",
    "    print(f\"   - COV: {values['COV']:.4e}\")\n",
    "    print(f\"   - Confidence Interval: [{values['Confidence Interval'][0]:.10e}, {values['Confidence Interval'][1]:.10e}]\")\n",
    "    print(f\"   - Number of Samples: {values['Number of Samples']}\")\n",
    "    print(f\"   - Alpha: {values['Alpha']:.4e}, Beta: {values['Beta']:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996af463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store final results for each component\n",
    "final_results_0 = {}\n",
    "\n",
    "# Iterate over all components in probs_cpm\n",
    "for X_n in probs_cpm.keys():  \n",
    "    print(f\"... Computing for Component: {X_n}\")\n",
    "\n",
    "    # Step 1: Filter branches and apply system function\n",
    "    survival_known_branch_shortest, unknown_branch_shortest = batch.eventspace_x0_filter(brs_u_shortest, X_n, probs_cpm[X_n][0], sys_fun_shortest)\n",
    "\n",
    "    # Step 2: Compute probabilities\n",
    "    total_prob_survival_shortest = batch.compute_total_probability(survival_known_branch_shortest)\n",
    "    total_prob_unknown_shortest = batch.compute_total_probability(unknown_branch_shortest)\n",
    "    P_Xi_0_shortest = probs_cpm[X_n][0]  # Extract P(X_i=0) from probs_cpm\n",
    "\n",
    "    # Step 3: Compute P(S=1, X_i=0) from known branches\n",
    "    B_s_shortest = [branch for branch in brs_shortest if branch.down_state == 's']\n",
    "    brc_survival_prob_shortest = batch.survivalprob_xi0_brc100(brc_branches=B_s_shortest, probs=probs_cpm, target_xi=X_n)\n",
    "\n",
    "    # Step 4: Run Monte Carlo Simulation (MCS) for unknown branches\n",
    "    survival_prob = brc_survival_prob_shortest + total_prob_survival_shortest\n",
    "\n",
    "    mcs_result_unknown_0 = batch.run_mcs_for_unknown_branch(\n",
    "        brs_u=brs_u_shortest,\n",
    "        unknown_branch=unknown_branch_shortest,\n",
    "        probs=probs_cpm,  \n",
    "        sys_fun_rs=sys_fun_rs_shortest, \n",
    "        cov_t=0.01,  \n",
    "        sys_st_monitor=1,\n",
    "        survival_prob=survival_prob,\n",
    "        rand_seed=None\n",
    "    )\n",
    "\n",
    "    # Compute conditional probability P(S=1 | X_i=0)\n",
    "    P_S_given_Xi_0_shortest = mcs_result_unknown_0['ps'] / P_Xi_0_shortest\n",
    "    P_S_given_Xi_0_low_shortest = mcs_result_unknown_0['cint_low'] / P_Xi_0_shortest\n",
    "    P_S_given_Xi_0_up_shortest = mcs_result_unknown_0['cint_up'] / P_Xi_0_shortest\n",
    "    COV0 = mcs_result_unknown_0['cov']\n",
    "\n",
    "    # Compute alpha and beta for Beta distribution\n",
    "    alpha0, beta0 = batch.beta_parameters(P_S_given_Xi_0_shortest, COV0)\n",
    "\n",
    "    # Store results\n",
    "    final_results_0[X_n] = {\n",
    "        'P(X_i=0, S=1)': mcs_result_unknown_0['ps'],\n",
    "        'P(S=1 | X_i=0)': P_S_given_Xi_0_shortest,\n",
    "        'COV': COV0,\n",
    "        'Confidence Interval': [P_S_given_Xi_0_low_shortest, P_S_given_Xi_0_up_shortest],\n",
    "        'Number of Samples': mcs_result_unknown_0['nsamp'],\n",
    "        'Alpha': alpha0,\n",
    "        'Beta': beta0\n",
    "    }\n",
    "\n",
    "# Print final results for all components\n",
    "print(\"\\nüîπ **Final Conditional Probabilities for All Components** üîπ\")\n",
    "for comp, values in final_results_0.items():\n",
    "    print(f\"\\nComponent: {comp}\")\n",
    "    print(f\"   - Estimated P(S=1 | {comp}=0): {values['P(S=1 | X_i=0)']:.10e}\")\n",
    "    print(f\"   - COV: {values['COV']:.4e}\")\n",
    "    print(f\"   - Confidence Interval: [{values['Confidence Interval'][0]:.10e}, {values['Confidence Interval'][1]:.10e}]\")\n",
    "    print(f\"   - Number of Samples: {values['Number of Samples']}\")\n",
    "    print(f\"   - Alpha: {values['Alpha']:.4e}, Beta: {values['Beta']:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(0, 1, 100)   # Define the range of x values for Beta PDF\n",
    "num_components = len(probs_cpm.keys())  # Number of components\n",
    "\n",
    "# Automatically determine grid size (rows and columns)\n",
    "ncols = min(6, num_components)  \n",
    "nrows = (num_components + ncols - 1) // ncols \n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows))\n",
    "axes = np.array(axes).flatten()  \n",
    "\n",
    "for ax, X_n in zip(axes, probs_cpm.keys()):\n",
    "    print(f\"Computing Beta PDF for Component: {X_n}\")\n",
    "\n",
    "    alpha1 = final_results_1[X_n]['Alpha']\n",
    "    beta1 = final_results_1[X_n]['Beta']\n",
    "    alpha2 = final_results_0[X_n]['Alpha']\n",
    "    beta2 = final_results_0[X_n]['Beta']\n",
    "\n",
    "    # Compute Beta PDFs\n",
    "    beta_pdf_1 = beta.pdf(x_values, alpha1, beta1)\n",
    "    beta_pdf_2 = beta.pdf(x_values, alpha2, beta2)\n",
    "   \n",
    "    ax.plot(x_values, beta_pdf_1, label=f\"P(S=1 | {X_n}=1)\", color='blue')  # Plot Beta PDF for P(S=1 | X_i=1)\n",
    "    ax.plot(x_values, beta_pdf_2, label=f\"P(S=1 | {X_n}=0)\", color='red')   # Plot Beta PDF for P(S=1 | X_i=0)\n",
    "    ax.set_title(f\"Component {X_n} Beta PDF\")\n",
    "    ax.set_xlabel(\"Probability\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eaf6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_values = np.linspace(-1, 1, 100)  # Define Y values for BM distribution\n",
    "num_components = len(probs_cpm.keys())  # Number of components\n",
    "\n",
    "# Automatically determine grid size (rows and columns)\n",
    "ncols = min(6, num_components) \n",
    "nrows = (num_components + ncols - 1) // ncols \n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows))\n",
    "axes = np.array(axes).flatten() \n",
    "\n",
    "bm_results = {}\n",
    "bm_confidence_intervals = {}\n",
    "most_probable_bm_values = [] \n",
    "component_names = []        \n",
    "\n",
    "for ax, X_n in zip(axes, probs_cpm.keys()):\n",
    "    print(f\"Computing BM distribution with mpmath for Component: {X_n}\")\n",
    "\n",
    "    alpha1 = final_results_1[X_n]['Alpha']\n",
    "    beta1 = final_results_1[X_n]['Beta']\n",
    "    alpha2 = final_results_0[X_n]['Alpha']\n",
    "    beta2 = final_results_0[X_n]['Beta']\n",
    "    \n",
    "    # Compute BM distribution\n",
    "    BM_distribution = np.array([\n",
    "        batch.f_Y_mpmath(y, alpha1, beta1, alpha2, beta2) for y in y_values\n",
    "    ])\n",
    "\n",
    "    # Check for invalid values\n",
    "    if np.sum(BM_distribution) == 0 or not np.all(np.isfinite(BM_distribution)):\n",
    "        print(f\"‚ùå Skipping component: {X_n} due to invalid BM values.\")\n",
    "        for y, val in zip(y_values, BM_distribution):\n",
    "            if not np.isfinite(val):\n",
    "                print(f\"   ‚Üí Y = {y:.4f}, f_Y = {val}\")\n",
    "        continue\n",
    "\n",
    "    # Mode = Y with max f_Y\n",
    "    mode_BM = y_values[np.argmax(BM_distribution)]\n",
    "\n",
    "    # Store results\n",
    "    bm_results[X_n] = BM_distribution\n",
    "    bm_confidence_intervals[X_n] = {\n",
    "        'Lower Bound': y_values[np.searchsorted(np.cumsum(BM_distribution) / np.sum(BM_distribution), 0.025)],\n",
    "        'Upper Bound': y_values[np.searchsorted(np.cumsum(BM_distribution) / np.sum(BM_distribution), 0.975)],\n",
    "        'Most Probable BM': mode_BM\n",
    "    }\n",
    "\n",
    "    # üî∏ Ï∂îÍ∞Ä: bar chartÏö© Í∞í Ï†ÄÏû•\n",
    "    component_names.append(X_n)\n",
    "    most_probable_bm_values.append(mode_BM)\n",
    "\n",
    "    # Plot\n",
    "    ax.plot(y_values, BM_distribution, label=f\"{X_n}\", color='blue')\n",
    "    ax.axvline(mode_BM, color='purple', linestyle='-', label=\"Most Probable BM\")\n",
    "    ax.set_title(f\"Component {X_n}\")\n",
    "    ax.set_xlabel(\"Y = P(S=1 | X_i=1) - P(S=1 | X_i=0)\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# üî∏ Bar Chart Î∞îÎ°ú Ïù¥Ïñ¥ÏÑú\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(component_names, most_probable_bm_values, color='gray', alpha=0.7)\n",
    "plt.xlabel(\"Component\")\n",
    "plt.ylabel(\"Most Probable BM Value\")\n",
    "plt.title(\"Component Importance Based on Birnbaum's Measure (BRC+Sampling)\")\n",
    "plt.ylim(-0.05, 1) \n",
    "plt.xticks(rotation=60, fontsize=6) \n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9657b8",
   "metadata": {},
   "source": [
    "### MCNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92231c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run BRC with stopping criteria (max_rules = 100)\n",
    "brs_mcnf, rules_mcnf, sys_res_mcnf, monitor_mcnf = brc.run(\n",
    "    probs=probs, \n",
    "    sys_fun=sys_fun_mcnf, \n",
    "    max_sf=np.inf, \n",
    "    max_nb=np.inf, \n",
    "    pf_bnd_wr=0.0, \n",
    "    max_rules=100,  \n",
    "    active_decomp=10,\n",
    "    display_freq=5,\n",
    "    brs=[],\n",
    ")\n",
    "\n",
    "# Check if BRC stopped due to max_rules\n",
    "if len(rules_mcnf['s'] + rules_mcnf['f']) >= 100:\n",
    "    print(\"\\nüîπ BRC terminated because 100 rules have been found.\")\n",
    "\n",
    "# System failure probability\n",
    "Pf_mcnf = sum(branch.p for branch in brs_mcnf if branch.down_state == 'f')\n",
    "print(f\"System Failure Probability (P_f): {Pf_mcnf}\")\n",
    "\n",
    "# Identify unknown branches (brs_u)\n",
    "brs_u_mcnf = [branch for branch in brs_mcnf if branch.down_state == 'u' or branch.up_state == 'u']\n",
    "\n",
    "# Print survival rules\n",
    "print(\"All Survival Rules:\")\n",
    "for i, rule in enumerate(rules_mcnf['s'], 1):\n",
    "    print(f\"Rule {i}: {rule}\")\n",
    "\n",
    "# Print failure rules\n",
    "print(\"\\nAll Failure Rules:\")\n",
    "for i, rule in enumerate(rules_mcnf['f'], 1):\n",
    "    print(f\"Rule {i}: {rule}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print branches\n",
    "print(\"MCNF All Branches:\")\n",
    "for i, branch in enumerate(brs_mcnf, 1):\n",
    "    print(f\"Branch {i}:\")\n",
    "    print(f\"  Down: {branch.down}\")\n",
    "    print(f\"  Up: {branch.up}\")\n",
    "    print(f\"  Down State: {branch.down_state}\")\n",
    "    print(f\"  Up State: {branch.up_state}\")\n",
    "    print(f\"  Probability: {branch.p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba5cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîπ Running Monte Carlo Simulation (MCS) after BRC termination.\")\n",
    "\n",
    "def sys_fun_rs(sample1):\n",
    "    val, sys_st, _ = sys_fun_mcnf(sample1)\n",
    "    return val, sys_st\n",
    "\n",
    "# Define system variable\n",
    "varis = {}\n",
    "for k in edges:\n",
    "    varis[k] = variable.Variable(name=k, values=[0, 1]) \n",
    "varis['sys_event'] = variable.Variable(name='sys_event', values=['f', 's', 'u'])\n",
    "\n",
    "# Create CPMs for each component event\n",
    "cpms = {}  # Initialize CPMs\n",
    "for k, v in edges.items():\n",
    "    cpms[k] = cpm.Cpm(\n",
    "        variables=[varis[k]], \n",
    "        no_child=1, \n",
    "        C=np.array([[0], [1]]),  # Define states (0: fail, 1: survive)\n",
    "        p=np.array([probs[k][0], probs[k][1]])  # Assign probabilities\n",
    "    )\n",
    "\n",
    "def get_composite_state(vari, states):\n",
    "    added = set(states)\n",
    "    if added not in vari.B:\n",
    "        vari.B.append(added)\n",
    "    cst = vari.B.index(added)\n",
    "    return vari, cst\n",
    "\n",
    "import mbnpy.variable as variable\n",
    "variable.get_composite_state = get_composite_state\n",
    "\n",
    "# Generate system constraint matrix\n",
    "Csys, varis = brc.get_csys(brs_mcnf, varis, {'f': 0, 's': 1, 'u': 2})\n",
    "Csys = np.array(Csys, dtype=np.int32)  # Ensure integer type\n",
    "\n",
    "# Ensure \"sys_event\" exists in CPMs\n",
    "if \"sys_event\" not in cpms:\n",
    "    cpms[\"sys_event\"] = cpm.Cpm(\n",
    "        variables=[varis[\"sys_event\"]] + [varis[e] for e in edges],\n",
    "        no_child=1,\n",
    "        C=np.zeros((1, len(edges) + 1), dtype=int),  # Initialize constraints matrix\n",
    "        p=np.array([1.0])  # Set initial probability\n",
    "    )\n",
    "\n",
    "# Adjust constraint matrix dimensions if needed\n",
    "expected_columns = len(cpms[\"sys_event\"].variables)\n",
    "actual_columns = Csys.shape[1]\n",
    "\n",
    "if actual_columns != expected_columns:\n",
    "    print(f\"‚ö† Warning: Adjusting Csys! Expected {expected_columns} columns, but got {actual_columns}.\")\n",
    "    if actual_columns > expected_columns:\n",
    "        Csys = Csys[:, :expected_columns]  # Trim excess columns\n",
    "    else:\n",
    "        missing_cols = expected_columns - actual_columns\n",
    "        extra_cols = np.full((Csys.shape[0], missing_cols), 0)  # Fill missing columns with 0\n",
    "        Csys = np.hstack((Csys, extra_cols))\n",
    "\n",
    "cpms[\"sys_event\"].Cs = Csys  # Assign constraint matrix to CPMs\n",
    "\n",
    "\n",
    "# Transform unknown branches for MCS\n",
    "brs_u_transformed_mcnf = [\n",
    "    (\n",
    "        b.down,\n",
    "        b.up,\n",
    "        round(float(b.p), 20),  # Convert probability to float and round\n",
    "        b.down_state,\n",
    "        b.up_state,\n",
    "    )\n",
    "    for b in brs_u_mcnf\n",
    "]\n",
    "\n",
    "# Run Monte Carlo Simulation for unknown branches\n",
    "cpms_mcnf, mcs_result_mcnf = batch.mcs_unknown(\n",
    "    brs_u=brs_u_transformed_mcnf,\n",
    "    probs=probs,\n",
    "    sys_fun_rs=sys_fun_rs,\n",
    "    cpms=cpms,\n",
    "    sys_name=\"sys_event\",\n",
    "    cov_t=0.01,  # Convergence threshold\n",
    "    sys_st_monitor=0,  # System failure monitoring state\n",
    "    sys_st_prob=round(Pf_mcnf, 20),  # Round system failure probability\n",
    "    rand_seed=1  # Set random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Print MCS results\n",
    "print(f\"\\nüîπ MCS Completed. Failure Probability (pf): {mcs_result_mcnf['pf']:.10e}\")\n",
    "print(f\"   - COV: {mcs_result_mcnf['cov']:.4e}\")\n",
    "print(f\"   - Confidence Interval: [{mcs_result_mcnf['cint_low']:.10e}, {mcs_result_mcnf['cint_up']:.10e}]\")\n",
    "print(f\"   - Number of Samples: {mcs_result_mcnf['nsamp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **ODÎßàÎã§ BRC ÎèåÎ¶¨Îäî ÏΩîÎìú**\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "##### Í≤∞Í≥º Ï†ÄÏû• ÎîïÏÖîÎÑàÎ¶¨\n",
    "brc_results_by_od = {}\n",
    "\n",
    "demand_dict = {}\n",
    "\n",
    "for idx, item in enumerate(demand, start=1):\n",
    "    key = f\"k{idx}\"\n",
    "    demand_dict[key] = {\n",
    "        \"origin\": item[\"origin_name\"],\n",
    "        \"destination\": item[\"destination_name\"],\n",
    "        \"amount\": item[\"journeys\"],\n",
    "        \"distance\": item[\"distance\"]\n",
    "    }\n",
    "    \n",
    "##### Í∞Å ODÏóê ÎåÄÌï¥ Î∞òÎ≥µ\n",
    "for od_key in demand_dict:\n",
    "    print(f\"\\nüîç Running BRC for OD pair: {od_key} ...\")\n",
    "\n",
    "    sys_fun = lambda comps_st, od=od_key: MCNF_od_systemfunc(\n",
    "    arcs=arcs, \n",
    "    comps_st=comps_st, \n",
    "    edges=edges, \n",
    "    arc_capacity={e: int(intact_capacity[e] * comps_st[e]) for e in comps_st},\n",
    "    demand=demand_dict,\n",
    "    max_distance=max_distance, \n",
    "    arc_distance=arc_distance, \n",
    "    target_od=od)\n",
    "\n",
    "    # BRC Ïã§Ìñâ\n",
    "    brs, rules, sys_res, monitor = brc.run(\n",
    "        probs=probs,\n",
    "        sys_fun=sys_fun,\n",
    "        max_sf=np.inf,\n",
    "        max_nb=np.inf,\n",
    "        pf_bnd_wr=0.0,\n",
    "        max_rules=30,\n",
    "        active_decomp=10,\n",
    "        display_freq=5,\n",
    "        brs=[],\n",
    "    )\n",
    "\n",
    "    # Í≤∞Í≥º Ï†ïÎ¶¨\n",
    "    P_f = sum(branch.p for branch in brs if branch.down_state == 'f')\n",
    "\n",
    "    print(f\"‚Üí OD {od_key}: P_f = {P_f:.6f}\")\n",
    "    if len(rules['s'] + rules['f']) >= 100:\n",
    "        print(\"‚ö†Ô∏è  BRC stopped due to max_rules=100\")\n",
    "\n",
    "    # Ï†ÄÏû•\n",
    "    brc_results_by_od[od_key] = {\n",
    "        'brs': brs,\n",
    "        'rules': rules,\n",
    "        'P_f': P_f,\n",
    "        'sys_res': sys_res,\n",
    "        'monitor': monitor,\n",
    "    }\n",
    "\n",
    "    print(rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def serialize_brc_results(results_dict):\n",
    "    serializable_results = {}\n",
    "\n",
    "    for od_key, result in results_dict.items():\n",
    "        serializable_results[od_key] = {\n",
    "            'P_f': result['P_f'],\n",
    "            'rule_count': {\n",
    "                's': len(result['rules'].get('s', [])),\n",
    "                'f': len(result['rules'].get('f', [])),\n",
    "            },\n",
    "            'rules': {\n",
    "                's': [rule['f'] for rule in result['rules'].get('s', [])],\n",
    "                'f': [rule['f'] for rule in result['rules'].get('f', [])],\n",
    "            }\n",
    "        }\n",
    "    return serializable_results\n",
    "\n",
    "serializable_data = serialize_brc_results(brc_results_by_od)\n",
    "\n",
    "output_path = \"brc_results_by_od_summary.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(serializable_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved summary to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjkang",
   "language": "python",
   "name": "mjkang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
